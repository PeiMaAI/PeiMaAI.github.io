<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-15T19:35:36+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pei Ma</title><subtitle>A blog about Large Language Model, RAG, and fine-tuning.</subtitle><entry><title type="html">RAG End2end Frame and Vector DB Summary(CN)</title><link href="http://localhost:4000/RAG-End2end-Frame-and-Vector-DB-Summary-CN/" rel="alternate" type="text/html" title="RAG End2end Frame and Vector DB Summary(CN)" /><published>2024-08-15T18:20:00+01:00</published><updated>2024-08-15T18:20:00+01:00</updated><id>http://localhost:4000/RAG-End2end-Frame-and-Vector-DB-Summary-CN</id><content type="html" xml:base="http://localhost:4000/RAG-End2end-Frame-and-Vector-DB-Summary-CN/"><![CDATA[<h1 id="引言">引言</h1>

<p>随着大规模语言模型（LLM）的发展，检索增强生成（Retrieval-Augmented Generation, RAG）技术在解决复杂的自然语言处理任务中展现出显著优势。RAG通过结合文本生成和信息检索的能力，极大地提升了模型的表现，使其在知识密集型任务中能够提供更加准确和上下文相关的答案。这种技术不仅在问答系统中得到了广泛应用，还为企业级应用、文档管理、和客户支持等多个领域提供了创新的解决方案。</p>

<p>然而，随着RAG的快速发展，市场上涌现了众多的框架和工具。这些工具各具特色，有些专注于隐私保护，有些则优化了性能和灵活性，还有一些提供了强大的扩展能力和集成选项。本文将对当前主流的RAG框架和相关的向量数据库进行全面总结，分析其优缺点，帮助读者在选择和部署RAG解决方案时做出更为明智的决策。</p>

<h2 id="rag-框架与工具">RAG 框架与工具</h2>

<h3 id="1-oobabooga-with-superboogav2">1. Oobabooga with Superboogav2</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/oobabooga/text-generation-webui">Oobabooga GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/oobabooga/text-generation-webui">Oobabooga 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
Oobabooga是一个开源的文本生成Web界面，设计初衷是提供一个轻量级的平台，供用户在本地或远程主机上运行各种预训练的大型语言模型（LLM）。Superboogav2作为Oobabooga的一个插件，旨在增强其文本生成能力。然而，对于需要实现本地化的检索增强生成（RAG）应用，该组合的功能相对有限。Oobabooga更侧重于基础的文本生成任务，缺乏复杂文档检索和问答的高级功能。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>简单易用</strong>: 设置和启动相对简单，适合初学者快速上手。</li>
  <li><strong>多模型支持</strong>: 支持多种模型和插件，具有较高的灵活性，适合不同的生成任务。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>功能不足</strong>: 在复杂的文档检索和问答任务中表现欠佳，难以应对RAG的高级需求。</li>
  <li><strong>配置受限</strong>: 缺乏对嵌入方法、向量存储等方面的细致控制，不适合需要高度定制化的应用场景。</li>
</ul>

<hr />

<h3 id="2-privategpt">2. privateGPT</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/imartinez/privateGPT">privateGPT GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/imartinez/privateGPT">privateGPT 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
privateGPT是一个专注于隐私保护的本地化RAG框架，允许用户在离线环境中使用私人文档进行问答。该框架尤其适合对数据隐私和安全性有严格要求的用户。privateGPT支持在本地运行，确保所有操作均在离线状态下进行，然而，其架构设计限制了功能的扩展性，使其在复杂任务中的表现稍显不足。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>隐私保护</strong>: 全程离线操作，确保数据隐私不受外部威胁。</li>
  <li><strong>易于部署</strong>: 无需互联网连接，即可在本地环境中快速部署和运行。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>单一向量存储</strong>: 仅支持单一向量存储，不便于管理多个文档集。</li>
  <li><strong>不可移除文档</strong>: 一旦文档被添加到向量存储中，无法移除，导致管理不便。</li>
  <li><strong>复杂的GPU支持</strong>: GPU利用较为复杂，可能导致性能下降，特别是在硬件配置较低的情况下。</li>
  <li><strong>配置麻烦</strong>: 更换模型需要手动编辑配置文件，并重新启动服务，缺乏用户友好的配置界面。</li>
</ul>

<hr />

<h3 id="3-localgpt">3. localGPT</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/PromtEngineer/localGPT">localGPT GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/PromtEngineer/localGPT">localGPT 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
localGPT是一个致力于提供本地化RAG功能的工具，允许用户在本地运行GPT模型并结合私人文档进行检索和问答。尽管localGPT在保证数据安全方面有一定优势，但其用户体验主要依赖于CLI（命令行界面），对不熟悉命令行操作的用户并不友好。此外，localGPT在模型和嵌入配置方面的灵活性也受到一定限制。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>本地化执行</strong>: 支持在本地环境中运行RAG应用，确保数据不会泄露到外部。</li>
  <li><strong>灵活配置</strong>: 虽然过程较为复杂，但允许用户通过代码更改嵌入方法，提供了一定的灵活性。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>用户体验差</strong>: 所有操作需通过CLI完成，缺乏直观的图形用户界面，增加了使用门槛。</li>
  <li><strong>复杂的模型管理</strong>: 更换模型和嵌入方法需要手动编辑代码并重新启动服务，操作不够简便。</li>
</ul>

<hr />

<h3 id="4-lmstudio">4. LMStudio</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/lmstudio-ai">LMStudio GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://lmstudio.ai/">LMStudio 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
LMStudio是一个功能强大的文本生成平台，提供了用户友好的GUI，用于管理、下载和切换大型语言模型。它为用户提供了简便的模型管理体验，然而缺乏与私人文档进行交互的功能，这使得它在RAG应用中表现不够全面。尽管如此，LMStudio仍然是一个出色的工具，尤其适合那些专注于文本生成而非文档检索的用户。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>强大的GUI</strong>: 允许用户通过图形界面轻松管理和切换模型，极大简化了操作流程。</li>
  <li><strong>多模型支持</strong>: 支持多种大型语言模型的管理和使用，提供了较高的灵活性。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>缺乏文档交互功能</strong>: 不支持“与文档交互”的功能，难以用于RAG应用。</li>
  <li><strong>受限于GGUF模型</strong>: 仅支持GGUF格式的模型，在特定任务中的表现可能受到限制。</li>
</ul>

<hr />

<h3 id="5-ollama">5. OLlama</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/jmorganca/ollama">OLlama GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/jmorganca/ollama">OLlama 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
OLlama是一个专为Mac用户设计的本地化聊天框架，充分利用了Mac的硬件优化。它支持本地运行大型语言模型，提供快速响应的聊天体验。然而，由于其仅限于Mac平台，无法满足Windows用户的需求，尤其是那些希望利用高性能GPU的用户。此外，OLlama的扩展性也受到了一定的限制。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>专为Mac优化</strong>: 利用Mac的硬件特性，提供高效的本地聊天功能。</li>
  <li><strong>易于使用</strong>: 对Mac用户来说，部署和使用都非常方便，几乎无需额外配置。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>平台限制</strong>: 仅支持Mac系统，对Windows和其他操作系统的用户不友好，尤其是无法利用高性能GPU。</li>
  <li><strong>扩展性有限</strong>: 由于平台限制，无法广泛应用于其他操作系统或更复杂的应用场景。</li>
</ul>

<hr />

<h3 id="6-langchain">6. LangChain</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/hwchase17/langchain">LangChain GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/hwchase17/langchain">LangChain 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
LangChain是一个构建大型语言模型应用程序的框架，提供了丰富的工具和集成选项，旨在帮助开发者创建基于语言模型的复杂应用。尽管LangChain功能强大，但它更适合用作工具包，而不是一个完整的RAG</p>

<p>解决方案。对于那些寻找一站式解决方案的用户来说，LangChain的灵活性反而可能成为一种负担。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>强大的工具集</strong>: 提供了广泛的API和集成选项，适合开发复杂的语言模型应用。</li>
  <li><strong>高灵活性</strong>: 允许开发者根据需求定制应用程序，提供了极大的设计自由度。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>学习曲线陡峭</strong>: 由于其功能广泛，初学者可能会觉得难以上手，需要投入大量时间和精力进行学习和掌握。</li>
  <li><strong>不适合作为单一解决方案</strong>: 更适合作为工具包，而非完整的RAG应用，难以直接应用于实际生产环境。</li>
</ul>

<hr />

<h3 id="7-memgpt">7. MemGPT</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/brown-iv-lab/memgpt">MemGPT GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/brown-iv-lab/memgpt">MemGPT 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
MemGPT是一个较新的项目，旨在通过结合记忆机制来增强GPT模型的表现。虽然该项目仍在开发和测试中，但它为RAG提供了一个有趣的视角，可能为未来的应用开辟新的方向。MemGPT的具体性能和适用性尚需进一步评估，但其创新性为未来的RAG应用提供了值得关注的潜力。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>创新性</strong>: 引入了记忆机制，有潜力提升模型的长期记忆和表现，特别是在复杂的对话和文档检索任务中。</li>
  <li><strong>潜在应用</strong>: 未来可能在RAG领域表现出色，特别是随着技术的进一步发展和成熟。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>尚在开发中</strong>: 功能和性能尚未成熟，需要进一步测试和验证，当前的稳定性和实用性尚不明确。</li>
  <li><strong>可用性未知</strong>: 由于项目尚处于早期阶段，缺乏明确的文档和用户案例，用户的使用体验可能会有所局限。</li>
</ul>

<hr />

<h3 id="8-autogpt">8. AutoGPT</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/Significant-Gravitas/Auto-GPT">AutoGPT GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/Significant-Gravitas/Auto-GPT">AutoGPT 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
AutoGPT是一个自主的GPT系统，能够自动完成一系列复杂任务，包括RAG。在这方面，它被视为一种开创性的工作，尝试构建具有自主能力的AI工具。尽管如此，AutoGPT的嵌入设置不可更改，这在一定程度上限制了用户的自定义能力，尤其是在特定RAG应用中。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>自动化能力</strong>: 能够自主完成复杂的任务链，减少用户干预，适合那些需要高度自动化的应用场景。</li>
  <li><strong>前沿创新</strong>: 代表了自动化AI系统的一个新的探索方向，可能会在未来引领RAG的进一步发展。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>配置受限</strong>: 嵌入设置无法更改，限制了个性化配置的可能性，难以满足特定应用的特殊需求。</li>
  <li><strong>系统复杂</strong>: 系统复杂性较高，可能需要用户具备较高的技术水平来充分利用其功能，入门难度较大。</li>
</ul>

<hr />

<h3 id="9-gpt4all">9. GPT4All</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/nomic-ai/gpt4all">GPT4All GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/nomic-ai/gpt4all">GPT4All 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
GPT4All是一个开源项目，旨在为用户提供本地化的GPT模型交互体验。它的目标是使大型语言模型在本地计算设备上可用，无需依赖云端服务。尽管GPT4All在降低云依赖方面表现出色，但其目前的功能相对基础，更加适合基本的模型交互，而非完整的RAG应用解决方案。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>本地化执行</strong>: 支持在本地计算设备上运行，减少对云服务的依赖，增强数据的安全性和隐私性。</li>
  <li><strong>开源</strong>: 完全开源，用户可以根据需要进行二次开发和定制，灵活性较高。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>功能尚不完善</strong>: 目前功能相对基础，难以支持复杂的RAG应用，还需进一步完善和扩展。</li>
  <li><strong>性能未知</strong>: 由于项目仍在开发中，实际性能和适用性尚待验证，可能存在一定的不确定性。</li>
</ul>

<hr />

<h3 id="10-chatdocs">10. ChatDocs</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/marella/chatdocs">ChatDocs GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/marella/chatdocs">ChatDocs 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
ChatDocs是privateGPT的一个衍生项目，旨在改进GPU支持和GPTQ模型集成。与privateGPT相比，ChatDocs提供了更多的配置选项，尤其是在嵌入设置方面。尽管如此，这些设置仍需通过文件手动修改，缺乏直观的GUI支持。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>增强的GPU支持</strong>: 预配置了GPU和GPTQ模型，性能得到显著改进，尤其是在处理大规模数据时表现更为出色。</li>
  <li><strong>可定制的嵌入设置</strong>: 允许用户更改嵌入设置，尽管需要通过手动操作，提供了一定的灵活性。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>社区支持较少</strong>: 在GitHub上的星数较低，说明社区参与度不高，可能影响用户的支持和帮助获取。</li>
  <li><strong>用户体验一般</strong>: 虽然功能有所增强，但操作仍然依赖于文件编辑和命令行，用户体验不够友好，可能影响普及性。</li>
</ul>

<hr />

<h3 id="11-docsgpt">11. DocsGPT</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/arc53/DocsGPT">DocsGPT GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/arc53/DocsGPT">DocsGPT 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
DocsGPT是一个专注于文档问答的系统，旨在通过GPT模型从文档中提取答案。然而，该系统的生成速度较慢，并且不支持GPU，这使得其在处理大规模数据时的性能受到限制。它更适合用于小规模的、非实时的文档查询任务。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>专注文档问答</strong>: 针对文档检索和问答进行了优化，适合特定领域的应用，尤其是小规模的知识管理和查询任务。</li>
  <li><strong>简单易用</strong>: 对于基础的文档问答任务，操作相对简单，适合非技术用户。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>性能有限</strong>: 由于不支持GPU，生成速度较慢，难以处理大规模或实时任务，在复杂场景中的表现受限。</li>
  <li><strong>扩展性不足</strong>: 在处理复杂或大规模文档集时表现不佳，难以适应多样化的应用需求。</li>
</ul>

<hr />

<h3 id="12-auto-rag">12. Auto RAG</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/IDSCETHZurich/AutoRAG">Auto RAG GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://github.com/IDSCETHZurich/AutoRAG">Auto RAG 主页</a></li>
</ul>

<p><strong>详细介绍</strong>:<br />
Auto RAG是一个自动化的RAG管道选择工具，旨在帮助用户根据具体需求选择最佳的RAG方案。它可以根据输入数据自动生成和选择最优的检索增强生成策略。然而，这个工具对用户的技术水平要求较高，需要使用或创建数据集才能有效使用。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>智能化管道选择</strong>: 能够自动选择和配置最佳的RAG策略，减少用户的手动干预，提高系统的适应性和灵活性。</li>
  <li><strong>针对性强</strong>: 为具体的应用场景提供优化的RAG解决方案，提升应用的效果和效率。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>使用复杂</strong>: 需要用户具备较高的技术水平，使用门槛较高，不适合技术能力较弱的用户。</li>
  <li><strong>数据集依赖</strong>: 必须使用或创建数据集才能启动，操作流程较为繁琐，可能影响用户体验。</li>
</ul>

<hr />

<h2 id="向量数据库">向量数据库</h2>

<h3 id="1-neo4j">1. Neo4j</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/neo4j/neo4j">Neo4j GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://neo4j.com/">Neo4j 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
Neo4j是一个专为处理复杂关系数据设计的图数据库，广泛应用于社交网络分析、推荐系统等领域。虽然它可以在某些RAG场景下使用，但由于其图数据库的特性和架构</p>

<p>，在处理大规模向量数据时表现较慢，并且仅支持有限的结构类型。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>强大的关系数据处理能力</strong>: 擅长复杂关系数据的建模和查询，尤其适合网络分析、推荐系统等应用场景。</li>
  <li><strong>图形查询语言</strong>: 支持专门为图形数据库设计的Cypher查询语言，提供了强大的数据操作能力。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>性能问题</strong>: 在处理大规模数据时，尤其是向量数据，性能表现不够理想。</li>
  <li><strong>有限支持</strong>: 仅支持有限的数据结构类型，在应用场景上存在一定局限性。</li>
</ul>

<hr />

<h3 id="2-chroma">2. Chroma</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/chroma-core/chroma">Chroma GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://www.trychroma.com/">Chroma 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
Chroma是一个现代化的向量数据库，专为简化向量存储和检索而设计。它支持多模态数据，提供丰富的API和内置的嵌入功能，适合快速构建和扩展RAG应用。Chroma的设计目标是提供简便易用的配置选项，帮助开发者快速实现向量数据的存储和检索。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>易于安装</strong>: 依赖Docker或Python，安装配置相对简单，便于快速部署和使用。</li>
  <li><strong>高度可配置</strong>: 提供了丰富的配置选项，能够满足不同应用场景的需求。</li>
  <li><strong>多模态支持</strong>: 支持多模态数据的存储和检索，并提供内置嵌入功能，适合复杂的RAG应用。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>依赖Docker</strong>: 需要Docker或Python环境来运行，可能增加部署的复杂性，特别是在非技术用户中。</li>
</ul>

<hr />

<h3 id="3-lancedb">3. LanceDB</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/lancedb/lance">LanceDB GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://www.lancedb.com/">LanceDB 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
LanceDB是一个专为向量数据设计的数据库，以其极快的速度和简洁的API著称。它可以在本地机器上运行，并支持从磁盘加载数据。即使在拥有超过100万条记录的情况下，LanceDB的检索速度依然非常快，是一个优秀的选择，尤其适用于需要快速检索的本地应用。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>速度快</strong>: 即使在处理大规模数据时，检索速度依然非常快，适合实时性要求较高的应用。</li>
  <li><strong>简单易用</strong>: 提供了简单直接的API，易于集成和使用，降低了开发难度。</li>
  <li><strong>本地运行</strong>: 支持在本地机器上运行，并可从磁盘加载数据，适合数据量大且需高效检索的场景。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>功能有限</strong>: 尽管检索性能优越，但在处理复杂应用场景时可能略显不足，功能相对基础。</li>
</ul>

<hr />

<h3 id="4-pinecone">4. Pinecone</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/pinecone-io">Pinecone GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://www.pinecone.io/">Pinecone 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
Pinecone是一个云原生的向量数据库，专为大规模向量检索和相似度搜索设计。它提供了简单易用的API，并支持全托管服务，非常适合初学者或小规模项目。然而，在某些应用场景下，Pinecone的检索速度可能较慢，且依赖于云服务，对于需要本地化解决方案的用户来说不太适合。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>简单的API</strong>: 提供简单易用的API，便于快速上手，降低了开发和部署的难度。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>性能较慢</strong>: 在某些复杂场景下，检索速度可能不如预期，影响使用体验。</li>
  <li><strong>依赖云服务</strong>: 主要依赖云端服务，对于需要本地化或离线解决方案的用户来说可能不太适合。</li>
</ul>

<hr />

<h3 id="5-astradb">5. AstraDB</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/datastax/astra">AstraDB GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://www.datastax.com/products/datastax-astra">AstraDB 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
AstraDB是由DataStax提供的云数据库服务，基于Apache Cassandra构建，旨在提供高度灵活和快速的查询性能。特别是在处理大规模分布式数据时，AstraDB表现出色，适合需要无服务器架构的应用。然而，由于其功能强大，学习成本较高，可能需要较长时间来掌握其使用方法。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>高性能</strong>: 在分布式环境中表现出色，支持高效的查询和数据操作，适合处理大规模数据。</li>
  <li><strong>灵活性高</strong>: 支持多种数据模型，能够适应复杂应用的需求。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>复杂性</strong>: 功能强大，学习成本较高，可能需要较长时间来掌握并充分利用其全部功能。</li>
</ul>

<hr />

<h3 id="其他常用的-rag-向量数据库">其他常用的 RAG 向量数据库</h3>

<h3 id="6-milvus">6. Milvus</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/milvus-io/milvus">Milvus GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://milvus.io/">Milvus 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
Milvus是一个开源的向量数据库，专为海量向量数据的检索和管理设计。它支持多种索引类型，能够处理大规模、高维度的数据。Milvus具有高度的扩展性，特别适用于需要处理数十亿向量的应用场景。由于其强大的功能，Milvus已成为RAG应用中广泛使用的工具。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>扩展性强</strong>: 能够处理海量向量数据，支持多种索引类型，适合处理高维度、复杂的数据集。</li>
  <li><strong>开源</strong>: 完全开源，提供了灵活的部署选项，适合多种不同的使用场景。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>部署复杂</strong>: 在大规模环境下，部署和维护可能较为复杂，需要较高的技术能力。</li>
</ul>

<hr />

<h3 id="7-weaviate">7. Weaviate</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/semi-technologies/weaviate">Weaviate GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://weaviate.io/">Weaviate 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
Weaviate是一个开源的向量数据库，支持基于AI模型的自动分类和相似度搜索。它具有高度可扩展的架构，能够轻松集成到现有系统中。Weaviate支持多模态数据，并提供了丰富的插件系统，适合需要高度定制化的应用。其灵活的架构使其成为构建复杂RAG应用的理想选择。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>高度可扩展</strong>: 支持多模态数据，具有灵活的插件系统，适应性强。</li>
  <li><strong>AI集成</strong>: 支持AI模型驱动的自动分类和搜索，增强了数据处理和检索的智能性。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>学习曲线较陡</strong>: 由于功能丰富且复杂，初学者可能需要更多时间来掌握其使用方法。</li>
</ul>

<hr />

<h3 id="8-faiss">8. Faiss</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/facebookresearch/faiss">Faiss GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://faiss.ai/">Faiss 主页</a></li>
</ul>

<p><strong>简介</strong>:<br />
Faiss是由Facebook AI Research开发的向量相似度搜索库，专为高效的向量相似度搜索设计。它能够在CPU和GPU上运行，适合处理大规模、高维度的数据集。Faiss是RAG应用中非常流行的选择，尤其是在需要高性能的场景中。虽然它的性能强大，但作为一个库，集成到现有系统中可能需要更多的开发工作。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>高效的搜索性能</strong>: 在处理大规模、高维度向量数据时表现出色，尤其适合需要高性能的应用场景。</li>
  <li><strong>支持GPU</strong>: 支持在GPU上运行，大幅提升处理速度，适合需要高效处理的复杂任务。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>集成复杂</strong>: 作为一个库，而非完整的数据库解决方案，集成到系统中可能需要更多的开发工作，增加了使用难度。</li>
</ul>

<hr />

<h3 id="9-qdrant">9. Qdrant</h3>

<ul>
  <li><strong>源代码</strong>: <a href="https://github.com/qdrant/qdrant">Qdrant GitHub</a></li>
  <li><strong>网站</strong>: <a href="https://qdrant.tech/">Qdrant 主页</a></li>
</ul>

<p><strong>简介</strong>:</p>

<p>Qdrant是一个开源的向量数据库，专注于快速、高效的向量检索。它支持通过REST API进行访问，易于集成到各种应用中。Qdrant提供了强大的向量搜索和过滤功能，适合实时推荐和个性化搜索等应用场景。由于其简洁的设计和高效的性能，Qdrant已成为构建实时性强的RAG应用的热门选择。</p>

<p><strong>优点</strong>:</p>
<ul>
  <li><strong>高效检索</strong>: 支持快速的向量检索和过滤功能，适合实时性要求较高的应用。</li>
  <li><strong>易于集成</strong>: 提供REST API，集成简单，适合快速开发和部署。</li>
</ul>

<p><strong>缺点</strong>:</p>
<ul>
  <li><strong>功能相对基础</strong>: 虽然检索性能出色，但功能相对简单，适合特定场景，可能难以满足更复杂的需求。</li>
</ul>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[引言]]></summary></entry><entry><title type="html">RAG End2end Frame and Vector DB Summary</title><link href="http://localhost:4000/RAG-End2end-Frame-and-Vector-DB-Summary/" rel="alternate" type="text/html" title="RAG End2end Frame and Vector DB Summary" /><published>2024-08-15T16:27:00+01:00</published><updated>2024-08-15T16:27:00+01:00</updated><id>http://localhost:4000/RAG-End2end-Frame-and-Vector-DB-Summary</id><content type="html" xml:base="http://localhost:4000/RAG-End2end-Frame-and-Vector-DB-Summary/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>With the development of large-scale language models (LLMs), Retrieval-Augmented Generation (RAG) technology has shown significant advantages in solving complex natural language processing tasks. By combining the capabilities of text generation and information retrieval, RAG greatly enhances model performance, enabling it to provide more accurate and contextually relevant answers in knowledge-intensive tasks. This technology has been widely applied not only in question-answering systems but also in enterprise applications, document management, customer support, and other fields, offering innovative solutions.</p>

<p>However, with the rapid advancement of RAG, numerous frameworks and tools have emerged in the market. Each of these tools has its unique features—some focus on privacy protection, others on optimizing performance and flexibility, while some offer powerful extensibility and integration options. This article provides a comprehensive summary of the current mainstream RAG frameworks and related vector databases, analyzing their strengths and weaknesses to help readers make more informed decisions when selecting and deploying RAG solutions.</p>

<h2 id="rag-frameworks-and-tools">RAG Frameworks and Tools</h2>

<h3 id="1-oobabooga-with-superboogav2">1. Oobabooga with Superboogav2</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/oobabooga/text-generation-webui">Oobabooga GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/oobabooga/text-generation-webui">Oobabooga Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
Oobabooga is an open-source text generation web interface designed to provide a lightweight platform for running various pre-trained large language models (LLMs) on local or remote hosts. Superboogav2, a plugin for Oobabooga, aims to enhance its text generation capabilities. However, this combination has relatively limited functionality for localized Retrieval-Augmented Generation (RAG) applications. Oobabooga is more focused on basic text generation tasks and lacks advanced features for complex document retrieval and question-answering.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>User-Friendly</strong>: Relatively easy to set up and start, making it suitable for beginners.</li>
  <li><strong>Multi-Model Support</strong>: Supports various models and plugins, offering high flexibility for different generation tasks.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Functionality</strong>: Underperforms in complex document retrieval and question-answering tasks, making it difficult to meet advanced RAG requirements.</li>
  <li><strong>Restricted Configuration</strong>: Lacks detailed control over embedding methods and vector storage, making it unsuitable for highly customized applications.</li>
</ul>

<hr />

<h3 id="2-privategpt">2. privateGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/imartinez/privateGPT">privateGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/imartinez/privateGPT">privateGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
privateGPT is a localized RAG framework focused on privacy protection, allowing users to perform question-answering on private documents in an offline environment. This framework is particularly suitable for users with strict data privacy and security requirements. privateGPT supports running entirely locally, ensuring that all operations are conducted offline. However, its architectural design limits its extensibility, leading to underperformance in more complex tasks.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Privacy Protection</strong>: Operates entirely offline, ensuring that data privacy is not compromised by external threats.</li>
  <li><strong>Easy Deployment</strong>: Can be quickly deployed and run in a local environment without the need for an internet connection.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Single Vector Store</strong>: Supports only a single vector store, making it inconvenient to manage multiple document sets.</li>
  <li><strong>Non-Removable Documents</strong>: Once added to the vector store, documents cannot be removed, leading to management challenges.</li>
  <li><strong>Complex GPU Support</strong>: GPU utilization is complex and may lead to performance issues, particularly in lower-end hardware setups.</li>
  <li><strong>Cumbersome Configuration</strong>: Changing models requires manual configuration file edits and restarting the service, lacking a user-friendly configuration interface.</li>
</ul>

<hr />

<h3 id="3-localgpt">3. localGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/PromtEngineer/localGPT">localGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/PromtEngineer/localGPT">localGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
localGPT is a tool dedicated to providing localized RAG capabilities, allowing users to run GPT models locally and perform retrieval and question-answering with private documents. While localGPT has certain advantages in ensuring data security, its user experience primarily relies on the CLI (Command Line Interface), making it less accessible to users unfamiliar with command line operations. Additionally, localGPT’s flexibility in model and embedding configurations is somewhat limited.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Localized Execution</strong>: Supports running RAG applications in a local environment, ensuring data is not exposed externally.</li>
  <li><strong>Flexible Configuration</strong>: Allows users to change embedding methods through code, providing a degree of flexibility, though the process is complex.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Poor User Experience</strong>: All operations must be performed via CLI, lacking an intuitive graphical user interface, raising the barrier to use.</li>
  <li><strong>Complex Model Management</strong>: Changing models and embedding methods requires manual code edits and service restarts, making operations less straightforward.</li>
</ul>

<hr />

<h3 id="4-lmstudio">4. LMStudio</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/lmstudio-ai">LMStudio GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://lmstudio.ai/">LMStudio Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
LMStudio is a powerful text generation platform offering a user-friendly GUI for managing, downloading, and switching large language models. It provides a straightforward model management experience; however, it lacks interaction capabilities with private documents, limiting its utility in RAG applications. Nonetheless, LMStudio remains an excellent tool, especially for users focused on text generation rather than document retrieval.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Powerful GUI</strong>: Allows users to easily manage and switch models through a graphical interface, greatly simplifying the operational process.</li>
  <li><strong>Multi-Model Support</strong>: Supports managing and using various large language models, offering high flexibility.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Lacks Document Interaction</strong>: Does not support interaction with documents, making it less suitable for RAG applications.</li>
  <li><strong>GGUF Model Limitation</strong>: Only supports GGUF format models, which may limit its performance in specific tasks.</li>
</ul>

<hr />

<h3 id="5-ollama">5. OLlama</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/jmorganca/ollama">OLlama GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/jmorganca/ollama">OLlama Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
OLlama is a localized chat framework designed specifically for Mac users, fully leveraging Mac hardware optimization. It supports running large language models locally, providing a responsive chat experience. However, since it is limited to the Mac platform, it cannot meet the needs of Windows users, particularly those looking to utilize high-performance GPUs. Additionally, OLlama’s extensibility is somewhat restricted.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Mac Optimization</strong>: Leverages Mac hardware to provide efficient local chat functionality.</li>
  <li><strong>Ease of Use</strong>: For Mac users, deployment and usage are very convenient, requiring minimal configuration.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Platform Limitation</strong>: Supports only Mac systems, making it unfriendly to Windows and other operating system users, particularly those unable to utilize high-performance GPUs.</li>
  <li><strong>Limited Extensibility</strong>: Platform limitations restrict its broad application in other operating systems or more complex scenarios.</li>
</ul>

<hr />

<h3 id="6-langchain">6. LangChain</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/hwchase17/langchain">LangChain GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/hwchase17/langchain">LangChain Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
LangChain is a framework for building large language model applications, offering a rich set of tools and integration options to help developers create complex language model-based applications. While LangChain is powerful, it is more suitable as a toolkit rather than a complete RAG solution. For users seeking an all-in-one solution, LangChain’s flexibility might become a burden.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Powerful Toolset</strong>: Provides a wide range of APIs and integration options, suitable for developing complex language model applications.</li>
  <li><strong>High Flexibility</strong>: Allows developers to customize applications according to their needs, offering significant design freedom.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Steep Learning Curve</strong>: Due to its extensive functionality, beginners may find it challenging to get started, requiring substantial time and effort to learn and master.</li>
  <li><strong>Not Ideal as a Single Solution</strong>: More suited as a toolkit rather than a complete RAG application, making it difficult to directly apply in production environments.</li>
</ul>

<hr />

<h3 id="7-memgpt">7. MemGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/brown-iv-lab/memgpt">MemGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/brown-iv-lab/memgpt">MemGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
MemGPT is a relatively new project aiming to enhance GPT model performance by integrating memory mechanisms. Although the project is still under development and testing, it offers an interesting perspective on RAG, potentially paving the way for future applications. The specific performance and applicability of MemGPT require further evaluation, but its innovation offers promising potential for future RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Innovative</strong>: Introduces memory mechanisms with the potential to enhance long-term memory and performance, particularly in complex dialogues and document retrieval tasks.</li>
  <li><strong>Potential Applications</strong>: May perform well in the RAG field in the future, especially as the technology further develops and matures.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Still in Development</strong>: Features and performance are not yet mature, requiring further testing and validation; current stability and practicality are uncertain.</li>
  <li><strong>Unknown Usability</strong>: As the project is still in its early stages, it lacks clear documentation and user cases, potentially limiting user experience.</li>
</ul>

<hr />

<h3 id="8-autogpt">8. AutoGPT</h3>

<ul>
  <li><strong>Source Code</strong>: [AutoGPT GitHub](https://github</li>
</ul>

<p>.com/Significant-Gravitas/Auto-GPT)</p>
<ul>
  <li><strong>Website</strong>: <a href="https://github.com/Significant-Gravitas/Auto-GPT">AutoGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
AutoGPT is an autonomous GPT system capable of completing a series of complex tasks, including RAG. In this regard, it is seen as pioneering work, attempting to build AI tools with autonomous capabilities. Nevertheless, AutoGPT’s embedding settings are unchangeable, limiting users’ customization capabilities, especially in specific RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Automation Capability</strong>: Capable of autonomously completing complex task chains, reducing user intervention, and suitable for highly automated application scenarios.</li>
  <li><strong>Pioneering Innovation</strong>: Represents a new exploration direction for automated AI systems, potentially leading to further RAG development in the future.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Configuration</strong>: Embedding settings cannot be changed, restricting the possibility of personalized configuration, making it challenging to meet specific application requirements.</li>
  <li><strong>Complex System</strong>: The system’s complexity is high, possibly requiring users to have a high level of technical expertise to fully utilize its functions, increasing the difficulty of entry.</li>
</ul>

<hr />

<h3 id="9-gpt4all">9. GPT4All</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/nomic-ai/gpt4all">GPT4All GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/nomic-ai/gpt4all">GPT4All Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
GPT4All is an open-source project aimed at providing users with a localized GPT model interaction experience. Its goal is to make large language models available on local computing devices without relying on cloud services. While GPT4All excels at reducing cloud dependency, its current functionality is relatively basic, making it more suitable for basic model interaction rather than a complete RAG application solution.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Localized Execution</strong>: Supports running on local computing devices, reducing dependency on cloud services and enhancing data security and privacy.</li>
  <li><strong>Open Source</strong>: Fully open-source, allowing users to develop and customize it as needed, offering high flexibility.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Functionality</strong>: Currently basic, making it challenging to support complex RAG applications, requiring further refinement and expansion.</li>
  <li><strong>Performance Uncertain</strong>: As the project is still under development, actual performance and applicability remain to be verified, potentially involving some uncertainties.</li>
</ul>

<hr />

<h3 id="10-chatdocs">10. ChatDocs</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/marella/chatdocs">ChatDocs GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/marella/chatdocs">ChatDocs Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
ChatDocs is a derivative project of privateGPT, aiming to improve GPU support and GPTQ model integration. Compared to privateGPT, ChatDocs offers more configuration options, especially in embedding settings. However, these settings still need to be manually modified via files, lacking intuitive GUI support.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Enhanced GPU Support</strong>: Pre-configured with GPU and GPTQ models, significantly improving performance, especially in large-scale data processing.</li>
  <li><strong>Customizable Embedding Settings</strong>: Allows users to change embedding settings, though manual operation is required, providing a degree of flexibility.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Community Support</strong>: Low star count on GitHub suggests low community engagement, potentially affecting user support and assistance.</li>
  <li><strong>Average User Experience</strong>: Although functionality is enhanced, operations still rely on file editing and command lines, making the user experience less friendly, possibly affecting adoption.</li>
</ul>

<hr />

<h3 id="11-docsgpt">11. DocsGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/arc53/DocsGPT">DocsGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/arc53/DocsGPT">DocsGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
DocsGPT is a system focused on document question-answering, designed to extract answers from documents using GPT models. However, its generation speed is slow, and it does not support GPU, limiting its performance in large-scale data processing. It is better suited for small-scale, non-real-time document query tasks.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Specialized Document Q&amp;A</strong>: Optimized for document retrieval and question-answering, suitable for specific applications, especially small-scale knowledge management and query tasks.</li>
  <li><strong>Simple to Use</strong>: For basic document question-answering tasks, the operation is relatively simple, suitable for non-technical users.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Performance</strong>: Due to the lack of GPU support, the generation speed is slow, making it difficult to handle large-scale or real-time tasks, with limited performance in complex scenarios.</li>
  <li><strong>Insufficient Scalability</strong>: Performs poorly in handling complex or large-scale document collections, making it difficult to adapt to diverse application needs.</li>
</ul>

<hr />

<h3 id="12-auto-rag">12. Auto RAG</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/IDSCETHZurich/AutoRAG">Auto RAG GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/IDSCETHZurich/AutoRAG">Auto RAG Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
Auto RAG is an automated RAG pipeline selection tool designed to help users choose the best RAG solution based on specific needs. It can automatically generate and select the optimal retrieval-augmented generation strategy based on input data. However, this tool requires a high level of technical expertise from users and requires the use or creation of datasets to be effectively utilized.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Intelligent Pipeline Selection</strong>: Capable of automatically selecting and configuring the best RAG strategy, reducing user manual intervention, and increasing system adaptability and flexibility.</li>
  <li><strong>Targeted Approach</strong>: Provides optimized RAG solutions for specific application scenarios, enhancing application effectiveness and efficiency.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complex Usage</strong>: Requires users to have a high level of technical expertise, making the entry barrier high and unsuitable for users with weaker technical skills.</li>
  <li><strong>Dataset Dependency</strong>: Requires the use or creation of datasets to start, making the operational process more cumbersome, potentially affecting user experience.</li>
</ul>

<hr />

<h2 id="vector-databases">Vector Databases</h2>

<h3 id="1-neo4j">1. Neo4j</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/neo4j/neo4j">Neo4j GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://neo4j.com/">Neo4j Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Neo4j is a graph database specifically designed for handling complex relational data and is widely used in social network analysis, recommendation systems, and other fields. Although it can be used in some RAG scenarios, its characteristics and architecture as a graph database lead to slower performance when handling large-scale vector data, and it supports only limited structure types.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Powerful Relational Data Handling</strong>: Excels in modeling and querying complex relational data, especially suited for applications such as network analysis and recommendation systems.</li>
  <li><strong>Graph Query Language</strong>: Supports Cypher, a query language designed specifically for graph databases, providing powerful data manipulation capabilities.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Performance Issues</strong>: Performance is suboptimal when handling large-scale data, especially vector data.</li>
  <li><strong>Limited Support</strong>: Supports only limited data structure types, imposing certain limitations on its application scenarios.</li>
</ul>

<hr />

<h3 id="2-chroma">2. Chroma</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/chroma-core/chroma">Chroma GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.trychroma.com/">Chroma Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Chroma is a modern vector database specifically designed to simplify vector storage and retrieval. It supports multimodal data and offers rich APIs and built-in embedding functions, making it suitable for rapidly building and scaling RAG applications. Chroma aims to provide simple and easy-to-use configuration options, helping developers quickly implement vector data storage and retrieval.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Easy to Install</strong>: Installation and configuration are relatively simple, relying on Docker or Python, making it easy to quickly deploy and use.</li>
  <li><strong>Highly Configurable</strong>: Offers a wide range of configuration options to meet the needs of different application scenarios.</li>
  <li><strong>Multimodal Support</strong>: Supports the storage and retrieval of multimodal data and offers built-in embedding functions, making it suitable for complex RAG applications.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Docker Dependency</strong>: Requires Docker or Python environment to run, which may increase deployment complexity, especially among non-technical users.</li>
</ul>

<hr />

<h3 id="3-lancedb">3. LanceDB</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/lancedb/lance">LanceDB GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.lancedb.com/">LanceDB Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
LanceDB is a database designed specifically for vector data, known for its extremely fast speed and simple API. It can run on local machines and supports data loading from disk. Even with over a million records, LanceDB’s retrieval speed remains very fast, making it an excellent choice, especially for local applications requiring rapid retrieval.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Fast Speed</strong>: Maintains very fast retrieval speed even when handling large-scale data, making it suitable for applications with high real-time requirements.</li>
  <li><strong>Simple to Use</strong>: Provides a simple and straightforward API, making it easy to integrate and use, reducing development difficulty.</li>
  <li><strong>Local Operation</strong>: Supports running on local machines and loading data from disk, making it suitable for scenarios with large data volumes and requiring efficient retrieval.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Functionality</strong>: Although it excels in retrieval performance, it may fall short in handling more complex application scenarios, offering relatively basic functionality.</li>
</ul>

<hr />

<h3 id="4-pinecone">4. Pinecone</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/pinecone-io">Pinecone GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.pinecone.io/">Pinecone Homepage</a></li>
</ul>

<dl>
  <dt><strong>Introduction</strong></dt>
  <dd>
    <p>Pinecone is a cloud-native vector database designed for large-scale vector retrieval and similarity search. It provides a simple, easy-to-use API and supports fully managed services, making it very suitable for beginners or small-scale projects. However, Pinecone’s retrieval speed may be slower in some application scenarios, and its reliance on cloud services makes it less suitable for users requiring localized solutions.</p>
  </dd>
</dl>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Simple API</strong>: Offers a simple and easy-to-use API, facilitating quick adoption and reducing the difficulty of development and deployment.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Slower Performance</strong>: Retrieval speed may not meet expectations in some complex scenarios, affecting user experience.</li>
  <li><strong>Cloud Dependency</strong>: Primarily relies on cloud services, making it less suitable for users requiring localized or offline solutions.</li>
</ul>

<hr />

<h3 id="5-astradb">5. AstraDB</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/datastax/astra">AstraDB GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.datastax.com/products/datastax-astra">AstraDB Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
AstraDB, provided by DataStax, is a cloud database service built on Apache Cassandra, designed to offer highly flexible and fast query performance. AstraDB performs exceptionally well when handling large-scale distributed data and is suitable for applications requiring a serverless architecture. However, due to its powerful features, the learning curve is steep, and it may take considerable time to master.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>High Performance</strong>: Performs exceptionally well in distributed environments, supporting efficient querying and data operations, making it suitable for handling large-scale data.</li>
  <li><strong>High Flexibility</strong>: Supports various data models, adapting to complex application needs.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complexity</strong>: The extensive functionality leads to a high learning curve, potentially requiring significant time to master and fully utilize its capabilities.</li>
</ul>

<hr />

<h3 id="other-commonly-used-rag-vector-databases">Other Commonly Used RAG Vector Databases</h3>

<h3 id="6-milvus">6. Milvus</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/milvus-io/milvus">Milvus GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://milvus.io/">Milvus Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Milvus is an open-source vector database specifically designed for large-scale vector data retrieval and management. It supports various index types and can handle large-scale, high-dimensional data. Milvus offers high scalability, making it particularly suitable for applications requiring the processing of billions of vectors. Due to its powerful features, Milvus has become widely used in RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>High Scalability</strong>: Capable of handling massive vector data, supporting various index types, and suitable for handling high-dimensional, complex datasets.</li>
  <li><strong>Open Source</strong>: Fully open-source, offering flexible deployment options, suitable for a wide range of use cases.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complex Deployment</strong>: Deployment and maintenance can be complex in large-scale environments, requiring high technical capability.</li>
</ul>

<hr />

<h3 id="7-weaviate">7. Weaviate</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/semi-technologies/weaviate">Weaviate GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://weaviate.io/">Weaviate Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Weaviate is an open-source vector database supporting AI model-based automatic classification and similarity search. It has a highly scalable architecture, making it easy to integrate into existing systems. Weaviate supports multimodal data and offers a rich plugin system, making it suitable for applications requiring high customization. Its flexible architecture makes it an ideal choice for building complex RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Highly Scalable</strong>: Supports multimodal data with a flexible plugin system, offering strong adaptability.</li>
  <li><strong>AI Integration</strong>: Supports AI model-driven automatic classification and search, enhancing data processing and retrieval intelligence.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Steep Learning Curve</strong>: Due to its rich and complex functionality, beginners may need more time to master its usage.</li>
</ul>

<hr />

<h3 id="8-faiss">8. Faiss</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/facebookresearch/faiss">Faiss GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://faiss.ai/">Faiss Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Faiss, developed by Facebook AI Research, is a vector similarity search library specifically designed for efficient vector similarity search. It can run on both CPU and GPU, making it suitable for handling large-scale, high-dimensional datasets. Faiss is a very popular choice in RAG applications, especially in scenarios requiring high performance. Although its performance is powerful, as a library, integrating it into existing systems may require additional development work.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Efficient Search Performance</strong>: Performs exceptionally well when handling large-scale, high-dimensional vector data, making it suitable for high-performance applications.</li>
  <li><strong>GPU Support</strong>: Supports running on GPU, significantly enhancing processing speed, making it suitable for complex tasks requiring efficient processing.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complex Integration</strong>: As a library rather than a complete database solution, integrating it into systems may require additional development work, increasing the difficulty of use.</li>
</ul>

<hr />

<h3 id="9-qdrant">9. Qdrant</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/qdrant/qdrant">Qdrant GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://qdrant.tech/">Qdrant Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Qdrant is an open-source vector database focused on fast and efficient vector retrieval. It supports access through REST API, making it easy to integrate into various applications. Qdrant provides strong vector search and filtering capabilities, suitable for applications such as real-time recommendation and personalized search. Due to its simple design and efficient performance, Qdrant has become a popular choice for building real-time RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Efficient Retrieval</strong>: Supports fast vector retrieval and filtering, making it suitable for applications with high real-time requirements.</li>
  <li><strong>Easy Integration</strong>: Provides a REST API, making integration simple and suitable for rapid development and deployment.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Relatively Basic Functionality</strong>: While it excels in retrieval performance, its functionality is relatively simple, suitable for specific scenarios, and may struggle to meet more complex needs.</li>
</ul>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">TableRAG</title><link href="http://localhost:4000/TableRAG-RAG-for-Table-Data/" rel="alternate" type="text/html" title="TableRAG" /><published>2024-08-15T14:30:00+01:00</published><updated>2024-08-15T14:30:00+01:00</updated><id>http://localhost:4000/TableRAG-RAG-for-Table-Data</id><content type="html" xml:base="http://localhost:4000/TableRAG-RAG-for-Table-Data/"><![CDATA[<h1 id="background">Background</h1>

<p>Tables, as a fundamental and widely used semi-structured data type, are prevalent in relational databases, spreadsheet applications, and programming languages used for data processing. They cover a range of fields such as financial analysis (Zhang et al., 2020; Li et al., 2022), risk management (Babaev et al., 2019), and healthcare analytics. In these applications, table-based question answering (TableQA) is a key downstream task for reasoning over tabular data (Ye et al., 2023a; Cheng et al., 2023).</p>

<p>The goal of TableQA is to enable computers to understand human queries about table contents and respond with natural language answers. With the rapid development of large-scale language models (LLMs) in recent years, TableQA has emerged as an important subfield and has made significant progress (Ray, 2023). Currently, most research leveraging LLMs for TableQA is based on single tables (Li et al., 2023). These approaches typically involve preprocessing the table, then inputting the question and the table into the LLM, focusing on improving the LLM’s understanding of table structure. Such methods are primarily applied in practical contexts within the financial sector, such as financial table question answering, financial audit table processing (Zhu et al., 2021), and financial numerical reasoning (Chen et al., 2021; Chen et al., 2020). However, in real-world scenarios, the challenge often involves a set of tables rather than a single table, where users may pose arbitrary questions related to multiple tables. In these cases, the LLM needs not only to input tables one by one but also to retrieve relevant tables from a large collection and provide answers. However, research in this area is still relatively lacking, and our research aims to fill this gap.</p>

<p>Fine-tuning large-scale language models is a common approach to address the challenges of TableQA, but this method requires large amounts of domain-specific labeled data and significant computational resources. Moreover, most models, when handling domain-specific and complex tabular data, often overly rely on pre-trained knowledge, leading to hallucinations and incorrect information (Ray, 2023; Gao et al., 2023).</p>

<p>To address these challenges, retrieval-augmented generation (RAG) methods combine retrieval mechanisms with generative models, referencing external knowledge bases to reduce model hallucination and improve the accuracy of domain-specific question answering while reducing resource consumption (Gao et al., 2023). However, despite the strong performance of RAG in handling unstructured text data, there are several challenges when applying it to semi-structured tabular data. Specifically, we identified the following three limitations:</p>

<ol>
  <li>The tables required to answer questions may be very large, containing a significant amount of noise unrelated to the query (Lu et al., 2024). This not only increases unnecessary computation but also affects the accuracy of retrieval and the generator’s response. To address this issue, we can employ table sampling (Sui et al., 2024) or table filtering methods to retrieve relevant rows and columns, thereby generating the most relevant sub-tables (Jiang et al., 2023).</li>
  <li>The raw content of the tables may include information that needs further clarification, such as domain-specific terms or abbreviations (Sui et al., 2024). These domain-specific details can lead to misunderstandings or biases by the generator. To solve this problem, we can use external knowledge bases to provide additional context for the tables (Bian et al., 2023), or generate term explanations through LLMs, a process we call the table clarifier.</li>
  <li>Tables often contain various types of information across different columns, and traditional retrieval methods such as BM25 (Robertson et al., 2009) or Dense Passage Retriever (DPR) (Karpukhin, et al., 2020) may overlook table details, impacting the generated results. We can address this issue by employing the ColBERT model as a retriever, which encodes text at the token level, enabling more fine-grained retrieval (Li et al., 2023).</li>
</ol>

<p>By incorporating these improvements, our research aims to provide a more effective solution for handling large-scale TableQA tasks involving multiple tables, addressing more complex real-world scenarios.</p>

<h1 id="overview">Overview</h1>

<p>In tackling complex TableQA tasks, we designed a system that combines the latest large-scale language models (LLMs) with retrieval-augmented generation (RAG) techniques to handle multi-table issues in practical applications. Below is a graphical illustration and introduction of the core ideas of the project.</p>

<h3 id="rag-based-multi-table-qa-system-architecture">RAG-Based Multi-Table QA System Architecture</h3>

<p><img src="/insert_images/The_overall_structure.png" alt="The overall structure" /></p>

<p>In this system architecture, our goal is to retrieve relevant information from multiple tables and generate accurate natural language answers. The process can be divided into the following key steps:</p>

<ol>
  <li><strong>Table Processing and Text Segmentation</strong>: First, the raw table data undergoes preprocessing and text segmentation, converting the table content into multiple text segments. The purpose of this is to make the data easier to handle and more efficiently retrieved for queries.</li>
  <li><strong>Vector Database Construction</strong>: The segmented text and table fragments are embedded and stored in a vector database. The vector database, through efficient vectorized retrieval techniques, can quickly find the content fragments related to the query.</li>
  <li><strong>Query and Retrieval</strong>: When a user poses a question, the retriever searches the vector database for table fragments related to the question. In this process, we introduce the ColBERT model to enhance the accuracy of the retriever. ColBERT encodes text at the token level, allowing for more fine-grained retrieval, thus improving the relevance of the retrieval results.</li>
  <li><strong>Answer Generation</strong>: The retrieved relevant text fragments and the user’s question are input into a large-scale language model (LLM), which generates the final natural language answer.</li>
</ol>

<h3 id="enhanced-mechanisms-for-multi-table-qa">Enhanced Mechanisms for Multi-Table QA</h3>

<p><img src="/insert_images/Enhancement.png" alt="Enhancement" /></p>

<p>When handling data from multiple tables, our system introduces various enhancement mechanisms to improve the accuracy and effectiveness of the QA task.</p>

<ol>
  <li>
    <p><strong>Semantic-Based Table Filter</strong>: When dealing with a large number of tables, the system first filters the tables based on semantic analysis to select the most relevant ones. In this process, we used two different models for text embedding and comparison:</p>

    <p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

    <ul>
      <li><strong>Using OpenAI’s Embedding Model</strong>: We used OpenAI’s embedding model to embed the table content, then stored and retrieved the embedded data using the FAISS vector database, returning the table rows and columns most relevant to the query.</li>
      <li><strong>Using the ColBERT Model</strong>: We also used the ColBERT model to embed the table content and performed more fine-grained retrieval during the search process. By comparing the results with those of the OpenAI Embedding model, we were able to select the semantic filtering method best suited to the specific task.</li>
    </ul>
  </li>
  <li><strong>LLM-Based Filter</strong>: In addition to the semantic filter, we also used a large-scale language model (LLM) for intelligent table filtering. By analyzing the deep semantic relationship between the table content and the query, the LLM can more precisely select the most relevant table fragments, further improving retrieval accuracy.</li>
  <li>
    <p><strong>Table Clarifier</strong>: Based on the filtered tables, we introduced two clarification modules:</p>

    <p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

    <ul>
      <li><strong>Term Clarification</strong>: For domain-specific terms or abbreviations in the table, we called the LLM for explanation, helping the LLM better understand the question and table content.</li>
      <li><strong>Wiki-Based Summary Generation</strong>: First, we search Wikipedia for metadata related to the table title, header, or context. Then, we package this Wikipedia data with the original table context information to generate a summary related to the query or clarification statement. This approach not only improves the accuracy of information but also provides more comprehensive background support for understanding complex tables.</li>
    </ul>
  </li>
</ol>

<p>The above architecture and enhancement mechanisms effectively address the challenges present in current TableQA tasks, especially in the real-world application of multi-table environments. By combining advanced retrieval technology, semantic and LLM filtering, and large-scale language models, our system can quickly find relevant information from a large number of tables and generate accurate answers, providing strong support for various complex data analysis tasks.</p>

<h1 id="dataset-selection">Dataset Selection</h1>

<h2 id="tablefact"><a href="https://tabfact.github.io/">Tablefact</a></h2>

<p>In the existing TableQA datasets, we have conducted extensive attempts and research. For detailed dataset organization, please refer to my other blog: <a href="https://yuhangwuai.github.io/2024/08/14/Dataset-for-Question-Answering/">Dataset for Question Answering</a>. Through these experiences, we found that when using datasets for retrieval-augmented generation in TableQA, we mainly face the following issues:</p>

<ol>
  <li><strong>Short Questions Lead to Poor Recall</strong>:
    <ul>
      <li>Questions in many QA datasets are typically very short, consisting of only a few words. Such short queries often lead to poor recall of relevant tables in similarity-based retrieval or other dense retrieval processes.</li>
    </ul>
  </li>
  <li><strong>Uniform Question Format</strong>:
    <ul>
      <li>Questions often begin with similar interrogative words and conjunctions. For example, in the SQA dataset, questions like “What are the schools?” and “What are the countries?” involve completely different content, but their opening “What are the” is the same. If the dataset contains nearly 500 questions beginning with “What are the,” this format repetition makes it very difficult to accurately recall relevant tables.</li>
    </ul>
  </li>
  <li><strong>Lack of Table Titles</strong>:
    <ul>
      <li>A large number of QA datasets lack table titles, and typically, one table corresponds to one question, with no retrieval phase involved. In such cases, tables and questions are directly input together into the model. However, in the absence of table titles, accurately retrieving relevant tables from a large number of tables becomes much more difficult.</li>
    </ul>
  </li>
</ol>

<p>Based on these challenges, in our initial experiments, the TableFact dataset was our primary foundational dataset. The TableFact dataset focuses on the task of table fact verification, effectively evaluating a model’s reasoning and judgment capabilities.</p>

<p>TableFact is a large-scale dataset containing 117,854 manually annotated statements related to 16,573 Wikipedia tables. The relationships between these tables and statements are classified as “ENTAILMENT” and “REFUTATION.” This dataset first proposed evaluating language reasoning ability on structured data, involving a mix of symbolic and semantic reasoning skills. This complexity makes TableFact an ideal dataset for evaluating deep learning models’ ability to handle tasks that involve both semantic and symbolic reasoning.</p>

<table>
  <thead>
    <tr>
      <th>Channel</th>
      <th>Sentence</th>
      <th>Table</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Simple (r1)</td>
      <td>50,244</td>
      <td>9,189</td>
    </tr>
    <tr>
      <td>Complex (r2)</td>
      <td>68,031</td>
      <td>7,392</td>
    </tr>
    <tr>
      <td>Total (r1 + r2)</td>
      <td>118,275</td>
      <td>16,573</td>
    </tr>
    <tr>
      <td>Split</td>
      <td>Sentence</td>
      <td>Table</td>
    </tr>
    <tr>
      <td>Train</td>
      <td>92,283</td>
      <td>13,182</td>
    </tr>
    <tr>
      <td>Val</td>
      <td>12,792</td>
      <td>1,696</td>
    </tr>
    <tr>
      <td>Test</td>
      <td>12,779</td>
      <td>1,695</td>
    </tr>
  </tbody>
</table>

<p>An example from this dataset is as follows:</p>

<p><img src="/insert_images/tablefact.png" alt="Tablefact sample instances（Chen et al., 2019）" /></p>

<p>TableFact sample instances（Chen et al., 2019）</p>

<p>The primary advantage of the TableFact dataset lies in its focus on the task of table fact verification, effectively evaluating the model’s reasoning and judgment capabilities. The specific task is: given a table and a statement, the model is required to judge whether the statement is consistent with the information in the table. The model needs to deeply reason over the table content and mark the statement as “True” or “False.”</p>

<p>The TableFact dataset not only includes a large number of complex table and statement pairs, covering a wide range of domains and topics but also effectively simulates the multi-table QA scenarios that may be encountered in real-world situations. This provides us with a challenging test platform that helps us comprehensively evaluate and optimize our multi-table QA system. Another important reason for using this dataset is that it allows us to better control the output of the LLM, enabling us to precisely evaluate the model’s performance.</p>

<p><em>The reasons we chose to use the <a href="https://tabfact.github.io/">TableFact dataset</a> are as follows:</em></p>

<ol>
  <li><strong>Pure Table Dataset</strong>: TableFact data is mainly presented in tabular form, with relatively low similarity between statements, making it relatively easier to accurately locate relevant information during retrieval.</li>
  <li><strong>Clear Classification Task</strong>: The TableFact dataset’s task is clear: judging the truthfulness of statements. This task setting makes it easier to control the output of large models during answer generation, allowing us to more accurately evaluate the model’s reasoning ability.</li>
</ol>

<h2 id="feverous"><a href="https://fever.ai/dataset/feverous.html">Feverous</a></h2>

<p>After using TableFact, we chose the FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) dataset. FEVEROUS is a large-scale dataset specifically designed for fact verification tasks, which, unlike TableFact, contains not only structured tabular data but also unstructured text data. This makes FEVEROUS more complex and challenging in terms of retrieval and reasoning.</p>

<p><img src="/insert_images/feverous.png" alt="Feverous sample instances(Aly et al., 2021)" />
Feverous sample instances(Aly et al., 2021)</p>

<p>The <a href="https://fever.ai/dataset/feverous.html">FEVEROUS</a> dataset contains over 80,000 table and text paragraph pairs, as well as more than 120,000 associated fact verification questions. When dealing with the FEVEROUS dataset, the model, in addition to judging the truthfulness of the statement, must also choose between three options: <strong>Supported</strong>, <strong>Refuted</strong>, or <strong>Not Enough Information</strong>. This three-choice task setup further increases the complexity of the model’s reasoning, making FEVEROUS a more comprehensive dataset for evaluating the model’s reasoning ability, especially in the integration and judgment of multi-source information.</p>

<p><em>Reasons for choosing <a href="https://fever.ai/dataset/feverous.html">FEVEROUS</a>:</em></p>

<ul>
  <li>Combining structured and unstructured data increases the difficulty of the model’s reasoning.</li>
  <li>The three-choice task setup allows for a better evaluation of the model’s performance in complex reasoning tasks.</li>
</ul>

<h2 id="sqa"><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></h2>

<p>In further expanding our experiments, we introduced the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA (Sequential Question Answering)</a> dataset. The SQA dataset is designed to evaluate the model’s performance in complex, multi-step QA scenarios. This dataset contains over 6,000 conversational QA pairs, with each conversation involving multiple related questions that are usually contextually linked to the previous Q&amp;A. Unlike TableFact and FEVEROUS, SQA requires the model to maintain contextual understanding and consistency throughout a continuous Q&amp;A process.</p>

<p>Questions in SQA not only require answering the current question but also require reasoning based on previous Q&amp;A. Moreover, SQA requires the model’s answers to be freeform, covering various formats such as text, numbers, and more. This open-ended QA increases the model’s reasoning complexity and tests the model’s generative capabilities in handling freeform answers.</p>

<p><img src="/insert_images/sqa.png" alt="SQA sample instances (Lyyer et al., 2017)" /></p>

<p>SQA sample instances (Lyyer et al., 2017)</p>

<p><em>Reasons for choosing <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a>:</em></p>

<ul>
  <li>Purely structured data, temporarily not involving the integration of two types of data.</li>
  <li>Focuses on multi-step QA, increasing the challenges for the model in handling conversational context and continuous reasoning.</li>
  <li>The introduction of freeform answers tests the model’s performance in open-ended QA tasks.</li>
</ul>

<h2 id="hybridqa"><a href="https://hybridqa.github.io/">HybridQA</a></h2>

<p>Finally, we chose the HybridQA dataset to further enhance the evaluation of the model’s multi-modal information processing capabilities. <a href="https://hybridqa.github.io/">HybridQA</a> is a dataset that integrates tabular and textual information, designed to test the model’s comprehensive QA capabilities over multi-modal information. The dataset contains 6,241 QA pairs, with each question involving content from multiple different information sources, including tables and associated unstructured text information.</p>

<p>The unique aspect of HybridQA is that the model not only needs to extract and integrate relevant information from multiple sources but also needs to involve numerical reasoning steps during the answering process. This multi-modal, multi-step QA format requires the model to excel in complex tasks, especially in cross-modal information integration and numerical reasoning.</p>

<p><img src="/insert_images/hybridqa.png" alt="HybridQA sample instances (Chen et al., 2020)" />
HybridQA sample instances (Chen et al., 2020)</p>

<p><em>Reasons for choosing <a href="https://hybridqa.github.io/">HybridQA</a>:</em></p>

<ul>
  <li>Involves both tabular and textual types of information, further testing the model’s cross-modal integration capabilities.</li>
  <li>Complex QA formats and numerical reasoning steps provide higher challenges to evaluate the model’s comprehensive performance in handling multi-source information.</li>
  <li>The introduction of freeform answers tests the model’s performance in open-ended QA tasks.</li>
</ul>

<h1 id="implementation-plan">Implementation Plan</h1>

<h2 id="part-1-table-filter">Part 1: Table Filter</h2>

<p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

<ol>
  <li><strong>Semantic-Based Filtering</strong>
    <ul>
      <li><strong>Generating Embedding Vectors</strong>: Generate semantic embedding vectors for each row and column in the table, as well as for the user’s query. We implemented this process in two ways:
        <ol>
          <li><strong>Vector Database Matching</strong>: Use OpenAI or other embedding models to generate embedding vectors, then calculate similarities using a vector database like FAISS, quickly returning the rows and columns related to the query.</li>
          <li><strong>Fine-Grained Matching</strong>: Use the ColBERT pre-trained model to embed and match table data and queries for more fine-grained matching, selecting the most relevant rows and columns.</li>
        </ol>
      </li>
      <li><strong>Selecting Relevant Rows and Columns</strong>: Based on similarity scores, select the top k rows and columns most relevant to the query to construct new sub-tables.</li>
    </ul>
  </li>
  <li><strong>Large Language Model (LLM) Based Filtering</strong>
    <ul>
      <li><strong>Convert to String</strong>: Convert the query and table content into strings and concatenate them to form a context.</li>
      <li><strong>Call GPT for Filtering</strong>: Use the GPT model to filter and extract rows and columns related to the query, generating the corresponding Python code for filtering. To improve the accuracy and consistency of code generation, we adopted a self-consistency strategy:
        <ol>
          <li><strong>Self-Consistency Strategy</strong>: Have GPT generate the code five times, selecting the most frequently generated code as the final filtering code. If the generated code versions are different, select the result from the first generation.</li>
          <li><strong>Execution and Error Handling</strong>: Execute the final selected code segment to update the table. If an error occurs during code execution, capture the error message and return the original table to ensure the robustness of the process.</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<h3 id="challenges-and-solutions-in-llm-based-filtering">Challenges and Solutions in LLM-Based Filtering</h3>

<p>In the process of table filtering, especially in the LLM-based table filter, the following major challenges exist:</p>

<ol>
  <li>
    <p><strong>Column Name Consistency Issue</strong>: GPT may misinterpret column names when generating filtering code, leading to inconsistencies between the generated code and the original table column names, causing errors. For example, ‘scheduled’, and ‘capacity (mw)’ may be misunderstood as ‘scheduled capacity (mw)’ as a single column name, or the LLM may merge multiple column names into one or incorrectly split a single column name.</p>

    <p><strong>Solution</strong>: To address this issue, the Prompt can explicitly provide the cleaned-up column names as parameters to be passed to GPT, ensuring that the generated code uses column names that are completely consistent with the original table. This approach can fundamentally reduce the occurrence of column name recognition errors.</p>
  </li>
  <li>
    <p><strong>Information Loss Issue</strong>: During LLM filtering, the filtered table may lose critical information needed to answer the question due to over-filtering. This can lead to inaccurate or incorrect answers being generated in subsequent answer generation due to a lack of necessary evidence.</p>

    <p><strong>Solution</strong>: To address this issue, a “conservative filtering” strategy can be adopted, where the LLM only filters out content that it is very certain is unrelated to the statement. If the LLM is uncertain whether some content is related to the statement, it should lean towards retaining this content. This strategy can maximize the retention of potential key evidence, ensuring that the generated answers can be based on complete information, thus improving the accuracy and credibility of the answers.</p>
  </li>
  <li>
    <p><strong>Data Type Mismatch Filtering Issue</strong>: When processing table data, especially when filtering numerical data, mismatches in data types may result in empty or inaccurate filtering results.</p>

    <p><strong>Solution</strong>: Even when processing numerical data, it is recommended to perform filtering using string matching. This approach can avoid filtering errors caused by data type mismatches, thereby improving filtering accuracy and reliability.</p>
  </li>
  <li>
    <p><strong>Effectiveness of Prompt Design</strong>: The design of the Prompt is crucial for ensuring that GPT accurately understands the task and generates correct filtering code. An unclear Prompt may lead to GPT generating code that does not meet expectations.</p>

    <p><strong>Solution</strong>: In designing the Prompt, it should be ensured that it is clear, specific, and contains sufficient context information so that GPT can accurately understand the task requirements. At the same time, the Prompt can be repeatedly tested and adjusted to find the most suitable expression, improving the accuracy of code generation.</p>
  </li>
  <li>
    <p><strong>Code Generation Consistency Issue</strong>: GPT may generate multiple different versions of the code during code generation, leading to inconsistent results.</p>

    <p><strong>Solution</strong>: By using the self-consistency strategy, generating multiple versions of the code and selecting the most frequently occurring version, consistency and reliability of the results can be ensured. If all generated codes are inconsistent, the first generated code can be used with error capture handling to ensure the stability of the process.</p>
  </li>
</ol>

<p>Finally, the detailed settings we used are as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">call_llm_code_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Synthesize code snippet from the table context."""</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: Synthesize code snippet from the table context to select the proper rows and columns for verifying a statement / answering query.
        The generated code must use the exact column names provided, including spaces, capitalization, and punctuation.
        The generated code should treat all data as strings, even if they look like numbers.
        Only filter out rows and columns that are definitely not needed to verify the statement / answering query.

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: The scheduled date for the farm with 17 turbines be 2012.
        Columns: ['wind farm', 'scheduled', 'capacity (mw)', 'turbines', 'type', 'location']
        df = pd.DataFrame(wind farm)
        User 2:
        To verify the statement 'The scheduled date for the farm with 17 turbines be 2012', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'wind farm', 'scheduled', and 'turbines' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['wind farm', 'scheduled', 'turbines']].query("turbines == '17'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: All 12 club play a total of 22 game for the wru division one east.
        Columns: ['club', 'played', 'drawn', 'lost', 'points for', 'points against', 'tries for', 'tries against', 'try bonus', 'losing bonus', 'points']
        df = pd.DataFrame(club)
        User 2:
        To verify the statement 'All 12 club play a total of 22 game for the wru division one east', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'club' and 'played' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['club', 'played']].query("played == '22'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: Touchdown Atlantic, in the category of sporting, be established in 2010.
        Columns: ['event name', 'established', 'category', 'sub category', 'main venue']
        df = pd.DataFrame(event name)
        User 2:
        To verify the statement 'Touchdown Atlantic, in the category of sporting, be established in 2010', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'event name' and 'established' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['event name', 'established']].query("`event name` == 'touchdown atlantic' and established == '2010'")

        Now, generate a code snippet from the table context to select the proper rows and columns to verify the given statement / answering query.
        Use the existing column names from the provided DataFrame.
        The column names in the generated code must match the provided column names exactly, including spaces, capitalization, and punctuation.
        Only filter out rows and columns that are definitely not needed to verify the statement.
        Only return the code. 
        </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">
        </span><span class="se">\n\n</span><span class="s">:
        """</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">USE_SELF_CONSISTENCY</span><span class="p">:</span>
            <span class="n">generated_codes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Generated codes:"</span><span class="p">,</span> <span class="n">generated_codes</span><span class="p">)</span>
            
            <span class="c1"># Find the most common code
</span>            <span class="n">code_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">generated_codes</span><span class="p">)</span>
            <span class="n">most_common_code</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">code_counter</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">most_common_code</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">generated_codes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="part-2-table-clarifier">Part 2: Table Clarifier</h1>

<p>When dealing with complex tabular data, providing clarifications can significantly enhance the understanding of the table’s content. However, selecting the appropriate clarification method is crucial. In the initial design, we attempted to use the Google API to retrieve term explanations and enhance the table content with Wikipedia documents. Specifically, in the initial design, we followed the process outlined below for clarifying the table.</p>

<h2 id="early-method-workflow">Early Method Workflow</h2>

<h3 id="term-clarification"><strong>Term Clarification</strong></h3>

<ul>
  <li>First, a large language model (LLM) analyzes the table content to identify terms that require further explanation.</li>
  <li>For the identified terms, the Google API is used to search for relevant explanations.</li>
  <li>The retrieved explanations are then appended to the table as term clarification information. This process can be implemented using the <code class="language-plaintext highlighter-rouge">GoogleSearchAPIWrapper()</code> in Langchain.</li>
</ul>

<h3 id="wiki-document-clarification"><strong>Wiki Document Clarification</strong></h3>

<ul>
  <li>Based on the table’s title, context, or header information, a Wikipedia query is constructed. For example, if the table header includes “Company Name,” “Revenue,” and “Number of Employees,” a query like “company revenue employees market capitalization” can be constructed.</li>
  <li>The <code class="language-plaintext highlighter-rouge">WikipediaRetriever.get_relevant_documents()</code> in Langchain is then used to retrieve relevant Wikipedia documents.</li>
  <li>Metadata, such as titles, summaries, and links, is extracted from the retrieved documents and combined with the table content as additional clarification data.</li>
</ul>

<p>We used the following Prompt:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">You</span> <span class="n">are</span> <span class="n">an</span> <span class="n">expert</span> <span class="ow">in</span> <span class="n">data</span> <span class="n">analysis</span> <span class="ow">and</span> <span class="n">natural</span> <span class="n">language</span> <span class="n">processing</span><span class="p">.</span> <span class="n">Your</span> <span class="n">task</span> <span class="ow">is</span> <span class="n">to</span> <span class="n">help</span> <span class="n">identify</span> <span class="n">terms</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">table</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span> <span class="k">for</span> <span class="n">better</span> <span class="n">understanding</span><span class="p">.</span> <span class="n">The</span> <span class="n">table</span> <span class="n">contains</span> <span class="n">various</span> <span class="n">fields</span><span class="p">,</span> <span class="n">some</span> <span class="n">of</span> <span class="n">which</span> <span class="n">might</span> <span class="n">include</span> <span class="n">technical</span> <span class="n">jargon</span><span class="p">,</span> <span class="n">abbreviations</span><span class="p">,</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">commonly</span> <span class="n">understood</span> <span class="n">by</span> <span class="n">a</span> <span class="n">general</span> <span class="n">audience</span><span class="p">.</span>

<span class="n">Here</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">table</span><span class="p">:</span>
<span class="p">[</span><span class="n">Insert</span> <span class="n">table</span> <span class="n">here</span><span class="p">]</span>

<span class="n">Please</span> <span class="n">follow</span> <span class="n">these</span> <span class="n">steps</span><span class="p">:</span>
<span class="mf">1.</span> <span class="n">Analyze</span> <span class="n">the</span> <span class="n">content</span> <span class="n">of</span> <span class="n">each</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">table</span><span class="p">.</span>
<span class="mf">2.</span> <span class="n">Identify</span> <span class="nb">any</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="n">technical</span><span class="p">,</span> <span class="n">specialized</span><span class="p">,</span> <span class="ow">or</span> <span class="n">abbreviations</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span><span class="p">.</span>
<span class="mf">3.</span> <span class="n">Generate</span> <span class="n">a</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">these</span> <span class="n">terms</span> <span class="n">along</span> <span class="k">with</span> <span class="n">the</span> <span class="n">corresponding</span> <span class="n">cell</span> <span class="n">reference</span> <span class="p">(</span><span class="n">row</span> <span class="ow">and</span> <span class="n">column</span><span class="p">).</span>

<span class="n">Consider</span> <span class="n">the</span> <span class="n">following</span> <span class="n">when</span> <span class="n">identifying</span> <span class="n">terms</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Technical</span> <span class="n">terms</span> <span class="n">related</span> <span class="n">to</span> <span class="n">specific</span> <span class="n">industries</span> <span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span> <span class="n">finance</span><span class="p">,</span> <span class="n">healthcare</span><span class="p">,</span> <span class="n">technology</span><span class="p">).</span>
<span class="o">-</span> <span class="n">Abbreviations</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">universally</span> <span class="n">known</span><span class="p">.</span>
<span class="o">-</span> <span class="n">Jargon</span> <span class="n">that</span> <span class="n">may</span> <span class="n">be</span> <span class="n">specific</span> <span class="n">to</span> <span class="n">a</span> <span class="n">particular</span> <span class="n">field</span> <span class="ow">or</span> <span class="n">context</span><span class="p">.</span>

<span class="n">Output</span> <span class="n">the</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">need</span> <span class="n">explanation</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">following</span> <span class="nb">format</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="p">[</span><span class="n">Term</span><span class="p">]</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="n">Row</span><span class="p">,</span> <span class="n">Column</span><span class="p">]</span>

<span class="n">Example</span> <span class="n">output</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Revenue</span> <span class="p">(</span><span class="n">million</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Market</span> <span class="n">Cap</span> <span class="p">(</span><span class="n">billion</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="n">Be</span> <span class="n">thorough</span> <span class="ow">and</span> <span class="n">ensure</span> <span class="n">that</span> <span class="nb">all</span> <span class="n">potentially</span> <span class="n">confusing</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">are</span> <span class="n">included</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">list</span><span class="p">.</span>

</code></pre></div></div>
<p>Certainly! Here’s the translated content in academic English in Markdown format:</p>

<hr />

<p>Afterward, we pass it to Lanchain, utilizing the <code class="language-plaintext highlighter-rouge">GoogleSearchAPIWrapper()</code> to perform retrieval, and integrate the results as clarification information.</p>

<p>For the Wikipedia method, we implement it as follows:</p>

<p>For instance, consider the following table:</p>

<table>
  <thead>
    <tr>
      <th>Company Name</th>
      <th>Revenue (Million USD)</th>
      <th>Number of Employees</th>
      <th>Market Cap (Billion USD)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Company A</td>
      <td>1000</td>
      <td>5000</td>
      <td>50</td>
    </tr>
    <tr>
      <td>Company B</td>
      <td>2000</td>
      <td>10000</td>
      <td>100</td>
    </tr>
    <tr>
      <td>Company C</td>
      <td>1500</td>
      <td>7500</td>
      <td>75</td>
    </tr>
  </tbody>
</table>

<p>We construct the query based on the table headers:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"company revenue employees market capitalization"
</code></pre></div></div>

<p>The retrieved information is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="s">"title"</span><span class="p">:</span> <span class="s">"List of largest technology companies by revenue"</span><span class="p">,</span>
    <span class="s">"summary"</span><span class="p">:</span> <span class="s">"This is a list of the largest technology companies in the world by revenue."</span><span class="p">,</span>
    <span class="s">"url"</span><span class="p">:</span> <span class="s">"&lt;https://en.wikipedia.org/wiki/List_of_largest_technology_companies_by_revenue&gt;"</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The above document metadata, combined with the table, serves as the clarification data.</p>

<blockquote>
  <p>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S., &amp; Zhang, D. (2023). Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</p>
</blockquote>

<hr />

<p>This method theoretically helps us acquire rich information resources; however, in practice, it reveals some significant issues.</p>

<p>First, the <strong>accuracy issue of the Google API results</strong>. While retrieving term explanations through the Google API might be effective for handling certain specialized terms that usually have a unique definition, the situation becomes complicated when dealing with acronyms or polysemous words. For example, the acronym “ABC” might correspond to multiple different concepts, such as “American Broadcasting Company” or “Activity-Based Costing,” among others. In such cases, the term explanations retrieved from Google may exhibit inconsistencies, not only failing to achieve the intended enhancement but potentially causing confusion and making the results more complex and unreliable.</p>

<p>Second, the <strong>verbosity issue of the retrieved content</strong>. The content retrieved from Google and the documents returned by Wikipedia may be excessively verbose, containing large amounts of information related to the table content but irrelevant to the actual query needs. These verbose documents, when further processed, might negatively impact the retrieval effectiveness of the data pipeline. Most research currently focuses on feeding each query individually into an LLM or pre-trained model for processing. However, our current task differs, and this approach might lead to suboptimal results. If the documents are too long and contain excessive irrelevant information, it could reduce the accuracy and efficiency of the model, thereby affecting the overall quality of the results.</p>

<h1 id="improving-and-refining-the-table-clarification-strategy">Improving and Refining the Table Clarification Strategy</h1>

<h2 id="precise-optimization-of-the-term-clarification-module">Precise Optimization of the Term Clarification Module</h2>

<p>Based on the above reasons, after extensive literature review and careful consideration, we propose the following two key requirements for table clarification information:</p>

<ol>
  <li>
    <p><strong>Clarification information must enhance the understanding of the table.</strong></p>

    <p>The primary goal of clarification information is to assist the model in better understanding the table content. The added information should be precise and helpful in enabling the model to more accurately grasp the structure and meaning of the table during processing, thereby improving overall comprehension.</p>
  </li>
  <li>
    <p><strong>Clarification information must improve the recall capability related to the table.</strong></p>

    <p>Secondly, clarification information should contribute to enhancing the model’s ability to recall content related to the table. This means that when faced with a query or analysis task, the model should be able to more effectively extract and utilize key information from the table.</p>
  </li>
</ol>

<p>In proposing these requirements, we also identified two situations that must be avoided:</p>

<ol>
  <li>
    <p><strong>Incorrect clarification information that impairs the LLM’s understanding of the table.</strong></p>

    <p>If the clarification information contains errors, it may lead to misinterpretation of the table by the model, thereby reducing its ability to correctly parse the table content. This not only defeats the purpose of providing clarification information but may also cause biases in the model’s output.</p>
  </li>
  <li>
    <p><strong>Excessively lengthy and redundant clarification information that hinders the model’s ability to recall relevant tables.</strong></p>

    <p>Lengthy or redundant information may increase the processing burden on the model, distracting it from the core content, and thus weakening the model’s efficiency and accuracy in recalling relevant table information.</p>
  </li>
</ol>

<h2 id="improvements-to-the-table-clarifier">Improvements to the Table Clarifier</h2>

<p>Based on the analysis of the requirements for table augmentation information and the potential issues, we propose further improvements to optimize the method of augmenting tables. These improvements aim to ensure that the augmentation information enhances both the model’s understanding and the retrieval efficiency of relevant information, thereby avoiding common pitfalls such as misunderstandings and redundancy.</p>

<h3 id="improvements-to-the-term-clarification-module"><strong>Improvements to the Term Clarification Module</strong></h3>

<p>For the term clarification module, we decided to directly utilize the LLM to extract and explain terms from the table, rather than relying on external retrieval through the GoogleSearchAPIWrapper. While this method may not obtain the broader comprehensive information available on the internet, the LLM is already capable of understanding most terms and abbreviations and can provide explanations in context. This approach not only improves the understanding of the table but also effectively avoids potential misleading information and redundancy issues arising from external retrieval, ensuring the precision and conciseness of the augmentation information.</p>

<h3 id="improvements-to-the-wiki-reference-module"><strong>Improvements to the Wiki Reference Module</strong></h3>

<h3 id="1-clarification-of-table-purpose"><strong>1. Clarification of Table Purpose</strong></h3>

<p>We introduced a new piece of clarification information, a brief explanation of the table’s purpose, i.e., what question the table is intended to answer. By generating information based on a clear statement of the table’s purpose, we can significantly improve recall rates when using ColBERT for information retrieval.</p>

<p>Through this method, we achieve an enhancement in the table’s recall ability, ensuring that the model can more accurately extract relevant data when faced with specific queries. The specific prompt and example usage are as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">generate_terms_explanation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">statement</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a statement, and the table's caption. Your task is to identify difficult to understand column names, terms, or abbreviations in the table and provide simple explanations for each. Only explain terms related to the statement.

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: The scheduled date for the farm with 17 turbines be 2012.
        Here is the table caption: Wind Farm Details in Ireland
        Here is the table:
        wind farm

        User 2:
        Explanations:
        "scheduled": "The planned date for the wind farm to be operational.",
        "turbines": "The number of wind turbines in the wind farm."

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: All 12 clubs play a total of 22 games for the WRU Division One East.
        Here is the table caption: WRU Division One East Standings
        Here is the table:
        club

        User 2:
        Explanations:
        "played": "The number of games played by the club.",
        "points for": "The total points scored by the club.",
        "points against": "The total points scored against the club."

        Now, explain the terms in the following table.

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Statement:
        </span><span class="si">{</span><span class="n">statement</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        explanations
        }}
        """</span>
        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>  
</code></pre></div></div>
<h3 id="2-optimization-of-wikipedia-external-information-augmentation"><strong>2. Optimization of Wikipedia External Information Augmentation</strong></h3>

<p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

<ol>
  <li><strong>Initial Retrieval</strong>:
    <ul>
      <li><strong>Wikipedia Retrieval Based on Table Title</strong>: Initially, we use the table title as a keyword to retrieve related augmentation information from Wikipedia.</li>
      <li><strong>Alternative Retrieval</strong>: If the title-based retrieval fails, we use the table header information to conduct the search, providing augmentation information relevant to the table content.</li>
    </ul>
  </li>
  <li><strong>Information Packaging</strong>:
    <ul>
      <li>We extract metadata from the Wikipedia data, but we do not directly incorporate this information into the clarification content to avoid redundancy.</li>
      <li>Instead, we package the Wikipedia metadata, query, table (including the filtered or original table), caption, and context (if available) together, and send it to the LLM for processing. The LLM will then generate a table summary based on this multifaceted information.</li>
    </ul>
  </li>
</ol>

<h3 id="key-considerations">Key Considerations:</h3>

<ul>
  <li><strong>Avoid Directly Revealing the Answer</strong>: When generating the summary, care should be taken to craft a guiding summary that avoids directly disclosing the answer to the question or providing an outright solution. The purpose of the summary is to help the LLM better understand and guide further exploration, rather than offering a direct solution. Additionally, directly revealing the answer may result in misleading information.</li>
  <li><strong>Focus on Relevant Content</strong>: Ensure that the summary generated by the LLM includes only information relevant to the query, avoiding redundancy or unnecessary details. This helps maintain the summary’s brevity and focus.</li>
</ul>

<p>Our detailed implementation is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_docs_references</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Starting get_docs_references method"</span><span class="p">)</span>

    <span class="n">retriever</span> <span class="o">=</span> <span class="n">WikipediaRetriever</span><span class="p">(</span><span class="n">lang</span><span class="o">=</span><span class="s">"en"</span><span class="p">,</span> <span class="n">load_max_docs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Use caption for document retrieval if available
</span>        <span class="k">if</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Using caption for document retrieval:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
            <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
        <span class="c1"># If caption is also not available, use table headers
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"No caption found, using header instead:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">'table'</span><span class="p">][</span><span class="s">'header'</span><span class="p">])</span>
            <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"header"</span><span class="p">]))</span>
        
        
        <span class="c1"># Extract relevant metadata from the retrieved documents
</span>        <span class="n">metadata_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">'title'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                <span class="s">'summary'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'summary'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                <span class="s">'source'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'source'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span>
            <span class="p">}</span>
            <span class="n">metadata_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>

        <span class="c1"># Print the metadata for debugging
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"Retrieved metadata: "</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">)</span>

        <span class="c1"># Extract table, statement, and caption from parsed_example
</span>        <span class="n">table</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"header"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="p">[]),</span>
            <span class="s">"rows"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"rows"</span><span class="p">,</span> <span class="p">[])</span>
        <span class="p">}</span>
        <span class="n">statement</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"query"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
        <span class="n">caption</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>

        <span class="c1"># Call the method to generate table summary using metadata
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"Calling generate_table_summary with metadata"</span><span class="p">)</span>
        <span class="n">generated_summary</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">call_llm</span><span class="p">.</span><span class="n">generate_table_summary</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">statement</span><span class="p">,</span> <span class="n">caption</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Generated summary:"</span><span class="p">,</span> <span class="n">generated_summary</span><span class="p">)</span>
        
        <span class="c1"># Return the generated summary in a dictionary under the 'table_summary' key
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="n">generated_summary</span><span class="p">}</span>
    <span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An error occurred while retrieving documents: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"Document retrieval failed"</span><span class="p">}</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An unexpected error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"An unexpected error occurred"</span><span class="p">}</span>
</code></pre></div></div>

<p>The specific prompt and example content used are as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">generate_table_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""
        Generate a summary for a table that directly addresses a given query, using metadata and context.

        :param metadata_list: List of metadata from related Wikipedia documents.
        :param context: Additional context about the table.
        :param table: Dictionary representing the table's data.
        :param query: The query or statement to be addressed by the summary.
        :param caption: Caption of the table for context.
        :return: JSON string containing the generated summary.
        """</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a query, the table's caption, metadata from related Wikipedia documents, and the context of the table. 
        Your task is to generate a concise summary for the table that directly addresses the query, using the Wikipedia metadata and the context to enhance understanding. 
        Ensure the summary begins by rephrasing or summarizing the query in a way that naturally introduces the purpose of the table. 
        Do not directly reveal the answer, but guide the reader to make an informed decision based on the provided information.

        Now, generate a summary for the given table, addressing the query and using the Wikipedia metadata and the context provided for enhanced understanding. 
        Ensure the summary starts by rephrasing or summarizing the query to introduce the table's purpose and includes only content related to the query. 
        Please avoid directly revealing the answer.

        Query:
        </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Wikipedia metadata:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Context:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        summary

        """</span>

        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>

</code></pre></div></div>

<p>We have conducted in-depth consideration and optimization of the table augmentation methods. Through the aforementioned methods, we can largely ensure that when dealing with complex data, the model can more accurately understand and recall key information from the tables. By improving the term clarification module and the Wikipedia reference module, we have successfully avoided the potential pitfalls of external information, such as misleading data and redundancy, thereby enhancing the overall performance of the model in various scenarios. These improvements not only guarantee the quality of augmented information but also lay a solid foundation for the reliability and efficiency of the model in practical applications.</p>

<h1 id="part-three-enhancing-the-retrieval-process">Part Three: Enhancing the Retrieval Process</h1>

<p>In the retrieval process, traditional methods such as BM25, DPR (Dense Passage Retrieval), or direct vector database searches are commonly used. BM25, a classical and efficient text retrieval method, ranks documents based on the frequency of keyword occurrences. On the other hand, DPR employs a dual-tower model that uses deep learning techniques to embed queries and documents into high-dimensional vector spaces, matching them based on approximate similarity. Both methods perform well in simple query scenarios, but they may have limitations in precision and efficiency when handling complex and diverse queries. Vector database retrieval, leveraging efficient vector similarity search libraries like Faiss, meets the demands of large-scale data retrieval.</p>

<p>However, these methods may lack sufficient retrieval accuracy when faced with complex queries or table-like data. Therefore, we ultimately chose to enhance the TableRAG system using ColBERT. This decision was based not only on ColBERT’s unique innovations and advantages but also on its demonstrated efficiency and accuracy in practical applications. Currently, ColBERT can be easily integrated into the RAG pipeline via <a href="https://github.com/bclavie/RAGatouille">RAGatouille</a>, and Llamaindex provides integration with this repository, making its application even more convenient.</p>

<h2 id="innovations-and-advantages-of-colbert">Innovations and Advantages of ColBERT</h2>

<h3 id="innovations"><strong>Innovations</strong></h3>

<ol>
  <li><strong>Late Interaction Framework</strong>: ColBERT reduces online query computation by decoupling the encoding of queries and documents, with similarity calculation performed after encoding. This allows for precomputing document representations, significantly improving computational efficiency.</li>
  <li><strong>MaxSim Operation</strong>: ColBERT uses the MaxSim operation to evaluate the relevance between queries and documents. It sums the maximum cosine similarity or L2 distance between each query embedding and the document embeddings, a simple yet effective approach.</li>
  <li><strong>Shared BERT Encoder</strong>: By sharing a BERT encoder and adding special tokens ([Q] and [D]) before input, ColBERT saves computational resources while retaining contextual understanding.</li>
  <li><strong>Document Segmentation and Filtering</strong>: Unrelated information, such as punctuation, is filtered out to reduce computational and storage burdens.</li>
  <li><strong>Vector Similarity-Based Retrieval</strong>: Leveraging vector similarity search libraries like Faiss, ColBERT efficiently retrieves documents from large collections end-to-end.</li>
</ol>

<h3 id="advantages"><strong>Advantages</strong></h3>

<ol>
  <li><strong>High Computational Efficiency</strong>: Precomputing document representations and the late interaction mechanism drastically reduce the computational load during query processing, improving speed by two orders of magnitude.</li>
  <li><strong>High Space Utilization</strong>: Through normalization and dimensionality reduction, ColBERT effectively reduces storage space requirements, enhancing feasibility in practical applications.</li>
  <li><strong>Strong Scalability</strong>: ColBERT’s architecture is designed to handle large document collections without sacrificing accuracy, particularly excelling in efficient pruning operations during vector similarity searches.</li>
  <li><strong>End-to-End Retrieval Capability</strong>: ColBERT can directly retrieve from large document collections, improving system recall rates and accuracy.</li>
</ol>

<h3 id="improvements-in-colbertv2">Improvements in ColBERTv2</h3>

<p>In ColBERTv2, these advantages are further enhanced. Specifically, the introduction of <strong>residual compression mechanisms</strong> and <strong>denoising supervision</strong> significantly reduces storage needs while improving training effectiveness. Additionally, ColBERTv2 optimizes the indexing and retrieval process, achieving more efficient candidate generation and passage ranking, further enhancing retrieval performance.</p>

<h3 id="practical-applications-in-the-retrieval-process">Practical Applications in the Retrieval Process</h3>

<p>In our TableRAG system, ColBERT is used not only to rerank the pre-retrieved document set but also to directly improve system recall and accuracy through its end-to-end retrieval capabilities. To further optimize the quality of retrieval results, we have introduced a rerank mechanism that reorders the initially retrieved document set. This mechanism helps refine and enhance the relevance and accuracy of the results after the initial retrieval.</p>

<p>Specifically, when using ColBERT for queries, the system first preprocesses and encodes all documents in the table, generating efficient vector representations. During the query process, ColBERT uses these pre-generated document vectors to quickly identify the most relevant documents through the MaxSim operation. Subsequently, the rerank mechanism further refines the ordering of these initial results, ensuring that the final documents presented to the user most closely align with the query intent.</p>

<p>Our tests show that using ColBERT combined with the rerank mechanism not only significantly improves retrieval accuracy but also further optimizes query response times. Through this multi-layered retrieval and ranking approach, we can ensure high-precision retrieval results while avoiding the high computational costs and long response times associated with traditional methods.</p>

<p>In conclusion, by integrating ColBERT and the rerank mechanism into our TableRAG system, we effectively utilize augmented information during the retrieval process. This enhancement strategy not only boosts the system’s computational efficiency and storage utilization but also, through its innovative retrieval and ranking mechanisms, significantly increases retrieval speed and result relevance without sacrificing accuracy. As a result, our system can quickly and accurately return the most relevant information when handling complex table queries, thereby significantly enhancing user experience and overall system performance.</p>

<h1 id="part-four-enhancing-input-formats">Part Four: Enhancing Input Formats</h1>

<h2 id="optimization-of-table-formats-passed-to-llms">Optimization of Table Formats Passed to LLMs</h2>

<p>In the process of table augmentation and retrieval, the format in which tables are passed to large language models (LLMs) is crucial to the final processing effectiveness. Existing research has explored different table conversion methods and compared their impact on the performance of LLM-based question-answering systems. These methods include Markdown format, template serialization, traditional pre-trained language model (TPLM) methods, and direct text generation using large language models (LLMs). Studies have shown that the performance of table conversion methods varies across different paradigms.</p>

<p>In the paper <strong>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</strong>, the authors compared the performance of different table conversion methods on hybrid datasets, particularly their effects on LLM-based question-answering systems:</p>

<ul>
  <li><strong>Markdown Format</strong>: Representing table content using Markdown format.</li>
  <li><strong>Template Serialization</strong>: Using predefined templates to convert tables into text.</li>
  <li><strong>Traditional Pre-trained Language Model (TPLM) Methods</strong>: Fine-tuning models such as T5 and BART for table-to-text tasks.</li>
  <li><strong>Large Language Model (LLM) Methods</strong>: Generating text in one-shot using models like ChatGPT.</li>
</ul>

<p>The study concludes that:</p>

<ul>
  <li>In the Data-Specific Feature Transfer (DSFT) paradigm, table-to-text conversion methods using language models (TPLM and LLM) performed best.</li>
  <li>In the Retrieval-Augmented Generation (RAG) paradigm, the Markdown format exhibited unexpected efficiency, though LLM methods still performed well.</li>
</ul>

<blockquote>
  <p><a href="https://arxiv.org/abs/2402.12869">Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</a></p>
</blockquote>

<h2 id="optimization-of-input-formats">Optimization of Input Formats</h2>

<p>Based on the above research, we selected two table formats to be passed to LLMs in our experiments to further optimize system performance:</p>

<ol>
  <li><strong>HTML Format</strong>: HTML format provides a clear, structured representation that allows the model to accurately understand the hierarchy and relational content of the table. This format is suitable for scenarios requiring the preservation of complex table structures, especially in multi-dimensional or nested table contexts where HTML format can effectively convey the semantic information of the table.</li>
  <li><strong>Markdown Format</strong>: Markdown format, known for its simplicity and human readability, is widely used in various text representation tasks. Research indicates that in the RAG paradigm, Markdown format not only effectively represents table content but also enhances model processing efficiency. Therefore, we adopted the Markdown format in our experiments to evaluate its performance in practical applications.</li>
</ol>

<p>By adopting these two formats, we aim to maximize the potential of LLMs in table processing tasks. The structural advantage of the HTML format and the concise efficiency of the Markdown format offer flexible choices for different scenarios, ensuring that table content can be accurately understood and efficiently processed by LLMs, thereby further improving the overall performance of table-based question-answering systems.</p>

<p>The implementation of this format optimization strategy is not only theoretically supported by existing research but has also been practically validated in our experiments, providing a solid foundation for subsequent system development. We will continue to explore other possible formats to further optimize the way tables are passed to LLMs, ensuring that the system maintains excellent performance in various complex scenarios.</p>

<h1 id="evaluation-experiments">Evaluation Experiments</h1>

<h2 id="1-control-experiment">1. Control Experiment</h2>

<p>The purpose of the control experiment is to evaluate the performance changes when gradually adding various modules to the baseline model. The specific design is as follows:</p>

<ul>
  <li><strong>Baseline</strong>: The original model without any additional modules, serving as a reference standard.</li>
  <li><strong>Filter</strong>: Gradually adding different filtering modules to the baseline model.
    <ul>
      <li><strong>Semantics-based</strong>: This is further divided into two sub-parts:
        <ul>
          <li><strong>ColBERT</strong>: Adding the ColBERT semantic similarity comparison module.</li>
          <li><strong>OpenAI Embedding Model</strong>: Adding the OpenAI Embedding Model for semantic similarity comparison.</li>
        </ul>
      </li>
      <li><strong>LLM-based</strong>: Adding an LLM-based filter.</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>: Gradually adding different clarification strategies to the baseline model.
    <ul>
      <li><strong>Term Exp.</strong>: Adding the term expansion module.</li>
      <li><strong>Table Summary</strong>: Adding the table summary module.</li>
      <li><strong>Exp. &amp; Summary</strong>: Adding both the term expansion and table summary modules.</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>: Gradually adding different formatting</li>
</ul>

<p>methods to the baseline model.
    - <strong>String</strong>: Using string formatting.
    - <strong>Markdown</strong>: Using Markdown formatting.
    - <strong>HTML</strong>: Using HTML formatting.</p>
<ul>
  <li><strong>Retriever</strong>: Testing different retrieval strategies on the baseline model, particularly evaluating the impact of using the rerank mechanism with the ColBERT model to reorder results.
    <ul>
      <li><strong>BM25</strong>: Using BM25 for retrieval.</li>
      <li><strong>DPR</strong>: Using DPR for retrieval.</li>
      <li><strong>ColBERT</strong>: Using ColBERT for retrieval, also evaluating whether reranking the retrieval results impacts the outcome.</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>: Testing the performance of the model after adding a consistency module.</li>
</ul>

<h2 id="2-ablation-experiment">2. Ablation Experiment</h2>

<ul>
  <li><strong>Filter</strong>: Exploring the impact of different filters on model performance.
    <ul>
      <li><strong>Semantics-based</strong>: Further divided into two sub-parts, where the modules using ColBERT and the OpenAI Embedding Model for semantic similarity comparison are removed.</li>
      <li><strong>LLM-based</strong>: Removing the LLM-based filter module.</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>: Evaluating the contribution of different clarification strategies to the model.
    <ul>
      <li><strong>Term Exp.</strong>: Removing the term expansion module.</li>
      <li><strong>Table Summary</strong>: Removing the table summary module.</li>
      <li><strong>All Removed</strong>: Removing all clarification-related modules.</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>: Testing the impact of different formatting methods on the model.
    <ul>
      <li><strong>Markdown</strong>: Removing Markdown formatting.</li>
      <li><strong>HTML</strong>: Removing HTML formatting.</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>: Testing the model’s performance without the consistency module.</li>
</ul>

<h3 id="retriever-evaluation">Retriever Evaluation</h3>

<p>To evaluate the recall rates of different retrievers, the following experiments were conducted on four datasets, with and without table summaries:</p>

<ul>
  <li><strong>BM25</strong>: Traditional TF-IDF retriever.</li>
  <li><strong>ColBERT</strong>:
    <ul>
      <li>No rerank: Using the initial retrieval results generated by ColBERT.</li>
      <li>With rerank: Reordering the initial retrieval results.</li>
    </ul>
  </li>
  <li><strong>DPR</strong>: A dense vector retriever based on deep learning.</li>
  <li><strong>Faiss Vector Database</strong>: An efficient vector retrieval database.</li>
</ul>

<h1 id="acknowledgments">Acknowledgments</h1>
<p>I would like to express my sincere gratitude to the authors of the paper <a href="https://arxiv.org/abs/2312.09039">“Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning”</a> for providing valuable insights that influenced some of the ideas presented in this article. 
Additionally, I would like to thank PeiMa from the University of Leeds for her significant contributions to this project. Her expertise and support were instrumental in shaping the outcome of this work.</p>

<h3 id="copyright-notice">Copyright Notice</h3>
<p>© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from the author. For inquiries, please contact me at yuhang.wu-4 [at] postgrad.manchester.ac.uk.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Zhang, T., Li, Y., Jin, Y. and Li, J., 2020. Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment. <em>arXiv preprint arXiv:2002.08245</em>.</li>
  <li>Li, L., Wang, H., Zha, L., Huang, Q., Wu, S., Chen, G. and Zhao, J., 2023. Learning a data-driven policy network for pre-training automated feature engineering. In <em>The Eleventh International Conference on Learning Representations</em>.</li>
  <li>Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B. and Wang, W.Y., 2021. Finqa: A dataset of numerical reasoning over financial data. <em>arXiv preprint arXiv:2109.00122</em>.</li>
  <li>Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H. and Wang, W., 2020. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. <em>arXiv preprint arXiv:2004.07347</em>.</li>
  <li>Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F. and Chua, T.S., 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. <em>arXiv preprint arXiv:2105.07624</em>.</li>
  <li>Babaev, D., Savchenko, M., Tuzhilin, A. and Umerenkov, D., 2019, July. Et-rnn: Applying deep learning to credit loan applications. In <em>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> (pp. 2183-2190).</li>
  <li>Ye, Y., Hui, B., Yang, M., Li, B., Huang, F. and Li, Y., 2023, July. Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. In <em>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 174-184).</li>
  <li>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L. and Smith, N.A., 2022. Binding language models in symbolic languages.<em>arXiv preprint arXiv:2210.02875</em>.</li>
  <li>Robertson, S. and Zaragoza, H., 2009. The probabilistic relevance framework: BM25 and beyond. <em>Foundations and Trends® in Information Retrieval</em>, <em>3</em>(4), pp.333-389.</li>
  <li>Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. <em>arXiv preprint arXiv:2004.04906</em>.</li>
  <li>Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J. and Wang, H., 2023. 
Retrieval-augmented generation for large language models: A survey. <em>arXiv preprint arXiv:2312.10997</em>.</li>
  <li>Lu, W., Zhang, J., Zhang, J. and Chen, Y., 2024. Large language model for table processing: A survey. <em>arXiv preprint arXiv:2402.05121</em>.</li>
  <li>Jiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W.X. and Wen, J.R., 2023. Structgpt: A general framework for large language model to reason over structured data. <em>arXiv preprint arXiv:2305.09645</em>.</li>
  <li>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023. Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</li>
  <li>Bian, N., Han, X., Sun, L., Lin, H., Lu, Y., He, B., Jiang, S. and Dong, B., 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. <em>arXiv preprint arXiv:2303.16421</em>.</li>
  <li>Lin, W., Blloshmi, R., Byrne, B., de Gispert, A. and Iglesias, G., 2023, July. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em> (pp. 1557-1566).</li>
  <li>Li, X., Chan, S., Zhu, X., Pei, Y., Ma, Z., Liu, X. and Shah, S., 2023.  Are ChatGPT and GPT-4 general-purpose solvers for financial text analytics? A study on several typical tasks. <em>arXiv preprint arXiv:2305.05862</em>.</li>
  <li>Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X. and Wang, W.Y., 2019. Tabfact: A large-scale dataset for table-based fact verification. <em>arXiv preprint arXiv:1909.02164</em>.</li>
  <li>Aly, R., Guo, Z., Schlichtkrull, M., Thorne, J., Vlachos, A., Christodoulopoulos, C., Cocarascu, O. and Mittal, A., 2021. Feverous: Fact extraction and verification over unstructured and structured information. <em>arXiv preprint arXiv:2106.05707</em>.</li>
  <li>Iyyer, M., Yih, W.T. and Chang, M.W., 2017, July. Search-based neural structured learning for sequential question answering. In <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> (pp. 1821-1831).</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">TableRAG(CN)</title><link href="http://localhost:4000/TableRAG-RAG-for-Table-Data-CN/" rel="alternate" type="text/html" title="TableRAG(CN)" /><published>2024-08-15T13:51:00+01:00</published><updated>2024-08-15T13:51:00+01:00</updated><id>http://localhost:4000/TableRAG-RAG-for-Table-Data-CN</id><content type="html" xml:base="http://localhost:4000/TableRAG-RAG-for-Table-Data-CN/"><![CDATA[<h1 id="背景">背景</h1>
<p>表格作为一种基础且广泛应用的半结构化数据类型，广泛存在于关系数据库、电子表格应用程序和用于数据处理的编程语言中，涵盖了金融分析（Zhang et al., 2020; Li et al., 2022）、风险管理（Babaev et al., 2019）和医疗保健分析等多个领域。在这些应用中，表格问答（TableQA）是对表格数据进行推理的一个关键下游任务（Ye et al., 2023a; Cheng et al., 2023）。</p>

<p>表格问答的目标是使计算机能够理解人类针对表格内容的查询，并以自然语言作出回答。随着近年来大规模语言模型（LLMs）的快速发展，表格问答已成为一个重要的子领域，并取得了显著的进展（Ray, 2023）。目前，大多数利用LLM进行表格问答的研究都是基于单个表格的（Li et al., 2023）。这些方法通常通过将表格预处理后，将问题和表格逐个输入LLM，侧重于让LLM更好地理解表格结构。这类方法在实际应用中主要集中于金融领域，如金融表格问答、金融审计表格处理(Zhu et al., 2021)和金融数值推理等(Chen et al., 2021, Chen et al., 2020)。然而，在现实场景中，往往面临的是一组表格（a set of tables）而非单个表格，用户可能会提出涉及多个表格的任意相关问题。在这种情况下，LLM不仅需要逐个输入回答，更重要的是能够从大量表格中召回相关表格并给出答案。然而，目前在这方面的研究还相对欠缺，我们的研究旨在弥补这一差距。</p>

<p>微调大规模语言模型是解决表格问答挑战的常见方法，但这种方法需要大量的领域特定的标注数据和巨大的计算资源。此外，大多数模型在处理领域特定和复杂的表格数据时，往往过度依赖预训练知识，从而导致幻觉和错误信息（Ray, 2023; Gao et al., 2023）。</p>

<p>为了解决这些挑战，检索增强生成（RAG）方法将检索机制与生成模型相结合，引用外部知识库，以减少模型幻觉并提高领域特定问答的准确性，同时降低资源消耗（Gao et al., 2023）。然而，尽管RAG在处理非结构化文本数据方面表现出色，但在应用于半结构化表格数据时仍存在若干挑战。具体而言，我们识别了以下三个局限性：</p>

<ol>
  <li>为回答问题所需的表格可能非常庞大，包含大量与查询无关的噪声（Lu et al., 2024）。这不仅增加了不必要的计算，还会影响检索器检索时召回的准确性以及生成器响应的准确性。为了解决这个问题，我们可以采用表格采样（Sui et al., 2024）或表格过滤的方法，检索相关的行和列，从而生成最相关的子表（Jiang et al., 2023）。</li>
  <li>表格的原始内容可能包含需要进一步澄清的信息，如领域特定术语或缩写（Sui et al., 2024）。这些领域特定的细节可能导致生成器的误解或偏见。为了解决这个问题，我们可以利用外部知识库为表格提供额外的上下文信息（Bian et al., 2023），或通过LLM生成术语解释，这一过程我们称之为table clarifier。</li>
  <li>表格通常在不同列中包含多种类型的信息，而传统的检索方法如BM25（Robertson et al., 2009）或Dense Passage Retriever（DPR）（Karpukhin, et al., ）可能会忽略表格细节，影响生成结果。我们可以通过采用ColBERT模型作为检索器来解决这一问题，该模型在标记级别对文本进行编码，使得检索更加细粒度（Li et al., 2023）。</li>
</ol>

<p>通过结合这些改进，我们的研究旨在为处理多个表格的大规模表格问答任务提供一个更有效的解决方案，以应对更复杂的现实场景。</p>
<h1 id="overview">Overview</h1>
<p>在处理复杂表格问答任务时，我们设计了一个结合最新大规模语言模型（LLM）与检索增强生成（RAG）技术的系统，以应对实际应用中的多表格问题。以下是项目核心思想的图示与介绍。</p>

<h3 id="基于rag的多表格问答系统架构">基于RAG的多表格问答系统架构</h3>

<p><img src="/insert_images/The_overall_structure.png" alt="The overall structure" /></p>

<p>在这个系统架构中，我们的目标是从多个表格中检索相关信息，并生成准确的自然语言答案。流程可以分为以下几个关键步骤：</p>

<ol>
  <li><strong>表格处理与文本切分</strong>：首先，原始表格数据经过预处理和文本切分，将表格内容转换为多个文本片段。这样做的目的是使得数据更易于处理，并能够针对查询进行高效的检索。</li>
  <li><strong>向量数据库的构建</strong>：切分后的文本和表格片段经过嵌入处理并存储在向量数据库中。向量数据库通过高效的向量化检索技术，可以迅速找到与查询相关的内容片段。</li>
  <li><strong>查询与检索</strong>：当用户提出问题时，检索器会从向量数据库中查找与问题相关的表格片段。在这个过程中，我们引入了ColBERT模型来增强检索器的精度。ColBERT通过在标记级别编码文本，能够实现更细粒度的检索，从而提高检索结果的相关性。</li>
  <li><strong>生成答案</strong>：检索到的相关文本片段与用户的提问一起输入到大规模语言模型（LLM）中，由LLM生成最终的自然语言答案。</li>
</ol>

<h3 id="多表格问答的增强机制">多表格问答的增强机制</h3>

<p><img src="/insert_images/Enhancement.png" alt="Enhancement" /></p>

<p>在处理来自多张表格的数据时，我们的系统引入了多种增强机制，以提高问答任务的精确性和有效性。</p>

<ol>
  <li>
    <p><strong>基于语义的表格过滤器</strong>：当面对大量表格时，系统首先通过语义分析对表格进行过滤，选择最相关的表格。在此过程中，我们采用了以下两种不同的模型进行文本嵌入，并进行了对比：</p>

    <p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

    <ul>
      <li><strong>利用OpenAI的Embedding模型</strong>：我们使用OpenAI的Embedding模型对表格内容进行嵌入处理，然后利用FAISS向量数据库对嵌入后的数据进行存储和检索，从中返回与查询最相关的表格行和列。</li>
      <li><strong>利用ColBERT模型</strong>：我们也使用ColBERT模型对表格内容进行嵌入，并在检索过程中使用ColBERT进行更细粒度的检索。通过与OpenAI Embedding模型的结果进行对比，我们能够选择更适合特定任务的语义过滤方法。</li>
    </ul>
  </li>
  <li><strong>基于LLM的过滤器</strong>：除了语义过滤器，我们还使用大规模语言模型（LLM）对表格进行智能过滤。通过分析表格内容与查询之间的深层语义关联，LLM能够更精准地选择出最相关的表格片段，进一步提高检索的准确性。</li>
  <li>
    <p><strong>表格澄清器</strong>：在过滤后的表格基础上，我们引入了两个澄清模块：</p>

    <p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

    <ul>
      <li><strong>术语澄清</strong>：对于表格中的领域特定术语或缩写，我们调用LLM进行解释，帮助LLM更好地理解问题和表格内容。</li>
      <li><strong>基于Wiki的摘要生成</strong>：首先，我们通过表格标题、表头或上下文信息，搜索维基百科并返回相关的元数据。接着，将这些维基数据与表格的原始上下文信息打包处理，生成与需要判断的问题或澄清的陈述相关的摘要。这种方式不仅提高了信息的准确性，还为复杂表格的理解提供了更全面的背景支持。</li>
    </ul>
  </li>
</ol>

<p>上述架构与增强机制有效地应对了当前表格问答任务中存在的挑战，特别是在多表格环境下的实际应用。通过结合先进的检索技术、语义与LLM过滤，以及大规模语言模型，我们的系统能够从大量表格中迅速找到相关信息并生成精确的答案，为各类复杂数据分析任务提供了有力的支持。</p>

<h1 id="数据集的选择">数据集的选择</h1>

<h2 id="tablefact"><a href="https://tabfact.github.io/">Tablefact</a></h2>

<p>在现有的表格问答数据集中，我们已经进行了广泛的尝试和研究。关于详细的数据集整理，请参阅我的另一篇博客：<a href="https://yuhangwuai.github.io/2024/08/14/Dataset-for-Question-Answering/">Dataset for Question Answering</a>：。通过这些经验，我们在使用数据集进行表格问答的检索增强生成时，发现主要面临以下几个问题：</p>

<ol>
  <li><strong>问题简短导致召回效果不佳</strong>：
    <ul>
      <li>许多问答数据集中的问题通常非常简短，仅由几个单词组成。这种简短的提问在相似度检索或其他密集型检索过程中，往往导致相关表格的召回效果不佳。</li>
    </ul>
  </li>
  <li><strong>问题形式单一</strong>：
    <ul>
      <li>问题通常以相似的疑问词和连词开头。例如，在SQA数据集中，”What are the schools?” 和 “What are the countries?” 这个问题尽管涉及完全不同的内容，但它们的开头 “What are the” 两是相同的。如果数据集中有近500个以 “What are the” 开头的问题，这种形式上的重复会使得相关表格的准确召回变得非常困难。</li>
    </ul>
  </li>
  <li><strong>缺乏表标题</strong>：
    <ul>
      <li>大量问答数据集不包含表标题，通常一个表格仅对应一个问题，完全不涉及检索阶段。在这种情况下，每次输入时将表格和问题直接一起输入模型。然而，当缺乏表标题时，从大量表格中精准返回相关表格的难度大大增加。</li>
    </ul>
  </li>
</ol>

<p>基于这些挑战，在我们最初的实验中，TableFact数据集是我们首选的基础数据集。TableFact的数据集专注于表格事实验证这一任务，能够有效地评估模型在推理和判断方面的能力。</p>

<p>TableFact是一个大规模的数据集，包含117,854条手动标注的声明，涉及16,573个维基百科表格。这些表格和声明之间的关系被分类为“ENTAILMENT”（蕴含）和“REFUTATION”（反驳）。该数据集首次提出在结构化数据上评估语言推理能力，涉及符号推理和语义推理的混合推理技能。这种复杂性使得TableFact成为评估深度学习模型在同时处理语义和符号推理任务时的能力的理想数据集。</p>

<table>
  <thead>
    <tr>
      <th>Channel</th>
      <th>Sentence</th>
      <th>Table</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Simple (r1)</td>
      <td>50,244</td>
      <td>9,189</td>
    </tr>
    <tr>
      <td>Complex (r2)</td>
      <td>68,031</td>
      <td>7,392</td>
    </tr>
    <tr>
      <td>Total (r1 + r2)</td>
      <td>118,275</td>
      <td>16,573</td>
    </tr>
    <tr>
      <td>Split</td>
      <td>Sentence</td>
      <td>Table</td>
    </tr>
    <tr>
      <td>Train</td>
      <td>92,283</td>
      <td>13,182</td>
    </tr>
    <tr>
      <td>Val</td>
      <td>12,792</td>
      <td>1,696</td>
    </tr>
    <tr>
      <td>Test</td>
      <td>12,779</td>
      <td>1,695</td>
    </tr>
  </tbody>
</table>

<p>该数据集的示例如下：</p>

<p><img src="/insert_images/tablefact.png" alt="Tablefact sample instances（Chen et al., 2019）" /></p>

<p>Tablefact sample instances（Chen et al., 2019）</p>

<p>TableFact数据集的主要优势在于其专注于表格事实验证这一任务，能够有效地评估模型在推理和判断方面的能力。具体任务是：给定一个表格和一个声明，要求模型判断该声明是否与表格中的信息一致。模型需要对表格内容进行深入推理，并对声明标记“True”（真实）或“False”（虚假）。</p>

<p>TableFact数据集不仅包含大量复杂的表格和声明对，覆盖多种领域和主题，能够很好地模拟现实中可能遇到的多表格问答场景。这为我们提供了一个具有挑战性的测试平台，可以帮助我们更全面地评估和优化我们的多表格问答系统。使用这个数据集的另一个重要原因是，它能够更好地控制LLM的输出，使我们能够精确评估模型的表现。</p>

<p><em>我们选择使用<a href="https://tabfact.github.io/">TableFact数据集</a>的原因如下：</em></p>

<ol>
  <li><strong>纯表格数据集</strong>：TableFact的数据主要以表格形式呈现，声明内容的相似性较低，使得在检索和召回过程中难度相对较小，有助于模型准确定位相关信息。</li>
  <li><strong>明确的分类任务</strong>：TableFact的数据集任务明确，即判断声明的真假。这种任务设置使得在生成答案时更容易控制大模型的输出，从而更准确地评估模型的推理能力。</li>
</ol>

<h2 id="feverous"><a href="https://fever.ai/dataset/feverous.html">Feverous</a></h2>

<p>在使用TableFact之后，我们选择了FEVEROUS（Fact Extraction and VERification Over Unstructured and Structured information）数据集。FEVEROUS是一个专为事实验证任务设计的大规模数据集，与TableFact不同，它不仅包含结构化表格数据，还包含非结构化文本数据。这使得FEVEROUS在检索和推理过程中更加复杂和具有挑战性。</p>

<p><img src="/insert_images/feverous.png" alt="Feverous sample instances(Aly et al., 2021)" />
Feverous sample instances(Aly et al., 2021)</p>

<p><a href="https://fever.ai/dataset/feverous.html">Feverous</a>的数据集包含超过80,000个表格和文本段落对，以及与之关联的超过120,000个事实验证问题。模型在处理FEVEROUS数据集时，除了判断声明的真假之外，还需在三个选项之间做出选择：<strong>Supported</strong>（支持）、<strong>Refuted</strong>（反驳）、或<strong>Not Enough Information</strong>（信息不足）。这种三选一的任务设置进一步增加了模型的推理复杂度，与TableFact的二元分类任务相比，FEVEROUS能够更全面地评估模型的推理能力，尤其是在多源信息整合和判断中的表现。</p>

<p><em>选择<a href="https://fever.ai/dataset/feverous.html">Feverous</a>的原因</em>：</p>

<ul>
  <li>结合结构化和非结构化数据，增加了模型的推理难度。</li>
  <li>三选一任务设置，能够更好地评估模型在复杂推理任务中的表现。</li>
</ul>

<h2 id="sqa"><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></h2>

<p>在进一步扩展实验时，我们引入了<a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA（Sequential Question Answering）</a>数据集。SQA数据集的设计旨在评估模型在复杂、多步骤问答场景中的表现。这一数据集包含超过6,000个对话式问答对，每个对话涉及多个相关联的问题，这些问题通常与先前的问答上下文相关联。与TableFact和FEVEROUS不同，SQA要求模型在一个连续的问答过程中保持上下文的理解和一致性。</p>

<p>SQA中的问题不仅需要回答当前的问题，还需要基于之前的问答进行推理。更重要的是，SQA要求模型给出的答案是自由的，涵盖文本、数字等多种形式。这种开放式的问答增加了模型推理的复杂性，也考验了模型在处理自由回答时的生成能力。</p>

<p><img src="/insert_images/sqa.png" alt="SQA sample instances (Lyyer et al., 2017)" /></p>

<p>SQA sample instances (Lyyer et al., 2017)</p>

<p><em>选择<a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a>的原因</em>：</p>

<ul>
  <li>纯结构化数据，暂时不涉及两个类型数据的整合</li>
  <li>专注于多步骤问答，增加了模型在处理对话上下文和连续推理时的挑战。</li>
  <li>自由回答形式的引入，考验了模型在开放式问答任务中的表现。</li>
</ul>

<h2 id="hybridqa"><a href="https://hybridqa.github.io/">HybridQA</a></h2>

<p>最后，我们选择了HybridQA数据集，以进一步提升对模型多模态信息处理能力的评估。<a href="https://hybridqa.github.io/">HybridQA</a>是一个融合了表格和文本信息的数据集，旨在测试模型在多模态信息上的综合问答能力。该数据集包含6,241个问答对，每个问题涉及多个不同信息源的内容，包括表格和关联的非结构化文本信息。</p>

<p>HybridQA的独特之处在于，模型不仅需要从多个信息源中提取和整合相关信息，还需要在回答过程中涉及数值推理的步骤。这种多模态、多步骤的问答形式要求模型在复杂任务中表现出色，尤其是在跨模态信息整合和数值推理方面。</p>

<p><img src="/insert_images/hybridqa.png" alt="HybridQA sample instances (Chen et al., 2020)" />
HybridQA sample instances (Chen et al., 2020)</p>

<p><em>选择<a href="https://hybridqa.github.io/">HybridQA</a>的原因</em>：</p>

<ul>
  <li>涉及表格和文本的两个类型信息，进一步测试模型的跨模态整合能力。</li>
  <li>复杂的问答形式和数值推理步骤，提供了更高的挑战性，用以评估模型在处理多源信息时的综合表现。</li>
  <li>自由回答形式的引入，考验了模型在开放式问答任务中的表现。</li>
</ul>

<h1 id="实施方案">实施方案</h1>

<h2 id="第一部分表格过滤器">第一部分：表格过滤器</h2>

<p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

<ol>
  <li><strong>基于语义的过滤</strong>
    <ul>
      <li><strong>生成嵌入向量</strong>：为表格中的每一行和列生成语义嵌入向量，并为用户的查询生成相应的嵌入向量。我们采用两种方法来实现这一过程：
        <ol>
          <li><strong>向量数据库匹配</strong>：使用OpenAI或其他嵌入模型生成嵌入向量，然后通过FAISS等向量数据库计算相似度，快速返回与查询相关的行列。</li>
          <li><strong>细粒度匹配</strong>：使用ColBERT预训练模型对表格数据和查询进行嵌入和匹配，以实现更高的细粒度匹配，从而选择最相关的行列。</li>
        </ol>
      </li>
      <li><strong>选择相关行列</strong>：根据相似度得分，选取与查询最相关的前k行和前k列，构建新的子表格。</li>
    </ul>
  </li>
  <li><strong>基于大型语言模型（LLM）的过滤</strong>
    <ul>
      <li><strong>转换为字符串</strong>：将查询和表格内容转化为字符串并拼接，形成上下文。</li>
      <li><strong>调用GPT过滤</strong>：使用GPT模型过滤并提取与查询相关的行列，同时生成相应的Python代码以实现筛选。为了提高代码生成的准确性和一致性，采用了自一致性策略：
        <ol>
          <li><strong>自一致性策略</strong>：让GPT生成5次代码，选择出现频率最高的代码作为最终筛选代码。如果生成的代码版本各不相同，则选择第一次生成的结果。</li>
          <li><strong>执行和错误处理</strong>：执行最终选择的代码段，更新表格。如果代码执行过程中出现错误，则捕获错误信息并返回原始表格，以确保流程的鲁棒性。</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<h3 id="llm过滤过程中的难点及解决方案">LLM过滤过程中的难点及解决方案</h3>

<p>在表格过滤过程中，尤其是基于LLM的表格过滤器中，存在以下几个主要难点：</p>

<ol>
  <li>
    <p><strong>列名的一致性问题</strong>：GPT在生成筛选代码时，有时会误识别列名，导致生成的代码与原始表格中的列名不一致，从而引发错误。例如，把scheduled’, ‘capacity (mw)理解为scheduled capacity (mw)是一个列名，LLM将多个列名合并为一个，或者将单个列名错误分拆。</p>

    <p><strong>解决方案</strong>：为了解决这一问题，可以在Prompt中明确提供整理后的列名作为参数传递给GPT，以确保生成的代码使用的列名与原始表格完全一致。这种方式能够从根本上减少列名识别错误的发生。</p>
  </li>
  <li>
    <p><strong>信息丢失问题</strong>：在LLM过滤表格过程中，筛选后的表格可能会因为过度过滤而丢失回答问题所需的关键信息。这种情况会导致在后续生成回答时，由于缺乏必要的证据，生成的答案不准确甚至错误。</p>

    <p><strong>解决方案</strong>：为了解决这一问题，可以采用“保守筛选”策略，即让LLM仅过滤掉自己非常确定与陈述无关的内容。如果LLM在判断某些内容是否与陈述相关时存在不确定性，应倾向于保留这些内容。这种策略能够最大程度地保留潜在的关键证据，确保生成的回答能够基于完整的信息进行推理，从而提高答案的准确性和可信度。</p>
  </li>
  <li>
    <p><strong>数据类型不匹配导致的筛选问题</strong>：在处理表格数据时，尤其是在筛选数值类型的数据时，可能会因为数据类型不一致而导致筛选结果为空或不准确。</p>

    <p><strong>解决方案</strong>：即使是在处理数值数据时，也建议通过字符串匹配的方式进行筛选。这种做法可以避免由于数据类型不匹配引起的筛选错误，从而提高筛选的准确性和可靠性。</p>
  </li>
  <li>
    <p><strong>Prompt设计的有效性</strong>：为了让GPT能够准确理解任务并生成正确的筛选代码，Prompt的设计至关重要。一个不明确的Prompt可能导致GPT生成不符合预期的代码。</p>

    <p><strong>解决方案</strong>：在设计Prompt时，应确保其清晰、具体，并包含足够的上下文信息，以便GPT能够准确理解任务要求。同时，可以通过反复测试和调整Prompt，找到最适合的表达方式，提高代码生成的准确性。</p>
  </li>
  <li>
    <p><strong>代码生成的一致性问题</strong>：GPT在生成代码时可能会产生多个不同版本的代码，导致结果不一致。</p>

    <p><strong>解决方案</strong>：通过自一致性策略，生成多个版本的代码并选择出现频率最高的版本，确保结果的一致性和可靠性。如果所有生成的代码都不一致，则使用第一次生成的代码并进行错误捕获处理，以确保流程的稳定性。</p>
  </li>
</ol>

<p>最后我们使用的详细的设置如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">call_llm_code_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Synthesize code snippet from the table context."""</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: Synthesize code snippet from the table context to select the proper rows and columns for verifying a statement / answering query.
        The generated code must use the exact column names provided, including spaces, capitalization, and punctuation.
        The generated code should treat all data as strings, even if they look like numbers.
        Only filter out rows and columns that are definitely not needed to verify the statement / answering query.

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: The scheduled date for the farm with 17 turbines be 2012.
        Columns: ['wind farm', 'scheduled', 'capacity (mw)', 'turbines', 'type', 'location']
        df = pd.DataFrame(wind farm)
        User 2:
        To verify the statement 'The scheduled date for the farm with 17 turbines be 2012', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'wind farm', 'scheduled', and 'turbines' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['wind farm', 'scheduled', 'turbines']].query("turbines == '17'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: All 12 club play a total of 22 game for the wru division one east.
        Columns: ['club', 'played', 'drawn', 'lost', 'points for', 'points against', 'tries for', 'tries against', 'try bonus', 'losing bonus', 'points']
        df = pd.DataFrame(club)
        User 2:
        To verify the statement 'All 12 club play a total of 22 game for the wru division one east', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'club' and 'played' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['club', 'played']].query("played == '22'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: Touchdown Atlantic, in the category of sporting, be established in 2010.
        Columns: ['event name', 'established', 'category', 'sub category', 'main venue']
        df = pd.DataFrame(event name)
        User 2:
        To verify the statement 'Touchdown Atlantic, in the category of sporting, be established in 2010', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'event name' and 'established' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['event name', 'established']].query("`event name` == 'touchdown atlantic' and established == '2010'")

        Now, generate a code snippet from the table context to select the proper rows and columns to verify the given statement / answering query.
        Use the existing column names from the provided DataFrame.
        The column names in the generated code must match the provided column names exactly, including spaces, capitalization, and punctuation.
        Only filter out rows and columns that are definitely not needed to verify the statement.
        Only return the code. 
        </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">
        </span><span class="se">\n\n</span><span class="s">:
        """</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">USE_SELF_CONSISTENCY</span><span class="p">:</span>
            <span class="n">generated_codes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Generated codes:"</span><span class="p">,</span> <span class="n">generated_codes</span><span class="p">)</span>
            
            <span class="c1"># Find the most common code
</span>            <span class="n">code_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">generated_codes</span><span class="p">)</span>
            <span class="n">most_common_code</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">code_counter</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">most_common_code</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">generated_codes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="第二部分表格澄清器">第二部分：表格澄清器</h1>

<p>在处理复杂的表格数据时，提供澄清信息有助于增强对表格内容的理解。然而，选择合适的澄清方法至关重要。在最初的设计中，我们尝试使用Google API来检索术语解释，并通过Wikipedia的文档来增强表格内容。具体来说，在最初的设计中，我们采用了以下流程来对表格进行澄清处理。</p>

<h2 id="早期方法的全流程">早期方法的全流程</h2>

<h3 id="术语澄清"><strong>术语澄清</strong></h3>

<ul>
  <li>首先，使用大型语言模型（LLM）对表格中的内容进行分析，筛选出需要进一步解释的术语。</li>
  <li>对筛选出的术语，利用Google API进行搜索，以获取相关解释。</li>
  <li>然后，将检索到的解释附加到表格中，作为术语澄清信息。这个过程可以借助Langchain中的<code class="language-plaintext highlighter-rouge">GoogleSearchAPIWrapper()</code>来实现。</li>
</ul>

<h3 id="wiki文档澄清"><strong>Wiki文档澄清</strong></h3>

<ul>
  <li>根据表格的标题、上下文或表头信息，构建Wikipedia查询。例如，如果表格的表头为“Company Name”、“Revenue”、“Number of Employees”等，可以构建类似“company revenue employees market capitalization”的查询。</li>
  <li>使用Langchain中的<code class="language-plaintext highlighter-rouge">WikipediaRetriever.get_relevant_documents()</code>进行检索，获取相关的Wikipedia文档。</li>
  <li>从检索到的文档中提取元数据，如标题、摘要和链接，将其与表格内容结合，作为进一步的澄清数据。</li>
</ul>

<p>我们使用了下面的Prompt：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">You</span> <span class="n">are</span> <span class="n">an</span> <span class="n">expert</span> <span class="ow">in</span> <span class="n">data</span> <span class="n">analysis</span> <span class="ow">and</span> <span class="n">natural</span> <span class="n">language</span> <span class="n">processing</span><span class="p">.</span> <span class="n">Your</span> <span class="n">task</span> <span class="ow">is</span> <span class="n">to</span> <span class="n">help</span> <span class="n">identify</span> <span class="n">terms</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">table</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span> <span class="k">for</span> <span class="n">better</span> <span class="n">understanding</span><span class="p">.</span> <span class="n">The</span> <span class="n">table</span> <span class="n">contains</span> <span class="n">various</span> <span class="n">fields</span><span class="p">,</span> <span class="n">some</span> <span class="n">of</span> <span class="n">which</span> <span class="n">might</span> <span class="n">include</span> <span class="n">technical</span> <span class="n">jargon</span><span class="p">,</span> <span class="n">abbreviations</span><span class="p">,</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">commonly</span> <span class="n">understood</span> <span class="n">by</span> <span class="n">a</span> <span class="n">general</span> <span class="n">audience</span><span class="p">.</span>

<span class="n">Here</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">table</span><span class="p">:</span>
<span class="p">[</span><span class="n">Insert</span> <span class="n">table</span> <span class="n">here</span><span class="p">]</span>

<span class="n">Please</span> <span class="n">follow</span> <span class="n">these</span> <span class="n">steps</span><span class="p">:</span>
<span class="mf">1.</span> <span class="n">Analyze</span> <span class="n">the</span> <span class="n">content</span> <span class="n">of</span> <span class="n">each</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">table</span><span class="p">.</span>
<span class="mf">2.</span> <span class="n">Identify</span> <span class="nb">any</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="n">technical</span><span class="p">,</span> <span class="n">specialized</span><span class="p">,</span> <span class="ow">or</span> <span class="n">abbreviations</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span><span class="p">.</span>
<span class="mf">3.</span> <span class="n">Generate</span> <span class="n">a</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">these</span> <span class="n">terms</span> <span class="n">along</span> <span class="k">with</span> <span class="n">the</span> <span class="n">corresponding</span> <span class="n">cell</span> <span class="n">reference</span> <span class="p">(</span><span class="n">row</span> <span class="ow">and</span> <span class="n">column</span><span class="p">).</span>

<span class="n">Consider</span> <span class="n">the</span> <span class="n">following</span> <span class="n">when</span> <span class="n">identifying</span> <span class="n">terms</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Technical</span> <span class="n">terms</span> <span class="n">related</span> <span class="n">to</span> <span class="n">specific</span> <span class="n">industries</span> <span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span> <span class="n">finance</span><span class="p">,</span> <span class="n">healthcare</span><span class="p">,</span> <span class="n">technology</span><span class="p">).</span>
<span class="o">-</span> <span class="n">Abbreviations</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">universally</span> <span class="n">known</span><span class="p">.</span>
<span class="o">-</span> <span class="n">Jargon</span> <span class="n">that</span> <span class="n">may</span> <span class="n">be</span> <span class="n">specific</span> <span class="n">to</span> <span class="n">a</span> <span class="n">particular</span> <span class="n">field</span> <span class="ow">or</span> <span class="n">context</span><span class="p">.</span>

<span class="n">Output</span> <span class="n">the</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">need</span> <span class="n">explanation</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">following</span> <span class="nb">format</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="p">[</span><span class="n">Term</span><span class="p">]</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="n">Row</span><span class="p">,</span> <span class="n">Column</span><span class="p">]</span>

<span class="n">Example</span> <span class="n">output</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Revenue</span> <span class="p">(</span><span class="n">million</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Market</span> <span class="n">Cap</span> <span class="p">(</span><span class="n">billion</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="n">Be</span> <span class="n">thorough</span> <span class="ow">and</span> <span class="n">ensure</span> <span class="n">that</span> <span class="nb">all</span> <span class="n">potentially</span> <span class="n">confusing</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">are</span> <span class="n">included</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">list</span><span class="p">.</span>

</code></pre></div></div>

<p>然后我们将其传递给Lanchain，利用GoogleSearchAPIWrapper()实现检索，并将结果加入作为澄清信息。</p>

<p>对于Wikipedia的方法，我们具体实现如下：</p>

<p>例如，下列表格：</p>

<table>
  <thead>
    <tr>
      <th>Company Name</th>
      <th>Revenue (Million USD)</th>
      <th>Number of Employees</th>
      <th>Market Cap (Billion USD)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Company A</td>
      <td>1000</td>
      <td>5000</td>
      <td>50</td>
    </tr>
    <tr>
      <td>Company B</td>
      <td>2000</td>
      <td>10000</td>
      <td>100</td>
    </tr>
    <tr>
      <td>Company C</td>
      <td>1500</td>
      <td>7500</td>
      <td>75</td>
    </tr>
  </tbody>
</table>

<p>利用表头信息构建查询：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"company revenue employees market capitalization"
</code></pre></div></div>

<p>查询到的信息如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="s">"title"</span><span class="p">:</span> <span class="s">"List of largest technology companies by revenue"</span><span class="p">,</span>
    <span class="s">"summary"</span><span class="p">:</span> <span class="s">"This is a list of the largest technology companies in the world by revenue."</span><span class="p">,</span>
    <span class="s">"url"</span><span class="p">:</span> <span class="s">"&lt;https://en.wikipedia.org/wiki/List_of_largest_technology_companies_by_revenue&gt;"</span>
<span class="p">}</span>
</code></pre></div></div>

<p>将上述文档原数据内容与表格结合，作为澄清数据。</p>

<blockquote>
  <p>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023.
 Tap4llm: Table provider on sampling, augmenting, and packing 
semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</p>

</blockquote>

<hr />

<p>这个方法理论上可以帮助我们获取丰富的信息资源，但在实践中却暴露出了一些不可忽视的问题。</p>

<p>首先，<strong>Google API的结果准确性问题</strong>。尽管通过Google API检索术语解释在处理某些专有术语时可能较为有效，因为这些术语通常具有唯一的定义。但当面对缩写或多义词时，问题就变得复杂了。例如，“ABC”这一缩写可能对应多个不同的概念，如“美国广播公司”（American Broadcasting Company）或“活动为基础的成本核算”（Activity-Based Costing），甚至还有其他可能的解释。在这种情况下，从Google检索到的术语解释可能会存在不一致性，不仅无法达到预期的增强效果，反而可能导致信息混淆，使结果变得更加复杂和不可靠。</p>

<p>其次，<strong>检索内容的冗长性问题</strong>。Google检索到的查询内容和Wikipedia返回的文档可能过于冗长，包含大量与表格内容相关但与实际查询需求无关的信息。这些冗长的文档在进一步处理时，可能对数据管道（pipeline）的检索效果产生负面影响。目前的研究主要侧重于将每条查询分别传入LLM或预训练模型中进行处理，而我们当前的任务有所不同，这种方法可能会导致效果不佳。如果文档过长且包含过多无关信息，可能会降低模型的准确性和效率，从而影响最终的结果质量。</p>

<h1 id="表格澄清策略的改进与完善">表格澄清策略的改进与完善</h1>

<h2 id="术语澄清模块的精准优化">术语澄清模块的精准优化</h2>

<p>基于上述原因，在对大量文献的阅读和深思熟虑之后，我们对表格澄清信息提出了以下两个关键要求：</p>

<ol>
  <li>
    <p><strong>澄清信息必须提升对表格的理解能力</strong></p>

    <p>澄清信息的首要目标是帮助模型更好地理解表格内容。信息的添加应当是精准且有助于模型在处理表格时，能够更准确地把握其结构和含义，从而提高整体的理解水平。</p>
  </li>
  <li>
    <p><strong>澄清信息必须提高对表格的召回能力</strong></p>

    <p>其次，澄清信息应当有助于提高模型对表格相关内容的召回能力。这意味着在面对查询或分析任务时，模型能够更有效地提取和利用表格中的关键信息。</p>
  </li>
</ol>

<p>在提出这些要求的同时，我们实际上也明确了两个必须避免的情况：</p>

<ol>
  <li>
    <p><strong>澄清后的信息有误，影响了LLM对表格的理解能力</strong></p>

    <p>如果澄清信息存在错误，可能会导致模型对表格的误解，从而降低其对表格内容的正确解析。这不仅违背了澄清信息的初衷，还可能使模型的输出结果产生偏差。</p>
  </li>
  <li>
    <p><strong>澄清信息过长，过多冗余，影响模型对相关表格的召回能力</strong></p>

    <p>过长或冗余的信息可能会增加模型处理时的负担，干扰其对核心内容的关注，从而削弱模型在召回相关表格信息时的效率和准确性。</p>
  </li>
</ol>

<h2 id="table澄清器的改进">Table澄清器的改进</h2>

<p>基于前述对表格增强信息的要求和潜在问题的分析，我们提出了进一步的改进方案，以优化表格增强的方法。这些改进旨在确保增强信息既能提升模型的理解能力，又能提高相关信息的召回效率，从而避免常见的误解和冗余问题。</p>

<h3 id="术语澄清模块的改进"><strong>术语澄清模块的改进</strong></h3>

<p>针对术语澄清模块，我们决定直接利用LLM从表格中提取术语并进行解释，而不再依赖GoogleSearchAPIWrap进行外部检索。尽管这一方法无法获得网络上更为广泛的综合信息，但LLM已经能够理解大部分术语和缩写，并且能够结合具体情境提供解释。这样做不仅提高了对表格的理解能力，还有效避免了可能由于外部检索带来的误导信息和冗余信息的问题，确保增强信息的精准和简洁。</p>

<h3 id="wiki参考模块的改进"><strong>Wiki参考模块的改进</strong></h3>

<h3 id="1-表格用途的澄清"><strong>1. 表格用途的澄清</strong></h3>

<p>我们引入了一个新的澄清信息，即简要的说明表格的用途，是用来回答什么问题的。这种通过明确表格目的生成的方式，可以在使用ColBERT进行信息检索时，显著提高召回率。</p>

<p>通过这种方式，我们实现了增强信息对表格召回能力的提升，确保模型在面对特定查询时能更准确地提取相关数据。具体使用prompt和用例如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">generate_terms_explanation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">statement</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a statement, and the table's caption. Your task is to identify difficult to understand column names, terms, or abbreviations in the table and provide simple explanations for each. Only explain terms related to the statement.

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: The scheduled date for the farm with 17 turbines be 2012.
        Here is the table caption: Wind Farm Details in Ireland
        Here is the table:
        wind farm

        User 2:
        Explanations:
        "scheduled": "The planned date for the wind farm to be operational.",
        "turbines": "The number of wind turbines in the wind farm."

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: All 12 clubs play a total of 22 games for the WRU Division One East.
        Here is the table caption: WRU Division One East Standings
        Here is the table:
        club

        User 2:
        Explanations:
        "played": "The number of games played by the club.",
        "points for": "The total points scored by the club.",
        "points against": "The total points scored against the club."

        Now, explain the terms in the following table.

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Statement:
        </span><span class="si">{</span><span class="n">statement</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        explanations
        }}
        """</span>
        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>  
</code></pre></div></div>

<h3 id="2-wikipedia外部信息增强的优化"><strong>2. WikiPedia外部信息增强的优化</strong></h3>

<p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

<ol>
  <li><strong>初步检索</strong>：
    <ul>
      <li><strong>基于表格标题进行WikiPedia检索</strong>：首先使用表格标题作为关键词进行WikiPedia的检索，获取相关的增强信息。</li>
      <li><strong>备用检索</strong>：如果标题检索失败，则使用表头信息进行检索，以提供与表格内容相关的增强信息。</li>
    </ul>
  </li>
  <li><strong>信息打包：</strong>
    <ul>
      <li>将Wikipedia中的数据提取元数据，但是我们不直接将这些信息加入澄清内容中，以避免冗余。</li>
      <li>我们把Wikipedia的元数据，query、table（包括筛选后的表格或原始表格）以及caption，还有context(如果有context的话)一起打包，发送给LLM进行处理，让LLM根据多方面的信息生成一个表格摘要。</li>
    </ul>
  </li>
</ol>

<p>注意事项：</p>

<ul>
  <li><strong>避免直接揭示问题答案</strong>：在生成summary时，要注意引导类摘要的撰写，避免直接透露问题的答案或提供直接的解答。总结的目的是帮助LLM更好地理解和引导他们进行进一步探索，而不是直接提供解决方案，并且直接揭示答案的话，可能这个答案也有误导性。</li>
  <li><strong>聚焦相关内容</strong>：确保LLM生成的摘要仅包括与查询内容相关的信息，避免冗余或不必要的细节。这样可以保持摘要的简洁和聚焦。</li>
</ul>

<p>具体来说我们的详细实现如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">get_docs_references</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Starting get_docs_references method"</span><span class="p">)</span>

        <span class="n">retriever</span> <span class="o">=</span> <span class="n">WikipediaRetriever</span><span class="p">(</span><span class="n">lang</span><span class="o">=</span><span class="s">"en"</span><span class="p">,</span> <span class="n">load_max_docs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Use caption for document retrieval if available
</span>            <span class="k">if</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">):</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Using caption for document retrieval:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
                <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
            <span class="c1"># If caption is also not available, use table headers
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"No caption found, using header instead:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">'table'</span><span class="p">][</span><span class="s">'header'</span><span class="p">])</span>
                <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"header"</span><span class="p">]))</span>
            
            
            <span class="c1"># Extract relevant metadata from the retrieved documents
</span>            <span class="n">metadata_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s">'title'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                    <span class="s">'summary'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'summary'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                    <span class="s">'source'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'source'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="n">metadata_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>

            <span class="c1"># Print the metadata for debugging
</span>            <span class="k">print</span><span class="p">(</span><span class="s">"Retrieved metadata: "</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">)</span>

            <span class="c1"># Extract table, statement, and caption from parsed_example
</span>            <span class="n">table</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">"header"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="p">[]),</span>
                <span class="s">"rows"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"rows"</span><span class="p">,</span> <span class="p">[])</span>
            <span class="p">}</span>
            <span class="n">statement</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"query"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
            <span class="n">caption</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>

            <span class="c1"># Call the method to generate table summary using metadata
</span>            <span class="k">print</span><span class="p">(</span><span class="s">"Calling generate_table_summary with metadata"</span><span class="p">)</span>
            <span class="n">generated_summary</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">call_llm</span><span class="p">.</span><span class="n">generate_table_summary</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">statement</span><span class="p">,</span> <span class="n">caption</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Generated summary:"</span><span class="p">,</span> <span class="n">generated_summary</span><span class="p">)</span>
            
            <span class="c1"># Return the generated summary in a dictionary under the 'table_summary' key
</span>            <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="n">generated_summary</span><span class="p">}</span>
        <span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An error occurred while retrieving documents: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"Document retrieval failed"</span><span class="p">}</span>
        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An unexpected error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"An unexpected error occurred"</span><span class="p">}</span>
</code></pre></div></div>

<p>使用的具体prompt以及用例内容如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">generate_table_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""
        Generate a summary for a table that directly addresses a given query, using metadata and context.

        :param metadata_list: List of metadata from related Wikipedia documents.
        :param context: Additional context about the table.
        :param table: Dictionary representing the table's data.
        :param query: The query or statement to be addressed by the summary.
        :param caption: Caption of the table for context.
        :return: JSON string containing the generated summary.
        """</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a query, the table's caption, metadata from related Wikipedia documents, and the context of the table. 
        Your task is to generate a concise summary for the table that directly addresses the query, using the Wikipedia metadata and the context to enhance understanding. 
        Ensure the summary begins by rephrasing or summarizing the query in a way that naturally introduces the purpose of the table. 
        Do not directly reveal the answer, but guide the reader to make an informed decision based on the provided information.

        Now, generate a summary for the given table, addressing the query and using the Wikipedia metadata and the context provided for enhanced understanding. 
        Ensure the summary starts by rephrasing or summarizing the query to introduce the table's purpose and includes only content related to the query. 
        Please avoid directly revealing the answer.

        Query:
        </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Wikipedia metadata:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Context:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        summary

        """</span>

        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>

</code></pre></div></div>

<p>我们对表格增强方法进行了深入的思考和优化，通过上述方法，我们基本可以确保在处理复杂数据时，模型能够更加准确地理解和召回表格中的关键信息。通过改进术语澄清模块和Wiki参考模块，我们成功避免了外部信息可能带来的误导和冗余问题，提升了模型在不同场景下的整体性能。这些改进不仅为增强信息的质量提供了保障，也为模型在实际应用中的可靠性和效率奠定了坚实基础。</p>

<h1 id="第三部分检索过程增强">第三部分：检索过程增强</h1>

<p>在检索过程中，传统的方法如BM25、DPR（Dense Passage Retrieval）、或者直接利用向量数据库进行检索，通常被广泛应用。BM25通过统计关键词在文档中的出现频率进行检索，是一种经典且高效的文本检索方法。而DPR采用双塔模型，利用深度学习技术，将查询和文档嵌入到高维向量空间中，通过向量的近似相似度进行匹配。这两种方法在简单查询场景中表现较好，但在处理复杂、多样化的查询时，可能存在精度和效率的局限性。向量数据库检索则依赖于高效的向量相似性搜索库，如Faiss，来实现快速的相似度计算，适合大规模数据的检索需求。</p>

<p>然而，这些方法在面对复杂的查询或表格类数据时，检索精度都不够。因此，我们在TableRAG系统中最终选择使用ColBERT进行增强。这一选择不仅基于ColBERT独特的创新点和优点，还因为其在实际应用中展现出的高效性和准确性。目前，ColBERT的实现可以通过<a href="https://github.com/bclavie/RAGatouille">RAGatouille</a>轻松集成到RAG管道中，而Llamaindex也提供了对该仓库的集成，这使得其应用变得更加便捷。</p>

<h2 id="colbert-的创新与优点">ColBERT 的创新与优点</h2>

<h3 id="创新点"><strong>创新点</strong></h3>

<ol>
  <li><strong>延迟交互框架</strong>：ColBERT通过将查询和文档的编码过程分离，并在编码后再进行相似度计算，减少了在线查询时的计算量。这使得系统能够预先计算文档的表示，大大提高了计算效率。</li>
  <li><strong>最大相似度操作（MaxSim）</strong>：ColBERT采用最大相似度操作来评估查询和文档之间的相关性，每个查询嵌入与文档嵌入之间的最大余弦相似度或L2距离相加，简单高效。</li>
  <li><strong>BERT编码器共享</strong>：通过共享BERT编码器，并在输入前分别加上特殊标记（[Q]和[D]），ColBERT在节省计算资源的同时，保留了上下文理解能力。</li>
  <li><strong>文档的分段和过滤</strong>：过滤掉无关信息，如标点符号，减少计算和存储负担。</li>
  <li><strong>基于向量相似性的检索</strong>：利用向量相似性搜索库（如faiss），ColBERT能够高效地从大型文档集合中进行端到端检索。</li>
</ol>

<h3 id="优点"><strong>优点</strong></h3>

<ol>
  <li><strong>计算效率高</strong>：预计算文档表示和延迟交互机制使ColBERT在查询处理时的计算量大幅降低，速度提高了两个数量级。</li>
  <li><strong>空间利用率高</strong>：通过归一化和降维处理，ColBERT有效地减少了存储空间需求，提升了实际应用的可行性。</li>
  <li><strong>强大的扩展性</strong>：ColBERT的架构设计允许其处理大规模文档集合而不牺牲精度，尤其是在向量相似性搜索中的高效剪枝操作中表现突出。</li>
  <li><strong>端到端检索能力</strong>：ColBERT能够直接从大型文档集合中检索，提高了系统的召回率和精度。</li>
</ol>

<h3 id="colbertv2-的改进">ColBERTv2 的改进</h3>

<p>在ColBERTv2中，这些优势得到了进一步增强。特别是引入的<strong>残差压缩机制</strong>和<strong>降噪监督</strong>，显著降低了存储需求并提高了训练效果。此外，ColBERTv2通过优化索引和检索过程，实现了更高效的候选生成和段落排序，进一步提升了检索性能。</p>

<h3 id="检索过程中的实际应用">检索过程中的实际应用</h3>

<p>在我们的TableRAG系统中，ColBERT不仅用于重新排序预检索的文档集，还通过其端到端的检索能力直接提升了系统的召回率和精度。为进一步优化检索结果的质量，我们还引入了rerank机制，对初步检索到的文档集进行重新排序。这一机制帮助我们在获得初步结果后，进一步细化和提升结果的相关性和准确性。</p>

<p>具体来说，当我们使用ColBERT进行查询时，系统首先对表格中的所有文档进行预处理和编码，生成高效的向量表示。在查询过程中，ColBERT利用这些预先生成的文档向量，通过最大相似度操作快速找到最相关的文档。接下来，rerank机制对这些初步结果进行精细化排序，确保最终呈现给用户的文档是最符合查询意图的。</p>

<p>我们对这一组合策略进行了测试，结果显示，使用ColBERT结合rerank机制不仅大幅度提高了检索的准确性，还进一步优化了查询的响应时间。通过这种多层次的检索与排序方法，我们能够确保检索结果的高精度，同时避免了传统方法中高计算成本和长响应时间的问题。</p>

<p>最终，通过集成ColBERT和rerank机制到我们的TableRAG系统中，我们实现了检索过程中增强信息的有效利用。这一增强策略不仅提升了系统的计算效率和存储利用率，还通过其创新的检索和排序机制，在不牺牲精度的情况下，大幅度提高了检索速度和结果的相关性。这样，我们的系统在处理复杂表格查询时，能够快速且准确地返回最相关的信息，从而显著提升了用户体验和系统的整体性能。</p>

<h1 id="第四部分传入格式增强">第四部分：传入格式增强</h1>

<h2 id="传入给llm的表格格式优化">传入给LLM的表格格式优化</h2>

<p>在进行表格增强和检索的过程中，传入给大型语言模型（LLM）的表格格式对最终的处理效果有着至关重要的影响。已有研究探讨了不同的表格转换方法，并比较了它们对LLM问答系统性能的影响。这些方法包括Markdown格式、模板序列化、传统预训练语言模型（TPLM）方法以及使用大型语言模型（LLM）直接生成文本。研究表明，在不同的范式下，表格转换方法的表现各不相同。</p>

<p>在 <strong>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</strong> 一文中，作者比较了不同表格转换方法在混合数据集上的表现，特别是它们在LLM问答系统中的效果：</p>

<ul>
  <li><strong>Markdown格式</strong>：使用Markdown格式表示表格内容。</li>
  <li><strong>模板序列化</strong>：利用预定义模板将表格转换为文本。</li>
  <li><strong>传统预训练语言模型（TPLM）方法</strong>：使用像T5和BART这样的模型进行表格到文本任务的微调。</li>
  <li><strong>大型语言模型（LLM）方法</strong>：如使用ChatGPT等模型进行一次性文本生成。</li>
</ul>

<p>研究结论显示：</p>

<ul>
  <li>在数据特征学习与迁移（DSFT）范式中，使用语言模型（TPLM和LLM）进行表格到文本转换的方法表现最佳。</li>
  <li>在检索增强生成（RAG）范式中，Markdown格式展现了意想不到的效率，但LLM方法依然表现出色。</li>
</ul>

<blockquote>
  <p><a href="https://arxiv.org/abs/2402.12869">Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</a></p>

</blockquote>

<h2 id="传入格式优化">传入格式优化</h2>

<p>基于上述研究，我们在实验中选择了两种表格格式将其传入LLM，以进一步优化系统的性能：</p>

<ol>
  <li><strong>HTML格式</strong>：HTML格式提供了清晰的结构化表示，使得模型能够准确理解表格的层次和内容关系。这种格式适合在需要保留复杂表格结构时使用，特别是在多维表格或嵌套表格的场景中，HTML格式能有效传达表格的语义信息。</li>
  <li><strong>Markdown格式</strong>：Markdown格式因其简洁性和人类可读性，在各种文本表示任务中广泛使用。研究表明，在RAG范式中，Markdown格式不仅能有效表示表格内容，还能提高模型的处理效率。因此，我们在实验中采用Markdown格式来评估其在实际应用中的表现。</li>
</ol>

<p>通过采用这两种格式，我们希望能够最大限度地发挥LLM在表格处理任务中的潜力。HTML格式的结构化优势和Markdown格式的简洁高效性为我们提供了不同场景下的灵活选择，确保表格内容能够被LLM准确理解和高效处理，从而进一步提高表格问答系统的整体性能。</p>

<p>这种格式优化策略的实施，不仅基于现有研究的理论支持，还在我们的实验中得到了实际验证，为后续的系统开发提供了坚实的基础。我们将继续探索其他可能的格式，以进一步优化表格传入LLM的方式，确保系统在各种复杂场景下都能保持卓越的表现。</p>

<h1 id="评估实验">评估实验</h1>

<h2 id="1-对照实验">1. 对照实验</h2>

<p>对照实验的目的是评估在基础模型上逐步添加各个模块后的性能变化。具体设计如下：</p>

<ul>
  <li><strong>Baseline</strong>（基线模型）：不包含任何额外模块的原始模型，用作参考标准。</li>
  <li><strong>Filter</strong>（过滤器）：在基线模型上逐步添加不同的过滤模块。
    <ul>
      <li><strong>Semantics-based</strong>：这里进一步分为两个小部分：
        <ul>
          <li><strong>Colbert</strong>：加入 Colbert 语义相似度比较模块。</li>
          <li><strong>OpenAI Embedding Model</strong>：加入 OpenAI Embedding Model 进行语义相似度比较的模块。</li>
        </ul>
      </li>
      <li><strong>LLM-based</strong>：加入基于大型语言模型的过滤器。</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>（澄清器）：在基线模型上逐步添加不同的澄清策略。
    <ul>
      <li><strong>Term Exp.</strong>：加入术语扩展模块。</li>
      <li><strong>Table Summary</strong>：加入表格摘要模块。</li>
      <li><strong>Exp. &amp; Summary</strong>（术语扩展与表格摘要组合）：同时加入术语扩展与表格摘要模块。</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>（格式化器）：在基线模型上逐步添加不同的格式化方式。
    <ul>
      <li><strong>String</strong>：使用字符串格式化。</li>
      <li><strong>Markdown</strong>：使用 Markdown 格式化。</li>
      <li><strong>Html</strong>：使用 Html 格式化。</li>
    </ul>
  </li>
  <li><strong>Retriever</strong>（检索器）：在基线模型上测试不同的检索策略，特别是对于 Colbert 模型，还评估了是否使用 rerank 机制对结果进行重新排序的影响。
    <ul>
      <li><strong>BM25</strong>：使用 BM25 进行检索。</li>
      <li><strong>DPR</strong>：使用 DPR 进行检索。</li>
      <li><strong>Colbert</strong>：使用 Colbert 进行检索，同时评估是否使用 rerank 机制对检索结果进行重新排序。</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>（一致性）：在基线模型上测试加入一致性模块后的性能。</li>
</ul>

<h2 id="2-消融实验">2. 消融实验</h2>

<ul>
  <li><strong>Filter</strong>（过滤器）：探讨不同过滤器对模型性能的影响。
    <ul>
      <li><strong>Semantics-based</strong>（语义基础过滤器）：这里进一步分为两个小部分，分别移除使用 Colbert 和 OpenAI Embedding Model 进行语义相似度比较的模块。</li>
      <li><strong>LLM-based</strong>（基于大型语言模型的过滤器）：移除基于LLM的过滤模块。</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>（澄清器）：评估不同澄清策略对模型的贡献。
    <ul>
      <li><strong>Term Exp.</strong>（术语扩展）：移除术语扩展模块。</li>
      <li><strong>Table Summary</strong>（表格摘要）：移除表格摘要模块。</li>
      <li><strong>All Removed</strong>（全部移除）：移除所有澄清相关模块。</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>（格式化器）：测试不同格式化方式对模型的影响。
    <ul>
      <li><strong>Markdown</strong>：移除 Markdown 格式化。</li>
      <li><strong>Html</strong>：移除 Html 格式化。</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>（一致性）：测试模型在没有一致性模块时的性能表现。
    <ol>
      <li>检索器评估</li>
    </ol>
  </li>
</ul>

<p>为了评估不同检索器的召回率，对四个数据集进行了以下实验，并且对每个实验设置开启表格摘要和不开启表格摘要：</p>

<ul>
  <li><strong>BM25</strong>：传统的 TF-IDF 检索器。</li>
  <li><strong>ColBERT</strong>：
    <ul>
      <li>不使用 rerank：直接使用 ColBERT 生成的初始检索结果。</li>
      <li>使用 rerank：对初始检索结果进行重新排序。</li>
    </ul>
  </li>
  <li><strong>DPR</strong>：基于深度学习的稠密向量检索器。</li>
  <li><strong>FASSI 向量数据库</strong>：高效向量检索数据库。</li>
</ul>

<h1 id="acknowledgments">Acknowledgments</h1>
<p>I would like to express my sincere gratitude to the authors of the paper <a href="https://arxiv.org/abs/2312.09039">“Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning”</a> for providing valuable insights that influenced some of the ideas presented in this article. Additionally, I would like to thank PeiMa from the University of Leeds for her significant contributions to this project. Her expertise and support were instrumental in shaping the outcome of this work.</p>

<h3 id="copyright-notice">Copyright Notice</h3>
<p>© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from the author. For inquiries, please contact me at yuhang.wu-4 [at] postgrad.manchester.ac.uk.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Zhang, T., Li, Y., Jin, Y. and Li, J., 2020. Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment. <em>arXiv preprint arXiv:2002.08245</em>.</li>
  <li>Li, L., Wang, H., Zha, L., Huang, Q., Wu, S., Chen, G. and Zhao, J., 2023. Learning a data-driven policy network for pre-training automated feature engineering. In <em>The Eleventh International Conference on Learning Representations</em>.</li>
  <li>Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B. and Wang, W.Y., 2021. Finqa: A dataset of numerical reasoning over financial data. <em>arXiv preprint arXiv:2109.00122</em>.</li>
  <li>Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H. and Wang, W., 2020. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. <em>arXiv preprint arXiv:2004.07347</em>.</li>
  <li>Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F. and Chua, T.S., 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. <em>arXiv preprint arXiv:2105.07624</em>.</li>
  <li>Babaev, D., Savchenko, M., Tuzhilin, A. and Umerenkov, D., 2019, July. Et-rnn: Applying deep learning to credit loan applications. In <em>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> (pp. 2183-2190).</li>
  <li>Ye, Y., Hui, B., Yang, M., Li, B., Huang, F. and Li, Y., 2023, July. Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. In <em>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 174-184).</li>
  <li>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L. and Smith, N.A., 2022. Binding language models in symbolic languages.<em>arXiv preprint arXiv:2210.02875</em>.</li>
  <li>Robertson, S. and Zaragoza, H., 2009. The probabilistic relevance framework: BM25 and beyond. <em>Foundations and Trends® in Information Retrieval</em>, <em>3</em>(4), pp.333-389.</li>
  <li>Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. <em>arXiv preprint arXiv:2004.04906</em>.</li>
  <li>Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J. and Wang, H., 2023. 
Retrieval-augmented generation for large language models: A survey. <em>arXiv preprint arXiv:2312.10997</em>.</li>
  <li>Lu, W., Zhang, J., Zhang, J. and Chen, Y., 2024. Large language model for table processing: A survey. <em>arXiv preprint arXiv:2402.05121</em>.</li>
  <li>Jiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W.X. and Wen, J.R., 2023. Structgpt: A general framework for large language model to reason over structured data. <em>arXiv preprint arXiv:2305.09645</em>.</li>
  <li>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023. Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</li>
  <li>Bian, N., Han, X., Sun, L., Lin, H., Lu, Y., He, B., Jiang, S. and Dong, B., 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. <em>arXiv preprint arXiv:2303.16421</em>.</li>
  <li>Lin, W., Blloshmi, R., Byrne, B., de Gispert, A. and Iglesias, G., 2023, July. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em> (pp. 1557-1566).</li>
  <li>Li, X., Chan, S., Zhu, X., Pei, Y., Ma, Z., Liu, X. and Shah, S., 2023.  Are ChatGPT and GPT-4 general-purpose solvers for financial text analytics? A study on several typical tasks. <em>arXiv preprint arXiv:2305.05862</em>.</li>
  <li>Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X. and Wang, W.Y., 2019. Tabfact: A large-scale dataset for table-based fact verification. <em>arXiv preprint arXiv:1909.02164</em>.</li>
  <li>Aly, R., Guo, Z., Schlichtkrull, M., Thorne, J., Vlachos, A., Christodoulopoulos, C., Cocarascu, O. and Mittal, A., 2021. Feverous: Fact extraction and verification over unstructured and structured information. <em>arXiv preprint arXiv:2106.05707</em>.</li>
  <li>Iyyer, M., Yih, W.T. and Chang, M.W., 2017, July. Search-based neural structured learning for sequential question answering. In <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> (pp. 1821-1831).</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[背景 表格作为一种基础且广泛应用的半结构化数据类型，广泛存在于关系数据库、电子表格应用程序和用于数据处理的编程语言中，涵盖了金融分析（Zhang et al., 2020; Li et al., 2022）、风险管理（Babaev et al., 2019）和医疗保健分析等多个领域。在这些应用中，表格问答（TableQA）是对表格数据进行推理的一个关键下游任务（Ye et al., 2023a; Cheng et al., 2023）。]]></summary></entry><entry><title type="html">RAG Augmentation Methods survey</title><link href="http://localhost:4000/RAG-Augmentation-Methods-survey/" rel="alternate" type="text/html" title="RAG Augmentation Methods survey" /><published>2024-08-15T13:27:00+01:00</published><updated>2024-08-15T13:27:00+01:00</updated><id>http://localhost:4000/RAG-Augmentation-Methods-survey</id><content type="html" xml:base="http://localhost:4000/RAG-Augmentation-Methods-survey/"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>With the development of large-scale language models (LLMs), Retrieval-Augmented Generation (RAG) technology has shown significant advantages in solving complex natural language processing tasks. By combining the capabilities of text generation and information retrieval, RAG greatly enhances model performance, enabling it to provide more accurate and contextually relevant answers in knowledge-intensive tasks. This technology has been widely applied not only in question-answering systems but also in enterprise applications, document management, customer support, and other fields, offering innovative solutions.</p>

<p>However, with the rapid advancement of RAG, numerous frameworks and tools have emerged in the market. Each of these tools has its unique features—some focus on privacy protection, others on optimizing performance and flexibility, while some offer powerful extensibility and integration options. This article provides a comprehensive summary of the current mainstream RAG frameworks and related vector databases, analyzing their strengths and weaknesses to help readers make more informed decisions when selecting and deploying RAG solutions.</p>

<h2 id="rag-frameworks-and-tools">RAG Frameworks and Tools</h2>

<h3 id="1-oobabooga-with-superboogav2">1. Oobabooga with Superboogav2</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/oobabooga/text-generation-webui">Oobabooga GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/oobabooga/text-generation-webui">Oobabooga Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
Oobabooga is an open-source text generation web interface designed to provide a lightweight platform for running various pre-trained large language models (LLMs) on local or remote hosts. Superboogav2, a plugin for Oobabooga, aims to enhance its text generation capabilities. However, this combination has relatively limited functionality for localized Retrieval-Augmented Generation (RAG) applications. Oobabooga is more focused on basic text generation tasks and lacks advanced features for complex document retrieval and question-answering.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>User-Friendly</strong>: Relatively easy to set up and start, making it suitable for beginners.</li>
  <li><strong>Multi-Model Support</strong>: Supports various models and plugins, offering high flexibility for different generation tasks.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Functionality</strong>: Underperforms in complex document retrieval and question-answering tasks, making it difficult to meet advanced RAG requirements.</li>
  <li><strong>Restricted Configuration</strong>: Lacks detailed control over embedding methods and vector storage, making it unsuitable for highly customized applications.</li>
</ul>

<hr />

<h3 id="2-privategpt">2. privateGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/imartinez/privateGPT">privateGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/imartinez/privateGPT">privateGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
privateGPT is a localized RAG framework focused on privacy protection, allowing users to perform question-answering on private documents in an offline environment. This framework is particularly suitable for users with strict data privacy and security requirements. privateGPT supports running entirely locally, ensuring that all operations are conducted offline. However, its architectural design limits its extensibility, leading to underperformance in more complex tasks.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Privacy Protection</strong>: Operates entirely offline, ensuring that data privacy is not compromised by external threats.</li>
  <li><strong>Easy Deployment</strong>: Can be quickly deployed and run in a local environment without the need for an internet connection.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Single Vector Store</strong>: Supports only a single vector store, making it inconvenient to manage multiple document sets.</li>
  <li><strong>Non-Removable Documents</strong>: Once added to the vector store, documents cannot be removed, leading to management challenges.</li>
  <li><strong>Complex GPU Support</strong>: GPU utilization is complex and may lead to performance issues, particularly in lower-end hardware setups.</li>
  <li><strong>Cumbersome Configuration</strong>: Changing models requires manual configuration file edits and restarting the service, lacking a user-friendly configuration interface.</li>
</ul>

<hr />

<h3 id="3-localgpt">3. localGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/PromtEngineer/localGPT">localGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/PromtEngineer/localGPT">localGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
localGPT is a tool dedicated to providing localized RAG capabilities, allowing users to run GPT models locally and perform retrieval and question-answering with private documents. While localGPT has certain advantages in ensuring data security, its user experience primarily relies on the CLI (Command Line Interface), making it less accessible to users unfamiliar with command line operations. Additionally, localGPT’s flexibility in model and embedding configurations is somewhat limited.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Localized Execution</strong>: Supports running RAG applications in a local environment, ensuring data is not exposed externally.</li>
  <li><strong>Flexible Configuration</strong>: Allows users to change embedding methods through code, providing a degree of flexibility, though the process is complex.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Poor User Experience</strong>: All operations must be performed via CLI, lacking an intuitive graphical user interface, raising the barrier to use.</li>
  <li><strong>Complex Model Management</strong>: Changing models and embedding methods requires manual code edits and service restarts, making operations less straightforward.</li>
</ul>

<hr />

<h3 id="4-lmstudio">4. LMStudio</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/lmstudio-ai">LMStudio GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://lmstudio.ai/">LMStudio Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
LMStudio is a powerful text generation platform offering a user-friendly GUI for managing, downloading, and switching large language models. It provides a straightforward model management experience; however, it lacks interaction capabilities with private documents, limiting its utility in RAG applications. Nonetheless, LMStudio remains an excellent tool, especially for users focused on text generation rather than document retrieval.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Powerful GUI</strong>: Allows users to easily manage and switch models through a graphical interface, greatly simplifying the operational process.</li>
  <li><strong>Multi-Model Support</strong>: Supports managing and using various large language models, offering high flexibility.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Lacks Document Interaction</strong>: Does not support interaction with documents, making it less suitable for RAG applications.</li>
  <li><strong>GGUF Model Limitation</strong>: Only supports GGUF format models, which may limit its performance in specific tasks.</li>
</ul>

<hr />

<h3 id="5-ollama">5. OLlama</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/jmorganca/ollama">OLlama GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/jmorganca/ollama">OLlama Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
OLlama is a localized chat framework designed specifically for Mac users, fully leveraging Mac hardware optimization. It supports running large language models locally, providing a responsive chat experience. However, since it is limited to the Mac platform, it cannot meet the needs of Windows users, particularly those looking to utilize high-performance GPUs. Additionally, OLlama’s extensibility is somewhat restricted.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Mac Optimization</strong>: Leverages Mac hardware to provide efficient local chat functionality.</li>
  <li><strong>Ease of Use</strong>: For Mac users, deployment and usage are very convenient, requiring minimal configuration.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Platform Limitation</strong>: Supports only Mac systems, making it unfriendly to Windows and other operating system users, particularly those unable to utilize high-performance GPUs.</li>
  <li><strong>Limited Extensibility</strong>: Platform limitations restrict its broad application in other operating systems or more complex scenarios.</li>
</ul>

<hr />

<h3 id="6-langchain">6. LangChain</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/hwchase17/langchain">LangChain GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/hwchase17/langchain">LangChain Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
LangChain is a framework for building large language model applications, offering a rich set of tools and integration options to help developers create complex language model-based applications. While LangChain is powerful, it is more suitable as a toolkit rather than a complete RAG solution. For users seeking an all-in-one solution, LangChain’s flexibility might become a burden.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Powerful Toolset</strong>: Provides a wide range of APIs and integration options, suitable for developing complex language model applications.</li>
  <li><strong>High Flexibility</strong>: Allows developers to customize applications according to their needs, offering significant design freedom.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Steep Learning Curve</strong>: Due to its extensive functionality, beginners may find it challenging to get started, requiring substantial time and effort to learn and master.</li>
  <li><strong>Not Ideal as a Single Solution</strong>: More suited as a toolkit rather than a complete RAG application, making it difficult to directly apply in production environments.</li>
</ul>

<hr />

<h3 id="7-memgpt">7. MemGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/brown-iv-lab/memgpt">MemGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/brown-iv-lab/memgpt">MemGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
MemGPT is a relatively new project aiming to enhance GPT model performance by integrating memory mechanisms. Although the project is still under development and testing, it offers an interesting perspective on RAG, potentially paving the way for future applications. The specific performance and applicability of MemGPT require further evaluation, but its innovation offers promising potential for future RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Innovative</strong>: Introduces memory mechanisms with the potential to enhance long-term memory and performance, particularly in complex dialogues and document retrieval tasks.</li>
  <li><strong>Potential Applications</strong>: May perform well in the RAG field in the future, especially as the technology further develops and matures.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Still in Development</strong>: Features and performance are not yet mature, requiring further testing and validation; current stability and practicality are uncertain.</li>
  <li><strong>Unknown Usability</strong>: As the project is still in its early stages, it lacks clear documentation and user cases, potentially limiting user experience.</li>
</ul>

<hr />

<h3 id="8-autogpt">8. AutoGPT</h3>

<ul>
  <li><strong>Source Code</strong>: [AutoGPT GitHub](https://github</li>
</ul>

<p>.com/Significant-Gravitas/Auto-GPT)</p>
<ul>
  <li><strong>Website</strong>: <a href="https://github.com/Significant-Gravitas/Auto-GPT">AutoGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
AutoGPT is an autonomous GPT system capable of completing a series of complex tasks, including RAG. In this regard, it is seen as pioneering work, attempting to build AI tools with autonomous capabilities. Nevertheless, AutoGPT’s embedding settings are unchangeable, limiting users’ customization capabilities, especially in specific RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Automation Capability</strong>: Capable of autonomously completing complex task chains, reducing user intervention, and suitable for highly automated application scenarios.</li>
  <li><strong>Pioneering Innovation</strong>: Represents a new exploration direction for automated AI systems, potentially leading to further RAG development in the future.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Configuration</strong>: Embedding settings cannot be changed, restricting the possibility of personalized configuration, making it challenging to meet specific application requirements.</li>
  <li><strong>Complex System</strong>: The system’s complexity is high, possibly requiring users to have a high level of technical expertise to fully utilize its functions, increasing the difficulty of entry.</li>
</ul>

<hr />

<h3 id="9-gpt4all">9. GPT4All</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/nomic-ai/gpt4all">GPT4All GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/nomic-ai/gpt4all">GPT4All Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
GPT4All is an open-source project aimed at providing users with a localized GPT model interaction experience. Its goal is to make large language models available on local computing devices without relying on cloud services. While GPT4All excels at reducing cloud dependency, its current functionality is relatively basic, making it more suitable for basic model interaction rather than a complete RAG application solution.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Localized Execution</strong>: Supports running on local computing devices, reducing dependency on cloud services and enhancing data security and privacy.</li>
  <li><strong>Open Source</strong>: Fully open-source, allowing users to develop and customize it as needed, offering high flexibility.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Functionality</strong>: Currently basic, making it challenging to support complex RAG applications, requiring further refinement and expansion.</li>
  <li><strong>Performance Uncertain</strong>: As the project is still under development, actual performance and applicability remain to be verified, potentially involving some uncertainties.</li>
</ul>

<hr />

<h3 id="10-chatdocs">10. ChatDocs</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/marella/chatdocs">ChatDocs GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/marella/chatdocs">ChatDocs Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
ChatDocs is a derivative project of privateGPT, aiming to improve GPU support and GPTQ model integration. Compared to privateGPT, ChatDocs offers more configuration options, especially in embedding settings. However, these settings still need to be manually modified via files, lacking intuitive GUI support.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Enhanced GPU Support</strong>: Pre-configured with GPU and GPTQ models, significantly improving performance, especially in large-scale data processing.</li>
  <li><strong>Customizable Embedding Settings</strong>: Allows users to change embedding settings, though manual operation is required, providing a degree of flexibility.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Community Support</strong>: Low star count on GitHub suggests low community engagement, potentially affecting user support and assistance.</li>
  <li><strong>Average User Experience</strong>: Although functionality is enhanced, operations still rely on file editing and command lines, making the user experience less friendly, possibly affecting adoption.</li>
</ul>

<hr />

<h3 id="11-docsgpt">11. DocsGPT</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/arc53/DocsGPT">DocsGPT GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/arc53/DocsGPT">DocsGPT Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
DocsGPT is a system focused on document question-answering, designed to extract answers from documents using GPT models. However, its generation speed is slow, and it does not support GPU, limiting its performance in large-scale data processing. It is better suited for small-scale, non-real-time document query tasks.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Specialized Document Q&amp;A</strong>: Optimized for document retrieval and question-answering, suitable for specific applications, especially small-scale knowledge management and query tasks.</li>
  <li><strong>Simple to Use</strong>: For basic document question-answering tasks, the operation is relatively simple, suitable for non-technical users.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Performance</strong>: Due to the lack of GPU support, the generation speed is slow, making it difficult to handle large-scale or real-time tasks, with limited performance in complex scenarios.</li>
  <li><strong>Insufficient Scalability</strong>: Performs poorly in handling complex or large-scale document collections, making it difficult to adapt to diverse application needs.</li>
</ul>

<hr />

<h3 id="12-auto-rag">12. Auto RAG</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/IDSCETHZurich/AutoRAG">Auto RAG GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://github.com/IDSCETHZurich/AutoRAG">Auto RAG Homepage</a></li>
</ul>

<p><strong>Detailed Overview</strong>:<br />
Auto RAG is an automated RAG pipeline selection tool designed to help users choose the best RAG solution based on specific needs. It can automatically generate and select the optimal retrieval-augmented generation strategy based on input data. However, this tool requires a high level of technical expertise from users and requires the use or creation of datasets to be effectively utilized.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Intelligent Pipeline Selection</strong>: Capable of automatically selecting and configuring the best RAG strategy, reducing user manual intervention, and increasing system adaptability and flexibility.</li>
  <li><strong>Targeted Approach</strong>: Provides optimized RAG solutions for specific application scenarios, enhancing application effectiveness and efficiency.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complex Usage</strong>: Requires users to have a high level of technical expertise, making the entry barrier high and unsuitable for users with weaker technical skills.</li>
  <li><strong>Dataset Dependency</strong>: Requires the use or creation of datasets to start, making the operational process more cumbersome, potentially affecting user experience.</li>
</ul>

<hr />

<h2 id="vector-databases">Vector Databases</h2>

<h3 id="1-neo4j">1. Neo4j</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/neo4j/neo4j">Neo4j GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://neo4j.com/">Neo4j Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Neo4j is a graph database specifically designed for handling complex relational data and is widely used in social network analysis, recommendation systems, and other fields. Although it can be used in some RAG scenarios, its characteristics and architecture as a graph database lead to slower performance when handling large-scale vector data, and it supports only limited structure types.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Powerful Relational Data Handling</strong>: Excels in modeling and querying complex relational data, especially suited for applications such as network analysis and recommendation systems.</li>
  <li><strong>Graph Query Language</strong>: Supports Cypher, a query language designed specifically for graph databases, providing powerful data manipulation capabilities.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Performance Issues</strong>: Performance is suboptimal when handling large-scale data, especially vector data.</li>
  <li><strong>Limited Support</strong>: Supports only limited data structure types, imposing certain limitations on its application scenarios.</li>
</ul>

<hr />

<h3 id="2-chroma">2. Chroma</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/chroma-core/chroma">Chroma GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.trychroma.com/">Chroma Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Chroma is a modern vector database specifically designed to simplify vector storage and retrieval. It supports multimodal data and offers rich APIs and built-in embedding functions, making it suitable for rapidly building and scaling RAG applications. Chroma aims to provide simple and easy-to-use configuration options, helping developers quickly implement vector data storage and retrieval.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Easy to Install</strong>: Installation and configuration are relatively simple, relying on Docker or Python, making it easy to quickly deploy and use.</li>
  <li><strong>Highly Configurable</strong>: Offers a wide range of configuration options to meet the needs of different application scenarios.</li>
  <li><strong>Multimodal Support</strong>: Supports the storage and retrieval of multimodal data and offers built-in embedding functions, making it suitable for complex RAG applications.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Docker Dependency</strong>: Requires Docker or Python environment to run, which may increase deployment complexity, especially among non-technical users.</li>
</ul>

<hr />

<h3 id="3-lancedb">3. LanceDB</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/lancedb/lance">LanceDB GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.lancedb.com/">LanceDB Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
LanceDB is a database designed specifically for vector data, known for its extremely fast speed and simple API. It can run on local machines and supports data loading from disk. Even with over a million records, LanceDB’s retrieval speed remains very fast, making it an excellent choice, especially for local applications requiring rapid retrieval.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Fast Speed</strong>: Maintains very fast retrieval speed even when handling large-scale data, making it suitable for applications with high real-time requirements.</li>
  <li><strong>Simple to Use</strong>: Provides a simple and straightforward API, making it easy to integrate and use, reducing development difficulty.</li>
  <li><strong>Local Operation</strong>: Supports running on local machines and loading data from disk, making it suitable for scenarios with large data volumes and requiring efficient retrieval.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Limited Functionality</strong>: Although it excels in retrieval performance, it may fall short in handling more complex application scenarios, offering relatively basic functionality.</li>
</ul>

<hr />

<h3 id="4-pinecone">4. Pinecone</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/pinecone-io">Pinecone GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.pinecone.io/">Pinecone Homepage</a></li>
</ul>

<dl>
  <dt><strong>Introduction</strong></dt>
  <dd>
    <p>Pinecone is a cloud-native vector database designed for large-scale vector retrieval and similarity search. It provides a simple, easy-to-use API and supports fully managed services, making it very suitable for beginners or small-scale projects. However, Pinecone’s retrieval speed may be slower in some application scenarios, and its reliance on cloud services makes it less suitable for users requiring localized solutions.</p>
  </dd>
</dl>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Simple API</strong>: Offers a simple and easy-to-use API, facilitating quick adoption and reducing the difficulty of development and deployment.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Slower Performance</strong>: Retrieval speed may not meet expectations in some complex scenarios, affecting user experience.</li>
  <li><strong>Cloud Dependency</strong>: Primarily relies on cloud services, making it less suitable for users requiring localized or offline solutions.</li>
</ul>

<hr />

<h3 id="5-astradb">5. AstraDB</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/datastax/astra">AstraDB GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://www.datastax.com/products/datastax-astra">AstraDB Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
AstraDB, provided by DataStax, is a cloud database service built on Apache Cassandra, designed to offer highly flexible and fast query performance. AstraDB performs exceptionally well when handling large-scale distributed data and is suitable for applications requiring a serverless architecture. However, due to its powerful features, the learning curve is steep, and it may take considerable time to master.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>High Performance</strong>: Performs exceptionally well in distributed environments, supporting efficient querying and data operations, making it suitable for handling large-scale data.</li>
  <li><strong>High Flexibility</strong>: Supports various data models, adapting to complex application needs.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complexity</strong>: The extensive functionality leads to a high learning curve, potentially requiring significant time to master and fully utilize its capabilities.</li>
</ul>

<hr />

<h3 id="other-commonly-used-rag-vector-databases">Other Commonly Used RAG Vector Databases</h3>

<h3 id="6-milvus">6. Milvus</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/milvus-io/milvus">Milvus GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://milvus.io/">Milvus Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Milvus is an open-source vector database specifically designed for large-scale vector data retrieval and management. It supports various index types and can handle large-scale, high-dimensional data. Milvus offers high scalability, making it particularly suitable for applications requiring the processing of billions of vectors. Due to its powerful features, Milvus has become widely used in RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>High Scalability</strong>: Capable of handling massive vector data, supporting various index types, and suitable for handling high-dimensional, complex datasets.</li>
  <li><strong>Open Source</strong>: Fully open-source, offering flexible deployment options, suitable for a wide range of use cases.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complex Deployment</strong>: Deployment and maintenance can be complex in large-scale environments, requiring high technical capability.</li>
</ul>

<hr />

<h3 id="7-weaviate">7. Weaviate</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/semi-technologies/weaviate">Weaviate GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://weaviate.io/">Weaviate Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Weaviate is an open-source vector database supporting AI model-based automatic classification and similarity search. It has a highly scalable architecture, making it easy to integrate into existing systems. Weaviate supports multimodal data and offers a rich plugin system, making it suitable for applications requiring high customization. Its flexible architecture makes it an ideal choice for building complex RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Highly Scalable</strong>: Supports multimodal data with a flexible plugin system, offering strong adaptability.</li>
  <li><strong>AI Integration</strong>: Supports AI model-driven automatic classification and search, enhancing data processing and retrieval intelligence.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Steep Learning Curve</strong>: Due to its rich and complex functionality, beginners may need more time to master its usage.</li>
</ul>

<hr />

<h3 id="8-faiss">8. Faiss</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/facebookresearch/faiss">Faiss GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://faiss.ai/">Faiss Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Faiss, developed by Facebook AI Research, is a vector similarity search library specifically designed for efficient vector similarity search. It can run on both CPU and GPU, making it suitable for handling large-scale, high-dimensional datasets. Faiss is a very popular choice in RAG applications, especially in scenarios requiring high performance. Although its performance is powerful, as a library, integrating it into existing systems may require additional development work.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Efficient Search Performance</strong>: Performs exceptionally well when handling large-scale, high-dimensional vector data, making it suitable for high-performance applications.</li>
  <li><strong>GPU Support</strong>: Supports running on GPU, significantly enhancing processing speed, making it suitable for complex tasks requiring efficient processing.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Complex Integration</strong>: As a library rather than a complete database solution, integrating it into systems may require additional development work, increasing the difficulty of use.</li>
</ul>

<hr />

<h3 id="9-qdrant">9. Qdrant</h3>

<ul>
  <li><strong>Source Code</strong>: <a href="https://github.com/qdrant/qdrant">Qdrant GitHub</a></li>
  <li><strong>Website</strong>: <a href="https://qdrant.tech/">Qdrant Homepage</a></li>
</ul>

<p><strong>Introduction</strong>:<br />
Qdrant is an open-source vector database focused on fast and efficient vector retrieval. It supports access through REST API, making it easy to integrate into various applications. Qdrant provides strong vector search and filtering capabilities, suitable for applications such as real-time recommendation and personalized search. Due to its simple design and efficient performance, Qdrant has become a popular choice for building real-time RAG applications.</p>

<p><strong>Pros</strong>:</p>
<ul>
  <li><strong>Efficient Retrieval</strong>: Supports fast vector retrieval and filtering, making it suitable for applications with high real-time requirements.</li>
  <li><strong>Easy Integration</strong>: Provides a REST API, making integration simple and suitable for rapid development and deployment.</li>
</ul>

<p><strong>Cons</strong>:</p>
<ul>
  <li><strong>Relatively Basic Functionality</strong>: While it excels in retrieval performance, its functionality is relatively simple, suitable for specific scenarios, and may struggle to meet more complex needs.</li>
</ul>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">RAG Augmentation Methods survey(CN)</title><link href="http://localhost:4000/RAG-Augmentation-Methods-survey-CN/" rel="alternate" type="text/html" title="RAG Augmentation Methods survey(CN)" /><published>2024-08-15T12:30:00+01:00</published><updated>2024-08-15T12:30:00+01:00</updated><id>http://localhost:4000/RAG-Augmentation-Methods-survey-CN</id><content type="html" xml:base="http://localhost:4000/RAG-Augmentation-Methods-survey-CN/"><![CDATA[<h1 id="目录">目录</h1>

<ol>
  <li><a href="#rag-基础范式">RAG 基础范式</a>
    <ul>
      <li>1.1 <a href="#稀疏检索方法">稀疏检索方法</a></li>
      <li>1.2 <a href="#密集检索方法">密集检索方法</a></li>
      <li>1.3 <a href="#检索器设计">检索器设计</a></li>
      <li>1.4 <a href="#其他相关方法">其他相关方法</a></li>
    </ul>
  </li>
  <li><a href="#前检索与后检索增强">前检索与后检索增强</a>
    <ul>
      <li>2.1 <a href="#前检索增强方法">前检索增强方法</a></li>
      <li>2.2 <a href="#后检索增强方法">后检索增强方法</a></li>
    </ul>
  </li>
  <li><a href="#生成与生成增强">生成与生成增强</a>
    <ul>
      <li>3.1 <a href="#生成器类型">生成器类型</a></li>
      <li>3.2 <a href="#生成器增强方法">生成器增强方法</a></li>
      <li>3.3 <a href="#检索与生成的集成增强">检索与生成的集成增强</a></li>
    </ul>
  </li>
  <li><a href="#检索与生成流程优化">检索与生成流程优化</a>
    <ul>
      <li>4.1 <a href="#检索必要性与频率">检索必要性与频率</a></li>
      <li>4.2 <a href="#无训练与独立训练">无训练与独立训练</a></li>
      <li>4.3 <a href="#顺序训练与联合训练">顺序训练与联合训练</a></li>
    </ul>
  </li>
  <li><a href="#rag-在应用中的实例">RAG 在应用中的实例</a>
    <ul>
      <li>5.1 <a href="#nlp-应用">NLP 应用</a></li>
      <li>5.2 <a href="#下游任务应用">下游任务应用</a></li>
      <li>5.3 <a href="#特定领域应用">特定领域应用</a></li>
    </ul>
  </li>
  <li><a href="#讨论与未来研究方向">讨论与未来研究方向</a>
    <ul>
      <li>6.1 <a href="#rag-的局限性">RAG 的局限性</a></li>
      <li>6.2 <a href="#未来潜在研究方向">未来潜在研究方向</a></li>
    </ul>
  </li>
</ol>

<hr />
<h2 id="引言">引言</h2>

<p>近年来，基于检索增强生成（Retrieval-Augmented Generation, RAG）技术的模型在自然语言处理领域取得了显著进展。RAG技术通过结合信息检索与文本生成的优势，有效解决了传统生成模型中常见的“幻觉”问题，即模型生成的内容与事实不符。此外，RAG技术无需依赖大规模的微调或预训练过程，这不仅降低了对计算资源的需求，还显著提升了模型的灵活性和适应性。更为重要的是，RAG技术能够直接从外部语料库中检索所需信息，而无需预先构建和维护一个庞大的知识库。这种“即插即用”的特性使得RAG系统在许多实际应用中表现出色。然而，尽管RAG技术的实现并不复杂，但要构建一个性能卓越且稳定的RAG系统却充满挑战。为了深入理解和优化RAG技术，本文对当前RAG系统中关键组件的设计与增强方法进行了系统总结，并提出了若干潜在的优化策略，以期为未来的研究和应用提供参考。</p>

<hr />

<h2 id="1-rag-基础范式">1. RAG 基础范式</h2>

<p>RAG系统的设计涵盖了多种范式，通过将稀疏检索和密集检索方法有机结合，实现了信息检索和文本生成的高效协同。优化检索器的设计，并引入前检索与后检索增强技术，可以显著提升生成器的表现。</p>

<h3 id="11-稀疏检索方法">1.1 稀疏检索方法</h3>

<ul>
  <li>
    <p><strong>TF-IDF</strong>：TF-IDF（Term Frequency-Inverse Document Frequency）是一种基于词频和倒排索引的传统文本检索方法。它通过衡量词语在文档中的重要性来定位与查询最相关的文档。在RAG系统中，TF-IDF常用于段落级别的检索，以增强生成器输入的质量。然而，由于该方法依赖于词语的表层频率，无法充分捕捉词语的语义信息，因此其性能在处理复杂查询时可能受到限制。</p>
  </li>
  <li>
    <p><strong>BM25</strong>：BM25是另一种基于词频和倒排索引的检索方法，它将文档表示为词袋模型，并根据词频和逆文档频率对文档进行排序。BM25在大规模文本数据库的查询中表现尤为突出，是RAG模型中广泛应用的段落检索方法之一。与TF-IDF相比，BM25通过引入参数调整了词语的频率效应，从而在一定程度上提高了检索的鲁棒性。</p>
  </li>
</ul>

<h3 id="12-密集检索方法">1.2 密集检索方法</h3>

<ul>
  <li>
    <p><strong>DPR（Dense Passage Retriever）</strong>：DPR是一种基于BERT结构的密集检索器，专为开放问答任务设计并预训练。DPR通过将查询和文档嵌入到相同的向量空间，并基于嵌入向量的语义相似性进行检索，展现出强大的预训练能力。这种方法在许多RAG模型中作为关键组件，显著提升了模型在处理复杂语义查询时的表现。</p>
  </li>
  <li>
    <p><strong>Contriever</strong>：Contriever是一种基于对比学习的密集检索器，采用单编码器结构，通过在大规模未对齐文档上进行预训练，展示了在开放域问答任务中的优异性能。Contriever特别适合与InstructGPT等大模型配合使用，在多样化任务中的表现卓越。</p>
  </li>
  <li>
    <p><strong>Spider</strong>：Spider与Contriever类似，作为一种通用预训练的密集检索器，通过对比学习进行预训练，能够适应多种任务和领域。Spider的灵活性和高效性使其在多种RAG方法中得到广泛应用，并且在处理大规模语料库时表现出色。</p>
  </li>
</ul>

<h3 id="13-检索器设计">1.3 检索器设计</h3>

<ul>
  <li>
    <p><strong>双流编码器（Bi-Encoder）</strong>：双流编码器由两个独立的BERT结构编码器组成，一个用于处理查询，另一个用于处理文档。该设计常用于句子嵌入的相似性检索以及多样示例检索，能够通过冻结或部分冻结参数，实现通用知识的提取与应用。</p>
  </li>
  <li>
    <p><strong>单编码器（One-Encoder）</strong>：单编码器结构通过一个编码器同时处理查询和文档，通常基于Transformer、BERT或其他序列建模结构。通过在大规模未对齐文档上进行对比学习预训练，单编码器展示了出色的适应性和泛化能力，能够灵活应对多种任务需求。</p>
  </li>
</ul>

<h3 id="14-其他相关方法">1.4 其他相关方法</h3>

<ul>
  <li>
    <p><strong>对比学习</strong>：对比学习是一种常用于单编码器结构预训练的方法，通过学习正负样本对的嵌入，使得模型能够在未对齐文档上进行有效训练。此方法在提升模型适应性和泛化能力方面表现突出，特别是在处理多样化和复杂语料时效果显著。</p>
  </li>
  <li>
    <p><strong>大规模专门预训练</strong>：针对特定任务（如开放问答）的数据进行大规模预训练，可以显著增强模型在知识密集型任务中的表现。DPR通过在开放问答任务上的专门预训练，极大提高了模型处理特定领域问题的准确性。</p>
  </li>
</ul>

<h3 id="方法总结">方法总结</h3>

<p>RAG系统中的检索方法主要分为稀疏检索和密集检索。稀疏检索方法如TF-IDF和BM25依赖于词频和倒排索引，适用于一般的文本检索任务。然而，这类方法的性能受限于数据库和查询的质量。相较之下，密集检索方法如DPR、Contriever和Spider通过将查询和文档嵌入到向量空间中，利用语义相似性进行检索，展现出更大的灵活性和适应性。此外，通过双流编码器和单编码器等检索器设计，并结合对比学习与大规模专门预训练，进一步提升了模型在各种任务中的表现。</p>

<hr />

<h2 id="2-前检索与后检索增强">2. 前检索与后检索增强</h2>

<p>为了进一步提升检索质量并优化生成器的输入与输出，前检索与后检索增强策略在RAG系统中得到了广泛应用。这些增强策略不仅改善了检索结果的质量，还显著提升了生成器的生成效果。</p>

<h3 id="21-前检索增强方法">2.1 前检索增强方法</h3>

<p><strong>Query2doc</strong>：</p>
<ul>
  <li>Wang等人提出了一种查询扩展方法，通过少量提示大规模语言模型（LLM）生成伪文档，将伪文档中的相关信息扩展至原始查询。这种方法能够有效改进查询的消歧能力，并为检索器提供更加明确的检索目标。在临时信息检索数据集上的实验表明，Query2doc显著提升了稀疏和密集检索器的表现。</li>
</ul>

<p><strong>Hypothetical Document Embedding （HyDE）</strong>：</p>
<ul>
  <li>Gao等人提出的HyDE方法通过引导LLM生成假设文档，并将这些文档作为新的查询嵌入，从而搜索相关邻居，进一步提升了检索的准确性和相关性。HyDE方法在处理复杂查询时表现出色，特别是在缺乏明确上下文的情况下，显著提高了检索效果。</li>
</ul>

<p><strong>Query Rewrite</strong>：</p>
<ul>
  <li>Ma等人提出了Rewrite-Retrieve-Read框架，通过提示LLM生成检索查询，并改写原始问题，以更好地匹配检索需求。这种方法不仅增强了检索器对输入的理解，还显著提高了生成输出的相关性和一致性。</li>
</ul>

<p><strong>Query Augmentation</strong>：</p>
<ul>
  <li>Yu等人提出的Query Augmentation方法通过将原始查询与初步生成的输出结合，形成新的查询以进一步检索相关信息。这种方法在澄清查询与生成内容之间的关系方面表现出色，能够帮助从语料库中提取更多相关信息，从而提升生成内容的完整性和准确性。</li>
</ul>

<h3 id="22-后检索增强方法">2.2 后检索增强方法</h3>

<p><strong>Pluggable Reward-driven Context Adapter （PRCA）</strong>：</p>
<ul>
  <li>Yang等人提出了一种基于强化学习的后检索增强方法，通过调整检索到的文档并微调轻量级适配器，使其更好地与生成器对齐，从而提升生成内容的质量和一致性。PRCA方法能够有效减少生成过程中出现的无关信息，并提高生成的流畅性。</li>
</ul>

<p><strong>Retrieve-Rerank-Generate （R2G）</strong>：</p>
<ul>
  <li>Glass等人提出的R2G方法通过对不同检索方法获得的文档进行重新排序，提高了检索结果的鲁棒性，并优化了最终生成的答案。该方法在多模态检索任务中表现出色，显著提升了生成结果的准确性和相关性。</li>
</ul>

<p><strong>BlendFilter</strong>：</p>
<ul>
  <li>Wang等人提出的BlendFilter方法结合前检索查询生成混合和后检索知识过滤，能够更好地处理复杂问题和噪声检索知识，从而增强生成内容的准确性。BlendFilter在处理长尾查询时表现尤为突出，显著减少了生成中的</li>
</ul>

<p>误差。</p>

<p><strong>Retrieve, Compress, Prepend （RECOMP）</strong>：</p>
<ul>
  <li>Xu等人提出的RECOMP方法在生成过程的上下文增强之前，对检索到的文档进行文本摘要处理，从而减少生成过程中的冗余信息，提高生成内容的相关性和紧凑性。</li>
</ul>

<p><strong>FiD（Fusion-in-Decoder）模型的轻量版</strong>：</p>
<ul>
  <li>Hofstätter等人提出了FiD模型的轻量版本，通过在拼接和解码器处理之前压缩每个检索段落的编码向量，并对检索结果重新排序，从而优化生成器的性能。这种方法有效减少了计算成本，并提高了生成过程的效率。</li>
</ul>

<h3 id="方法总结-1">方法总结</h3>

<p>前检索增强和后检索增强方法在提高RAG系统整体性能方面发挥了重要作用。前检索增强方法如Query2doc、HyDE、Query Rewrite和Query Augmentation通过改进查询表达，提升了检索器在处理复杂查询时的表现。而后检索增强方法如PRCA、R2G、BlendFilter、RECOMP和FiD轻量版则通过优化检索结果的排序和处理，进一步提升了生成器的输出质量。这些方法相辅相成，共同促进了RAG系统生成效果的提升。</p>

<hr />

<h2 id="3-生成与生成增强">3. 生成与生成增强</h2>

<p>生成器在RAG系统中扮演着至关重要的角色。通过优化生成器的设计与增强，可以显著提高生成文本的准确性和相关性。</p>

<h3 id="31-生成器类型">3.1 生成器类型</h3>

<p>生成器类型可大致分为参数可访问（白盒）和参数不可访问（黑盒）两大类。</p>

<p><strong>参数可访问生成器（白盒）</strong>：</p>

<ul>
  <li>
    <p><strong>编码器-解码器结构（Encoder-Decoder）</strong>：该结构独立处理输入和目标，并使用交叉注意力组件将输入与目标标记连接。代表性模型包括T5和BART。RAG系统中通常使用BART作为生成器，而FiD则使用T5作为生成器，以实现更高的生成质量。</p>
  </li>
  <li>
    <p><strong>仅解码器结构（Decoder-only）</strong>：仅解码器结构将输入和目标连接，使两部分表示并行构建，允许模型在生成过程中灵活调整生成策略。这种结构通过单一的解码器实现生成过程，具有较高的生成灵活性。</p>
  </li>
</ul>

<p><strong>参数不可访问生成器（黑盒）</strong>：</p>

<ul>
  <li>
    <p>代表模型包括GPT系列、Codex和Claude。这类生成器仅允许输入查询并接收响应，无法对其内部结构或参数进行修改。尽管如此，黑盒生成器仍通过丰富的预训练和强大的生成能力，在多样化任务中表现出色。</p>
  </li>
  <li>
    <p><strong>提示检索器</strong>：Rubin等人提出了一种训练提示检索器的方法，利用语言模型生成的数据，为生成器提供更好的示例用于上下文学习，进一步提升了生成质量。</p>
  </li>
  <li>
    <p><strong>文档压缩</strong>：Xu等人提出的方法通过在上下文集成之前对检索到的文档进行压缩，减少了计算成本，并减轻了生成器在处理大规模上下文时的负担。</p>
  </li>
</ul>

<h3 id="32-生成器增强方法">3.2 生成器增强方法</h3>

<p>生成器的性能直接影响RAG系统的最终输出质量。通过提示工程、解码调优和生成器微调，可以进一步提升生成器的表现。</p>

<p><strong>提示工程</strong>：</p>

<ul>
  <li>
    <p><strong>LLMLingua</strong>：通过一个小模型压缩查询长度，加快模型推理速度，减轻无关信息对模型的负面影响，并缓解生成过程中的“中间丢失”现象。</p>
  </li>
  <li>
    <p><strong>ReMoDiffuse</strong>：使用ChatGPT将复杂描述分解为清晰的文本脚本，提升了生成文本的准确性和一致性。</p>
  </li>
  <li>
    <p><strong>ASAP</strong>：将示例元组（包括输入代码、函数定义、分析结果和相应的评论）整合到提示中，以提高生成质量。</p>
  </li>
</ul>

<p><strong>解码调优</strong>：</p>

<ul>
  <li>
    <p><strong>InferFix</strong>：通过调整解码器的温度，平衡生成内容的多样性和质量，确保生成内容既准确又多样。</p>
  </li>
  <li>
    <p><strong>SYNCHROMESH</strong>：通过限制解码器的输出词汇，消除潜在的实现错误，提高生成器的可靠性和稳定性。</p>
  </li>
</ul>

<p><strong>生成器微调</strong>：</p>

<ul>
  <li>
    <p><strong>RETRO</strong>：固定检索器参数，利用块状交叉注意机制将查询和检索内容结合，提高生成效果。</p>
  </li>
  <li>
    <p><strong>APICoder</strong>：通过混合API信息和代码块微调生成器，提升代码生成任务的准确性和一致性。</p>
  </li>
  <li>
    <p><strong>CARE</strong>：通过微调解码器，减少字幕和概念检测损失，同时保持编码器和检索器的参数固定，从而优化多模态生成任务的性能。</p>
  </li>
</ul>

<h3 id="33-检索与生成的集成增强">3.3 检索与生成的集成增强</h3>

<p>检索与生成的集成是RAG系统的关键环节。通过在输入层、输出层和中间层进行增强设计，可以显著提升模型的整体性能。</p>

<p><strong>输入层集成</strong>：</p>

<ul>
  <li>输入层集成方法通过将检索到的文档与原始输入或查询结合，并传递给生成器。这种方法在In-Context RALM、FiD、Atlas和REPLUG等模型中应用广泛，通过串联输入和检索到的文档，使生成器能够更好地处理复杂任务。</li>
</ul>

<p><strong>输出层集成</strong>：</p>

<ul>
  <li>输出层集成方法通过结合检索和生成结果，提升生成内容的质量。kNN-LM在预测中插值两个下一个标记的分布，一个由语言模型引导，另一个由检索语料库中的最近邻引导，从而实现更灵活的生成过程。</li>
</ul>

<p><strong>中间层集成</strong>：</p>

<ul>
  <li>中间层集成方法通过引入半参数模块，将检索到的信息引入生成模型的内部层，与生成过程中间的表示进行交互。尽管这种集成方法增加了模型的复杂性，但通过有效的训练，能够显著增强生成模型的能力。例如，RETRO通过块状交叉注意层处理生成器块中的检索块，而EAE和TOME则通过实体记忆和记忆注意层整合检索到的实体和提及。</li>
</ul>

<h3 id="方法总结-2">方法总结</h3>

<p>生成器的设计与增强在RAG系统中至关重要。通过选择合适的生成器类型、优化提示工程、调优解码过程和微调生成器，可以有效提升生成器的性能。此外，通过输入层、输出层和中间层的集成增强设计，进一步优化了检索与生成的协同作用，从而提高了RAG系统的整体生成质量。</p>

<hr />

<h2 id="4-检索与生成流程优化">4. 检索与生成流程优化</h2>

<p>RAG系统中的检索与生成流程可以通过优化设计进一步提升模型的效率和准确性。以下内容介绍了检索必要性与频率、无训练与独立训练、顺序训练与联合训练等方面的流程优化方法。</p>

<h3 id="41-检索必要性与频率">4.1 检索必要性与频率</h3>

<p>在基于LLM的生成过程中，检索操作通常用于补充知识，以增强生成内容的准确性。然而，检索并非总是必要，且过度检索可能引入不相关信息，导致生成错误。因此，如何确定检索的必要性和频率，成为实现鲁棒RAG模型的关键。</p>

<p><strong>Self-RAG</strong>：</p>
<ul>
  <li>Self-RAG通过引入特殊标记，评估检索的必要性，并根据需要控制检索行为，从而减少不必要的资源消耗。这种方法在生成过程中通过动态调整检索频率，有效优化了模型的效率。</li>
</ul>

<p><strong>SKR（Self-Knowledge guided Retrieval）</strong>：</p>
<ul>
  <li>SKR利用LLMs的自我判断能力来决定是否调用检索器，从而动态调整检索行为，减少不必要的查询和计算。</li>
</ul>

<p><strong>FLARE</strong>：</p>
<ul>
  <li>FLARE在生成过程中基于概率主动决定是否以及何时进行搜索，以避免过度检索，优化计算资源的使用。</li>
</ul>

<p><strong>检索频率设计</strong>：</p>
<ul>
  <li>在生成过程中，检索频率决定了RAG模型对检索结果的依赖程度，影响模型的效率和有效性。常见设置包括一次性检索、每n个标记检索和每个标记检索。不同频率的检索策略在性能和计算成本之间进行权衡，适用于诸如REALM、In-Context RALM和RETRO等模型。</li>
</ul>

<h3 id="42-无训练与独立训练">4.2 无训练与独立训练</h3>

<p><strong>无需训练的方法</strong>：</p>
<ul>
  <li>随着LLMs的发展，许多研究建议通过检索机制增强LLMs，而无需对模型参数进行微调。基于提示工程的方法直接将检索到的知识集成到原始提示中，从而改进生成质量。例如，In-Context RAG通过将检索到的文档与原始提示结合，提升生成效果。</li>
</ul>

<p><strong>独立训练方法</strong>：</p>
<ul>
  <li>在独立训练中，检索器和生成器作为两个完全独立的模块进行训练，无需在训练过程中进行交互。DPR使用BERT进行对比学习训练，而CoG则通过前缀编码器和短语编码器的训练提升文本生成的准确性。</li>
</ul>

<h3 id="43-顺序训练与联合训练">4.3 顺序训练与联合训练</h3>

<p><strong>顺序训练</strong>：</p>
<ul>
  <li>顺序训练</li>
</ul>

<p>方法通过先独立预训练检索器或生成器，然后固定预训练模块，再进行另一个模块的训练。RETRO采用BERT模型作为预训练检索器，并使用编码器-解码器结构将检索块集成到模型的预测中。</p>

<p><strong>联合训练</strong>：</p>
<ul>
  <li>联合训练方法采用端到端训练范式，同时优化检索器和生成器。RAG通过最小化负对数似然损失，联合训练检索器和生成器，从而提升系统的整体性能。REALM采用类似的训练范式，并通过最大内积搜索技术定位最相关的文档。</li>
</ul>

<h3 id="方法总结-3">方法总结</h3>

<p>通过控制检索的必要性与频率、选择无训练与独立训练策略，以及优化顺序训练与联合训练流程，可以显著提升RAG系统的整体性能。这些流程优化方法不仅增强了模型的生成效果，还在计算效率和资源利用上取得了良好的平衡。</p>

<hr />

<h2 id="5-rag-在应用中的实例">5. RAG 在应用中的实例</h2>

<p>RAG技术在自然语言处理（NLP）、下游任务和特定领域中的广泛应用展示了其强大的适应性和高效性。以下内容介绍了RAG在这些领域中的具体应用实例。</p>

<h3 id="51-nlp-应用">5.1 NLP 应用</h3>

<p><strong>问答系统</strong>：</p>
<ul>
  <li>问答系统通过集成RAG技术，能够显著提升回答的准确性和上下文相关性。REALM在预训练、微调和推理过程中集成知识检索器，从大型语料库中检索信息，显著提高了系统的响应质量。Fusion-in-Decoder通过从支持文档中检索段落并与问题融合生成答案，实现了更高的准确性。</li>
</ul>

<p><strong>聊天机器人</strong>：</p>
<ul>
  <li>聊天机器人需要保持与用户的自然对话。通过集成RAG技术，聊天机器人可以从静态数据库或互联网中检索相关信息，增强对话的丰富性和连贯性。例如，BlenderBot3结合互联网信息和本地对话历史，显著提升了对话的生成质量。</li>
</ul>

<p><strong>事实验证</strong>：</p>
<ul>
  <li>事实验证任务旨在验证信息的准确性。RAG技术通过检索外部知识，增强了事实验证的能力。Atlas在少样本学习下验证了RAG技术在事实验证中的性能，展示了显著的进步。</li>
</ul>

<h3 id="52-下游任务应用">5.2 下游任务应用</h3>

<p><strong>推荐系统</strong>：</p>
<ul>
  <li>RAG技术在推荐系统中展示了巨大的潜力，通过整合检索和生成过程，提供个性化和上下文相关的推荐。例如，Di Palma提出的检索增强推荐模型利用电影或书籍数据集中的知识进行推荐，显著提升了系统的推荐精度。</li>
</ul>

<p><strong>软件工程</strong>：</p>
<ul>
  <li>RAG技术在软件工程中的应用包括代码生成和程序修复。例如，APICoder通过在代码库中检索相关代码片段并与输入聚合，提升了代码生成的准确性和效率。此外，RAG技术还在表格数据处理和Text-to-SQL语义解析方面展示了巨大的潜力。</li>
</ul>

<h3 id="53-特定领域应用">5.3 特定领域应用</h3>

<p><strong>医疗领域</strong>：</p>
<ul>
  <li>RAG技术在医疗领域的应用包括药物发现和分子分析。通过整合多种数据模态（如图像、文本、分子结构等），RAG技术能够提供更全面的分析和诊断支持。</li>
</ul>

<p><strong>法律领域</strong>：</p>
<ul>
  <li>RAG技术在法律领域中的应用主要体现在法律文献的检索和判例的生成上。通过结合法律知识库和案例数据库，RAG系统能够为法律从业者提供准确的法律建议和参考。</li>
</ul>

<h3 id="方法总结-4">方法总结</h3>

<p>RAG技术在自然语言处理、下游任务和特定领域的应用展示了其广泛的适应性和强大的处理能力。通过将检索与生成过程无缝结合，RAG系统能够在多种任务中提供高效且精准的支持。</p>

<hr />

<h2 id="6-讨论与未来研究方向">6. 讨论与未来研究方向</h2>

<h3 id="61-rag-的局限性">6.1 RAG 的局限性</h3>

<p>尽管RAG技术在众多领域展现出卓越的性能，但其自身也存在一些局限性：</p>

<p><strong>检索结果中的噪声</strong>：</p>
<ul>
  <li>信息检索过程可能会引入噪声，这些噪声在某些情况下可能干扰生成过程。然而，有研究表明，在某些情况下，噪声检索结果反而有助于提升生成质量。</li>
</ul>

<p><strong>额外开销</strong>：</p>
<ul>
  <li>RAG系统的检索过程会带来额外的计算开销，尤其是在需要频繁检索大规模数据时。这种开销会影响系统的实时性和计算资源的利用效率。</li>
</ul>

<p><strong>检索器和生成器之间的差距</strong>：</p>
<ul>
  <li>由于检索器和生成器的目标不完全一致，设计它们的交互需要精心的优化。尽管一些方法通过联合训练来减少这种差距，但这增加了系统的复杂性。</li>
</ul>

<p><strong>系统复杂性的增加</strong>：</p>
<ul>
  <li>RAG系统中引入检索功能会增加整体系统的复杂性，尤其是在需要对系统进行调整和优化时，要求更高的技术知识和经验。</li>
</ul>

<p><strong>上下文长度的增加</strong>：</p>
<ul>
  <li>基于查询的RAG系统会显著增加上下文长度，这可能会对生成器的性能产生负面影响，并减慢生成速度。</li>
</ul>

<h3 id="62-未来潜在研究方向">6.2 未来潜在研究方向</h3>

<p>为了进一步提升RAG技术的应用价值，未来的研究可以从以下几个方向展开：</p>

<p><strong>新型增强方法的设计</strong>：</p>
<ul>
  <li>现有研究已经探索了多种检索器与生成器的交互模式。未来的研究可以探索更为先进的增强方法，以充分释放RAG系统的潜力。</li>
</ul>

<p><strong>灵活的RAG流程</strong>：</p>
<ul>
  <li>通过递归、自适应和迭代RAG流程，未来的RAG系统可以实现更加精细化的任务处理和性能提升。</li>
</ul>

<p><strong>更广泛的应用</strong>：</p>
<ul>
  <li>虽然RAG技术已经在多个领域得到了应用，但未来可以进一步推广到更多应用场景，特别是在一些尚未充分探索的领域。</li>
</ul>

<p><strong>高效的部署和处理</strong>：</p>
<ul>
  <li>未来的研究应关注开发更加即插即用的RAG解决方案，以优化系统级别的部署效率和处理性能。</li>
</ul>

<p><strong>结合长尾和实时知识</strong>：</p>
<ul>
  <li>为了提高个性化信息服务的能力，可以设计具有持续更新知识库和适应实时信息的RAG系统。</li>
</ul>

<p><strong>与其他技术结合</strong>：</p>
<ul>
  <li>未来的研究可以探索RAG技术与其他提高AI生成内容（AIGC）有效性的技术（如微调、强化学习、链式思考等）的结合，以进一步提高生成效果。</li>
</ul>

<h1 id="to-be-continued">(To be continued…)</h1>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>I would like to express my deep gratitude to the authors of several key surveys that have been instrumental in the development of this work. Their comprehensive analyses and insights into Retrieval-Augmented Generation (RAG) technology have provided a robust foundation for my understanding and exploration of this rapidly evolving field.</p>

<p>In particular, the surveys on “<a href="(https://arxiv.org/abs/2404.10981)">Retrieval-Augmented Text Generation for Large Language Models” (arXiv:2404.10981)</a>, “<a href="(https://arxiv.org/abs/2402.19473)">Retrieval-Augmented Generation for AI-Generated Content” (arXiv:2402.19473)</a>, and “<a href="https://arxiv.org/abs/2405.06211">RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models” (arXiv:2405.06211)</a> have been invaluable resources. These works have significantly shaped my understanding of the current state of RAG technologies, their challenges, and their potential applications. I am deeply indebted to the authors for their contributions, which have greatly informed and guided the research presented in this paper.</p>

<h2 id="reference">Reference</h2>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[目录]]></summary></entry><entry><title type="html">LLM for table data enhancement</title><link href="http://localhost:4000/LLM-for-table-data-enhancement/" rel="alternate" type="text/html" title="LLM for table data enhancement" /><published>2024-08-15T09:19:00+01:00</published><updated>2024-08-15T09:19:00+01:00</updated><id>http://localhost:4000/LLM-for-table-data-enhancement</id><content type="html" xml:base="http://localhost:4000/LLM-for-table-data-enhancement/"><![CDATA[<p>With the widespread application of large language models (LLMs) in the field of natural language processing (NLP), these models have demonstrated remarkable capabilities across various tasks, including text generation, question answering systems, and sentiment analysis. However, despite their outstanding performance in handling unstructured data, LLMs still face numerous challenges when processing structured data, particularly tabular data. The structured nature of tabular data and its rich semantic information impose higher demands on LLMs, rendering traditional text processing methods often inapplicable to such data.</p>

<p>This article aims to summarize and discuss the key technologies and methodologies for processing tabular data. We conduct an in-depth analysis of significant literature and methods related to LLMs’ handling of tabular data. These studies attempt to address the challenges encountered by LLMs in processing tabular data, including encoding, querying, and generating tabular data. Through a detailed exploration of technologies such as ColBERT, ColBERTv2, DPR, and RAFT, we present the major advancements and innovations in the field of tabular data processing by LLMs. These technologies not only enhance the understanding and retrieval capabilities of tabular data but also provide important references for future research.</p>

<h2 id="llm-for-table">LLM for Table</h2>

<p><a href="https://arxiv.org/pdf/2402.05121">Large Language Model for Table Processing</a></p>

<h3 id="1-classification-of-llm-methods-for-table-processing">1. Classification of LLM Methods for Table Processing</h3>

<p>Research on processing tabular data primarily focuses on two major approaches: training-based methods and prompt-based methods, specifically including:</p>

<ol>
  <li><strong>Training-based Methods</strong>:
    <ul>
      <li><strong>Task-specific fine-tuning</strong>: Examples include TaPas and TaBERT, which enhance the performance of table-related tasks by adjusting the model architecture and training objectives.</li>
      <li><strong>Instruction fine-tuning</strong>: Techniques like TableLlama and Table-GPT improve the model’s performance on unseen tasks through fine-tuning on multiple datasets.</li>
      <li><strong>Retrieval-augmented methods</strong>: ITR and LI-RAGE, for instance, split large tables into sub-tables and jointly train retrievers and readers.</li>
    </ul>
  </li>
  <li><strong>Prompt-based Methods</strong>:
    <ul>
      <li><strong>Table serialization</strong>: This involves converting tables into a linear text format to make them more accessible to LLMs.</li>
      <li><strong>Example selection for few-shot learning</strong>: Selecting examples most relevant to the target task to improve model performance.</li>
    </ul>
  </li>
  <li><strong>Agent-based Methods</strong>:
    <ul>
      <li><strong>Decomposition of complex tasks</strong>: Techniques like DIN-SQL enhance accuracy by breaking down complex tasks into smaller sub-tasks.</li>
      <li><strong>Action definition</strong>: Abstracting software tools’ APIs into actions, facilitating LLMs in making calls.</li>
      <li><strong>Reflection and correction</strong>: Improving model accuracy by generating multiple reasoning paths and selecting the most consistent answer or through self-correction.</li>
      <li><strong>Multi-task framework</strong>: StructGPT, for example, can handle multiple table-related tasks.</li>
    </ul>
  </li>
</ol>

<h3 id="summary-of-specific-methods">Summary of Specific Methods</h3>

<ol>
  <li><strong>Task-specific Fine-tuning</strong>:
    <ul>
      <li><strong>TaPas</strong>: Extends the BERT model architecture with table pre-training and fine-tuning.</li>
      <li><strong>TaBERT</strong>: Encodes table content related to the input statement using a vertical attention mechanism.</li>
      <li><strong>TURL</strong>: Encodes information from table components as separate input embeddings and integrates them.</li>
    </ul>
  </li>
  <li><strong>Instruction Fine-tuning</strong>:
    <ul>
      <li><strong>Table-GPT</strong>: Constructs instruction fine-tuning datasets using a synthesis-enhanced approach.</li>
      <li><strong>TableLlama</strong>: Utilizes real data from existing datasets for instruction fine-tuning.</li>
      <li><strong>Magicoder</strong>: Collects open-source code snippets and generates programming problems and solutions for instruction fine-tuning.</li>
    </ul>
  </li>
  <li><strong>Retrieval-augmented Methods</strong>:
    <ul>
      <li><strong>ITR</strong>: Splits large tables into sub-tables and jointly trains retrievers and readers.</li>
      <li><strong>DB-GPT</strong>: Supports various functions such as retrieval-augmented generation, fine-tuning, and agents.</li>
    </ul>
  </li>
  <li><strong>Table Serialization</strong>:
    <ul>
      <li>Linearizes table content and inserts column delimiters.</li>
      <li>The table schema can be represented as plain text or through a CREATE TABLE statement.</li>
    </ul>
  </li>
  <li><strong>Example Selection for Few-shot Learning</strong>:
    <ul>
      <li>Selects examples most relevant to the target task, balancing quality and quantity.</li>
    </ul>
  </li>
  <li><strong>Decomposition of Complex Tasks</strong>:
    <ul>
      <li><strong>DIN-SQL</strong>: Breaks down text-to-SQL tasks into sub-tasks, generating intermediate sub-queries.</li>
    </ul>
  </li>
  <li><strong>Action Definition</strong>:
    <ul>
      <li><strong>SheetCopilot</strong>: Models existing spreadsheet software APIs as atomic actions through embedding and clustering methods.</li>
      <li><strong>ReAcTable</strong>: Extends the ReAct framework, defining three actions: generating SQL queries, generating Python code, and directly answering questions.</li>
    </ul>
  </li>
  <li><strong>Reflection and Correction</strong>:
    <ul>
      <li>Generates multiple reasoning paths and selects the most consistent answer.</li>
      <li>Adopts a proposal and correction mechanism, reflecting and improving past actions.</li>
    </ul>
  </li>
  <li><strong>Multi-task Framework</strong>:
    <ul>
      <li><strong>StructGPT</strong>: Addresses multiple table tasks by developing three actions for web tables, databases, and knowledge graphs.</li>
    </ul>
  </li>
</ol>

<p>This review systematically summarizes the latest advancements and specific methods of LLMs in table processing tasks, providing references for future research and applications.</p>

<h2 id="llm-on-tabular-data-retriever">LLM on Tabular Data (Retriever)</h2>

<p><a href="https://arxiv.org/pdf/2402.17944">Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding</a></p>

<h3 id="52-general-capabilities-of-large-language-models-in-question-answering-tasks">5.2 General Capabilities of Large Language Models in Question Answering Tasks</h3>

<p>Table 8 lists papers that study the performance of large language models (LLMs) in question answering (QA) and reasoning tasks, along with the models explored. While GPT-3.5 and GPT-4 are the most popular LLMs, these models have not been specifically optimized for table tasks. However, when combined with certain prompt engineering techniques (e.g., Chain of Thought, CoT), they perform well in executing complex table reasoning tasks.</p>

<h3 id="numerical-qa">Numerical QA</h3>

<p>Numerical QA tasks involve mathematical reasoning, such as “What is the average payment per transaction by American Express?” This type of mathematical reasoning task is prevalent in many practical applications, such as processing financial documents and annual reports. Akhtar et al. (2023) found that FlanT5 and GPT-3.5 outperform other models in various numerical reasoning tasks. On the DOCMATH-EVAL (Zhao et al., 2023d) dataset, GPT-4 with CoT significantly outperforms other LLMs, while open-source LLMs (e.g., LLaMa-2, Vicuna, Mistral, Starcoder, MPT, Qwen, AquilaChat2, etc.) perform poorly.</p>

<h3 id="text2sql">Text2SQL</h3>

<p>Liu et al. (2023c) designed a question matcher that identifies three types of keywords: 1) column-related terms, 2) restriction-related phrases (e.g., “Top 10”), and 3) algorithm or module keywords. Once these keywords are identified, the module merges the specific restrictions associated with each column into a unified combination, then matches it with the SQL algorithm or module indicated by the third keyword type. Zhang et al. (2023d) chose a more straightforward approach, allowing LLaMa-2 to generate SQL statements based on the question and table schema. Sun et al. (2023b) fine-tuned PaLM-2 on the Text2SQL task, achieving remarkable results on the Spider dataset. OpenTab (Kong et al., 2024) developed an open-domain table QA framework based on LLMs, combining it with a SQL generation module. Today, top models on Spider include those by Dong et al. (2023), Gao et al. (2024), and Pourreza &amp; Rafiei (2023), all of which build on OpenAI’s GPT model. SQL generation is highly popular in the industry, with many open-source fine-tuned models available.</p>

<h3 id="the-impact-of-model-size-on-performance">The Impact of Model Size on Performance</h3>

<p>Chen (2023) found that model size does indeed matter: on WebTableQuestions, comparing GPT-3 models of 6.7B and 175B, the smaller model achieved only half the score of the larger model. On TabFact, they found that the accuracy of smaller models (&lt;=6.7B) was almost random.</p>

<h3 id="to-fine-tune-or-not-to-fine-tune">To Fine-tune or Not to Fine-tune?</h3>

<p>Some larger models have been fine-tuned on various table tasks, including QA and fact verification tasks. Li et al. (2023d) found that fine-tuning always helps improve performance on various table tasks, especially in zero-shot settings where the improvement is most significant. Ye et al. (2023b) used the PASTA (Gu et al., 2022) model to achieve a higher score (93.00%) on TabFact compared to GPT-3 Codex (code-davinci-002) with a score of 85.60%. PASTA was pre-trained on a synthetic corpus of 1.2 million entries composed of Wikipedia tables for six types of sentence-table fill-in-the-blank tasks. This suggests that fine-tuning LLMs on table tasks still offers some advantages.</p>

<p>However, fine-tuning is less common compared to other methods working on prediction and generation tasks. This might be because LLMs (such as GPT-3.5, GPT-4) perform well in out-of-the-box QA tasks. In SQL generation on Spider, DIN-SQL (Pourreza &amp; Rafiei, 2023) and DAIL-S</p>

<p>QL (Sun et al., 2023b) each achieve scores above 90% with limited fine-tuning, indicating that prompt engineering techniques and retrieval methods could replace fine-tuning to some extent.</p>

<h3 id="53-special-data-considerations">5.3 Special Data Considerations</h3>

<p>When selecting training and evaluation data, it’s essential to account for specific table characteristics. Some datasets use manually annotated data, but others use synthetic or heuristic methods to generate labels. As a result, future research should pay attention to annotation quality, given that annotation standards vary across different domains and regions.</p>

<h2 id="li-rage">LI-RAGE</h2>

<p><a href="https://aclanthology.org/2023.acl-short.133.pdf">LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering</a></p>

<h3 id="summary-of-the-article">Summary of the Article</h3>

<p>The LI-RAGE framework is a novel approach to open-domain table question answering (TableQA). By combining a late interaction (LI) model with retrieval-augmented generation (RAGE) loss incorporating explicit signals, this method significantly improves the performance of table question answering. Compared to traditional retriever-reader pipelines, LI-RAGE offers enhanced accuracy and reliability through the following improvements:</p>

<ol>
  <li><strong>Late Interaction Model (LI):</strong> Utilizes the ColBERT model to encode both the query and the table on a word-by-word basis, capturing more fine-grained interaction information and thereby improving table retrieval effectiveness.</li>
  <li><strong>Joint Training with RAGE Loss:</strong> Combines the signals from the retriever and reader in joint training to optimize the effectiveness of both table retrieval and answer generation.</li>
  <li><strong>Binary Relevance Token:</strong> Introduces a binary relevance token (yes/no) before generating the answer to indicate whether the table is relevant to the query, thereby enhancing the reliability of the generated answer.</li>
</ol>

<h3 id="example-question">Example Question</h3>

<p>Consider the question: “Which country has the largest population?”</p>

<h3 id="table-dataset">Table Dataset</h3>

<p>Assume the following table data:</p>

<p><strong>Table 1:</strong></p>

<table>
  <thead>
    <tr>
      <th>Country</th>
      <th>Population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>China</td>
      <td>1,411 million</td>
    </tr>
    <tr>
      <td>India</td>
      <td>1,366 million</td>
    </tr>
    <tr>
      <td>USA</td>
      <td>331 million</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2:</strong></p>

<table>
  <thead>
    <tr>
      <th>Country</th>
      <th>Area</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Russia</td>
      <td>17 million km²</td>
    </tr>
    <tr>
      <td>Canada</td>
      <td>9.98 million km²</td>
    </tr>
    <tr>
      <td>China</td>
      <td>9.6 million km²</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3:</strong></p>

<table>
  <thead>
    <tr>
      <th>City</th>
      <th>Population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>New York</td>
      <td>8 million</td>
    </tr>
    <tr>
      <td>Tokyo</td>
      <td>14 million</td>
    </tr>
    <tr>
      <td>Shanghai</td>
      <td>24 million</td>
    </tr>
  </tbody>
</table>

<h3 id="end-to-end-example">End-to-End Example</h3>

<h4 id="1-table-retrieval-retriever">1. Table Retrieval (Retriever)</h4>

<p>The retriever selects the table most relevant to the query from the table corpus. In this example, the retriever might select Table 1 since it contains information related to countries and their populations.</p>

<p><strong>Retrieval Result:</strong><br />
Selected Table 1:</p>

<table>
  <thead>
    <tr>
      <th>Country</th>
      <th>Population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>China</td>
      <td>1,411 million</td>
    </tr>
    <tr>
      <td>India</td>
      <td>1,366 million</td>
    </tr>
    <tr>
      <td>USA</td>
      <td>331 million</td>
    </tr>
  </tbody>
</table>

<h4 id="2-answer-generation-reader">2. Answer Generation (Reader)</h4>

<p>The answer generator model takes the query and the retrieved table as input and generates the answer. In this example, the answer generator combines the question “Which country has the largest population?” with Table 1, identifying the maximum value corresponding to the population and generating the answer: “China.”</p>

<h4 id="3-binary-relevance-token">3. Binary Relevance Token</h4>

<p>To ensure that the selected table by the answer generator is reliable, a binary relevance token is added before generating the answer. During training, the system learns that answers generated from a golden table are prefixed with “yes,” whereas those from a non-golden table are prefixed with “no.” In this case, since the generated answer is derived from the golden table (Table 1), the answer is prefixed with “yes.”</p>

<p><strong>Final Output:</strong><br />
The answer generator outputs: “yes China.”</p>

<h4 id="4-filtering-and-final-answer-determination">4. Filtering and Final Answer Determination</h4>

<p>During inference, if the answer generator’s output is prefixed with “yes,” the answer is deemed reliable. The system prioritizes answers marked with “yes”; if all candidate answers are prefixed with “no,” the system selects the final answer based on the confidence score of the answer generator. In this case, the system identifies the “yes” prefix, confirms the answer’s reliability, and outputs the final answer: “China.”</p>

<h3 id="conclusion">Conclusion</h3>

<p>The above process demonstrates the complete workflow of an open-domain table question answering system from input query to final answer generation. Through the LI-RAGE framework, the system not only efficiently retrieves relevant information from vast table data but also ensures the reliability of the answer through binary relevance tokens.</p>

<h2 id="tap4llm">TAP4LLM</h2>

<p><a href="https://arxiv.org/pdf/2312.09039">TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning</a></p>

<h3 id="detailed-summary-of-tap4llm">Detailed Summary of TAP4LLM</h3>

<h3 id="application-of-large-language-models-to-tabular-data">Application of Large Language Models to Tabular Data</h3>

<p>As large language models (LLMs) advance in natural language processing, researchers have begun exploring their application to other modalities, such as vision and speech. However, directly applying traditional LLMs to the tabular domain presents two main challenges:</p>

<ol>
  <li><strong>Global Table Understanding:</strong> LLMs like GPT have a token length limitation, making it difficult to read and understand large tables comprehensively, thereby restricting their ability to grasp global information.</li>
  <li><strong>Generalization to the Tabular Domain:</strong> These models are primarily trained on natural language, resulting in weaker generalization when handling tabular data.</li>
</ol>

<p>Despite some research attempting to combine natural language processing with table data analysis, the performance of LLMs in table question answering remains limited.</p>

<h3 id="table-augmentation-techniques">Table Augmentation Techniques</h3>

<p>Table augmentation techniques aim to improve the generalization performance and robustness of machine learning models. To enhance LLMs’ performance in the tabular domain, researchers have explored various augmentation methods, including the integration of structured knowledge, common sense knowledge, and analytical knowledge. Studies have shown that leveraging domain-specific metadata or knowledge graphs can significantly improve LLMs’ understanding of tabular data. For example:</p>
<ul>
  <li><strong>Jena et al. (2022)</strong> proposed semi-automatically converting existing table data to create diversified natural language inference instances, improving zero-shot performance.</li>
  <li><strong>He et al. (2023)</strong> introduced a multi-task metadata model that accurately infers analytical metadata for tables using field distribution and knowledge graph information, demonstrating its application in intelligent data analysis products.</li>
</ul>

<h3 id="core-components-of-tap4llm">Core Components of TAP4LLM</h3>

<p>TAP4LLM addresses the main challenges of comprehensive table understanding through three core components:</p>

<ol>
  <li><strong>Table Sampling:</strong> Selecting and extracting the most relevant rows and columns from the table based on the query.
    <ul>
      <li><strong>Rule-based Sampling:</strong> Uses predefined criteria or rules, such as random sampling, uniform sampling, and content snapshot sampling.</li>
      <li><strong>Embedding-based Sampling:</strong> Selects rows and columns based on semantic and contextual representation, employing methods such as semantic-based sampling and centroid-based sampling.</li>
      <li><strong>LLM-based Sampling:</strong> Utilizes powerful LLMs to predict the indices of table rows and columns, though this approach is computationally expensive.</li>
    </ul>
  </li>
  <li><strong>Table Augmentation:</strong> Enriching table information by adding external knowledge and metadata.
    <ul>
      <li><strong>Metadata-based Augmentation:</strong> Includes the addition of information like dimensions/metrics, semantic field types, table size, statistical features, and header hierarchies.</li>
      <li><strong>Retrieval-based Augmentation:</strong> Acquires relevant content from external knowledge bases through a document retrieval system to reduce hallucination or factual errors.</li>
      <li><strong>Self-consistency Augmentation:</strong> Enhances the model’s reasoning capability through iterative generation and refinement of queries and responses.</li>
    </ul>
  </li>
  <li><strong>Table Packing and Serialization:</strong> Manages token allocation by packing tables and augmented information into sequences suitable for LLMs.
    <ul>
      <li>Empirical studies show that a sub-table length to augmentation information length ratio of 5:5 or 4:6 generally yields the best performance.</li>
      <li>Supports multiple serialization formats, such as HTML, XML, JSON, CSV, NL+Sep, and Markdown.</li>
    </ul>
  </li>
</ol>

<h3 id="conclusion-1">Conclusion</h3>

<p>TAP4LLM addresses the main challenges of comprehensive table understanding through table sampling, table augmentation, and table packing and serialization, enhancing the effectiveness of LLMs in table reasoning tasks. This method is not only applicable to table modeling but can also play a significant role in fields such as finance and transportation, promoting research based on tabular data.</p>

<h3 id="limitations">Limitations</h3>

<p>Code generation methods have been proposed to convert natural language queries into executable code or structured representations (Cheng et al., 2023; Gemmell and Dalton, 2023). This research direction is important, but due to space constraints, it is not explored in depth in this study. Current empirical research is primarily focused on English scenarios, with discussions on multilingual capabilities left for future research.</p>

<h3 id="example-using-tap4llm-for-table-data-analysis">Example: Using TAP4LLM for Table Data Analysis</h3>

<p>Suppose there is a financial data table containing a company’s quarterly financial reports over the past few years. The columns of the table include year, quarter, revenue, expenditure, net profit, and debt-to-equity ratio. The goal is to generate an accurate analysis based on the natural language query, “What is the trend of the company’s quarterly net profit over the past five years?”</p>

<h3 id="1-table-sampling">1. Table Sampling</h3>

<p><strong>Initial Table (T):</strong></p>

<table>
  <thead>
    <tr>
      <th>Year</th>
      <th>Quarter</th>
      <th>Revenue</th>
      <th>Expenditure</th>
      <th>Net Profit</th>
      <th>Debt-to-Equity Ratio</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2019</td>
      <td>Q1</td>
      <td>1000</td>
      <td>800</td>
      <td>200</td>
      <td>50%</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q2</td>
      <td>1100</td>
      <td>850</td>
      <td>250</td>
      <td>48%</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q3</td>
      <td>1050</td>
      <td>820</td>
      <td>230</td>
      <td>49%</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q4</td>
      <td>1200</td>
      <td>900</td>
      <td>300</td>
      <td>47%</td>
    </tr>
    <tr>
      <td>2020</td>
      <td>Q1</td>
      <td>1300</td>
      <td>950</td>
      <td>350</td>
      <td>46%</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>Q4</td>
      <td>1600</td>
      <td>1200</td>
      <td>400</td>
      <td>45%</td>
    </tr>
  </tbody>
</table>

<p>To answer the query “What is the trend of the company’s quarterly net profit over the past five years?” the</p>

<p>model applies TAP4LLM’s table sampling techniques. Using embedding-based sampling, it extracts the rows corresponding to the net profit column, which are most relevant to the query.</p>

<p><strong>Sampled Table:</strong></p>

<table>
  <thead>
    <tr>
      <th>Year</th>
      <th>Quarter</th>
      <th>Net Profit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2019</td>
      <td>Q1</td>
      <td>200</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q2</td>
      <td>250</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q3</td>
      <td>230</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q4</td>
      <td>300</td>
    </tr>
    <tr>
      <td>2020</td>
      <td>Q1</td>
      <td>350</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>Q4</td>
      <td>400</td>
    </tr>
  </tbody>
</table>

<h3 id="2-table-augmentation">2. Table Augmentation</h3>

<p>To enhance the model’s reasoning capabilities, TAP4LLM applies self-consistency augmentation, iteratively generating and refining queries to ensure accurate analysis. This might involve integrating the financial performance trends for each quarter into a broader context, perhaps by incorporating external knowledge about the economic climate during these years.</p>

<h3 id="3-table-packing-and-serialization">3. Table Packing and Serialization</h3>

<p>Finally, the augmented table information is packed into a format suitable for LLMs. In this case, the financial data might be serialized into a JSON or Markdown format, enabling efficient token management and allowing the LLM to process the data effectively.</p>

<p><strong>Packed and Serialized Table (in JSON format):</strong></p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"Year"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"Quarter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q4"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"Net Profit"</span><span class="p">:</span><span class="w"> </span><span class="s2">"400"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"Previous_Trend"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="nl">"Year"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023"</span><span class="p">,</span><span class="w"> </span><span class="nl">"Quarter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q3"</span><span class="p">,</span><span class="w"> </span><span class="nl">"Net Profit"</span><span class="p">:</span><span class="w"> </span><span class="s2">"380"</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"Year"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023"</span><span class="p">,</span><span class="w"> </span><span class="nl">"Quarter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q2"</span><span class="p">,</span><span class="w"> </span><span class="nl">"Net Profit"</span><span class="p">:</span><span class="w"> </span><span class="s2">"370"</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"Year"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2023"</span><span class="p">,</span><span class="w"> </span><span class="nl">"Quarter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q1"</span><span class="p">,</span><span class="w"> </span><span class="nl">"Net Profit"</span><span class="p">:</span><span class="w"> </span><span class="s2">"360"</span><span class="p">}</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nl">"Contextual_Info"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Company's net profit has shown a consistent upward trend over the past five years."</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h3 id="conclusion-2">Conclusion</h3>

<p>The final output provides a comprehensive analysis of the company’s quarterly net profit trend, supported by augmented and packed table data. By employing TAP4LLM, LLMs can accurately and efficiently handle complex queries over large tables, ensuring reliable and insightful results in financial data analysis.</p>

<h3 id="example-of-processing-with-tap4llm">Example of Processing with TAP4LLM</h3>

<p><strong>Task:</strong> Identify the upward or downward trend of the company’s quarterly net profit.</p>

<p><strong>Query:</strong> “What is the trend of the company’s quarterly net profit over the past five years?”</p>

<p><strong>Processed Answer:</strong> “The company’s quarterly net profit has shown a consistent upward trend over the past five years, with the highest profit recorded in Q4 2023.”</p>

<h2 id="colbert"><a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a></h2>

<h3 id="innovations">Innovations</h3>

<ol>
  <li><strong>Delayed Interaction Framework</strong>: <a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a> introduces a delayed interaction framework by decoupling the encoding processes of queries and documents. This allows document representations to be precomputed, thereby reducing the computation required during online queries.</li>
  <li><strong>MaxSim Operation</strong>: When assessing the relevance between queries and documents, <a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a> employs the MaxSim operation, which computes the maximum cosine similarity or L2 distance between each query embedding and document embedding, summing these maximum similarity values. This approach is both simple and efficient.</li>
  <li><strong>Shared BERT Encoder</strong>: <a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a> utilizes a single BERT model shared between the query and document encoders, distinguishing inputs with special tokens ([Q] and [D]) for queries and documents respectively. This method conserves computational resources while maintaining the model’s contextual understanding.</li>
  <li><strong>Segmentation and Filtering</strong>: The document encoder filters out embeddings of punctuation to reduce computational load and storage space.</li>
  <li><strong>Vector-based Retrieval</strong>: By leveraging existing vector similarity search libraries (such as Faiss), <a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a> achieves efficient end-to-end retrieval through pruning operations from large document collections.</li>
</ol>

<h3 id="advantages">Advantages</h3>

<ol>
  <li><strong>High Computational Efficiency</strong>: By precomputing document representations and employing a delayed interaction mechanism, ColBERT significantly reduces computation during query processing, achieving two orders of magnitude speed improvement compared to other BERT-based models.</li>
  <li><strong>Efficient Space Utilization</strong>: Through normalization and dimensionality reduction of document embeddings, ColBERT markedly decreases storage requirements, making it more feasible for practical applications.</li>
  <li><strong>Scalability</strong>: The architecture of ColBERT allows for processing large-scale document collections without sacrificing accuracy, especially when using vector similarity search for pruning, greatly enhancing retrieval efficiency.</li>
  <li><strong>End-to-End Retrieval Capability</strong>: ColBERT can be used not only for re-ranking pre-retrieved document sets but also for direct end-to-end retrieval from large document collections, improving the overall recall and precision of the retrieval system.</li>
</ol>

<h3 id="problems-addressed">Problems Addressed</h3>

<ol>
  <li><strong>High Computational Cost</strong>: Traditional BERT-based ranking models are extremely time-consuming for query-document pairs. ColBERT reduces online computation costs through delayed interaction and precomputation mechanisms.</li>
  <li><strong>Long Response Time</strong>: High computational costs result in lengthy query response times, negatively impacting user experience. ColBERT significantly reduces query latency through more efficient computation and retrieval mechanisms.</li>
  <li><strong>Large Storage Space</strong>: Deep language models typically require substantial storage for document representations. ColBERT reduces storage requirements through normalization and dimensionality reduction.</li>
  <li><strong>Trade-off Between Retrieval Accuracy and Efficiency</strong>: Existing methods often sacrifice accuracy to improve retrieval efficiency. ColBERT enhances retrieval efficiency without compromising accuracy through efficient delayed interaction and vector similarity search.</li>
</ol>

<h3 id="detailed-and-vivid-example-of-colbert-usage">Detailed and Vivid Example of ColBERT Usage</h3>

<h3 id="background">Background</h3>

<p>Suppose you are using an academic paper database containing millions of papers. You are researching “the benefits of machine learning” and want to find the most relevant papers. This is where ColBERT can assist you.</p>

<h3 id="1-offline-preprocessing-and-encoding-of-documents">1. Offline Preprocessing and Encoding of Documents</h3>

<p>Before any queries are made, we preprocess and encode each paper in the database. This is an offline process, akin to cataloging and numbering all the books in a library.</p>

<ol>
  <li><strong>Segmentation</strong>: Break down each paper into words. For instance, “Machine learning is a method of data analysis that can automatically build analytical models” is segmented into “Machine,” “learning,” “is,” “a,” “method,” “of,” “data,” “analysis,” “that,” “can,” “automatically,” “build,” “analytical,” “models,” etc.</li>
  <li><strong>Adding Markers</strong>: Add special markers at the beginning of each paper, such as “[D],” to indicate that it is a document.</li>
  <li><strong>BERT Encoding</strong>: Use the BERT model to encode each word, converting them into contextually meaningful vector representations. This is like generating a unique numerical signature for each word.</li>
  <li><strong>Filtering Unrelated Information</strong>: Remove punctuation and other irrelevant information to retain important words.</li>
  <li><strong>Normalization and Dimensionality Reduction</strong>: Normalize and reduce the dimensionality of these vectors to make their representation more compact and efficient, similar to compressing large files into smaller ones for easier storage and processing.</li>
  <li><strong>Storing Embeddings</strong>: Store the processed vectors in a database for future use.</li>
</ol>

<h3 id="2-query-preprocessing-and-encoding">2. Query Preprocessing and Encoding</h3>

<p>When you enter the query “the benefits of machine learning,” ColBERT processes this query immediately, which is an online operation.</p>

<ol>
  <li><strong>Segmentation</strong>: Break down the query into words, such as “Machine,” “learning,” “benefits,” “of.”</li>
  <li><strong>Adding Markers</strong>: Add special markers at the beginning of the query, such as “[Q],” to indicate that it is a query.</li>
  <li><strong>Padding and BERT Encoding</strong>: Pad the query to a fixed length and input it into the BERT model to generate context vectors for each word. These vectors represent the meaning of each word in the query and their relationships.</li>
  <li><strong>Normalization and Dimensionality Reduction</strong>: Normalize and reduce the dimensionality of these vectors to match the format of document vectors.</li>
</ol>

<h3 id="3-delayed-interaction-and-similarity-computation">3. Delayed Interaction and Similarity Computation</h3>

<p>Next, ColBERT finds the most relevant papers using delayed interaction and similarity computation.</p>

<ol>
  <li><strong>Loading Document Embeddings</strong>: Load all precomputed document vector representations from the database.</li>
  <li><strong>MaxSim Calculation</strong>: For each query word vector, find the maximum similarity with all word vectors in the document. This is like finding the best matching puzzle pieces.</li>
  <li><strong>Summing Similarities</strong>: Sum the maximum similarity values for each query word with document words to obtain an overall similarity score. This score represents how relevant the document is to the query.</li>
</ol>

<h3 id="4-document-ranking-and-retrieval">4. Document Ranking and Retrieval</h3>

<p>Finally, rank documents based on similarity scores and return the top k documents.</p>

<ol>
  <li><strong>Document Ranking</strong>: Sort all candidate documents by similarity score, similar to ranking exam results from highest to lowest.</li>
  <li><strong>Returning Results</strong>: Return the top k documents with the highest scores, which are the most relevant papers to your query.</li>
</ol>

<h3 id="vivid-example">Vivid Example</h3>

<p>Imagine you are in a library looking for books related to “the benefits of machine learning.” The librarian (ColBERT) has previously cataloged and tagged all the books in detail. When you make your request, the librarian quickly reviews the digital content of each book (query encoding and similarity computation), finds the most relevant ones, ranks them, and provides you with the best matches. This all happens very swiftly because the librarian has done a lot of preparation in advance.</p>

<p>In this way, ColBERT ensures efficient handling of large datasets while providing fast response times and high-quality results.</p>

<p><a href="https://arxiv.org/abs/2004.12832">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</a></p>

<h2 id="colbert-v2">ColBERT v2</h2>

<p><a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a></p>

<h3 id="key-improvements-and-optimizations-in-colbertv2-effective-and-efficient-retrieval-via-lightweight-late-interaction">Key Improvements and Optimizations in <a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a></h3>

<ol>
  <li><strong>Residual Compression Mechanism</strong>: A major innovation in <a href="https://arxiv.org/pdf/2112.01488">ColBERTv2</a> is the residual compression mechanism. By encoding each embedding vector as an index of its nearest centroid and quantization residual, ColBERTv2 significantly reduces storage requirements. This improvement lowers storage costs considerably without sacrificing model quality.</li>
  <li><strong>Denoising Supervision</strong>: <a href="https://arxiv.org/pdf/2112.01488">ColBERTv2</a> introduces a new supervision strategy, including cross-encoder distillation and hard negative mining. This method selects challenging negative samples to avoid rewarding false positives or penalizing false negatives, thus enhancing training effectiveness and model quality.</li>
  <li><strong>Efficient Indexing and Retrieval</strong>:
    <ul>
      <li><strong>Centroid Selection</strong>: During indexing, <a href="https://arxiv.org/pdf/2112.01488">ColBERTv2</a> optimizes paragraph representations through centroid selection.</li>
      <li><strong>Paragraph Encoding</strong>: The BERT encoder is used to compress output embeddings, assigning each embedding to the nearest centroid and computing quantization residuals.</li>
      <li><strong>Inverted Index</strong>: To support fast nearest neighbor search, <a href="https://arxiv.org/pdf/2112.01488">ColBERTv2</a> groups embedding IDs corresponding to each centroid and maintains inverted lists, enabling rapid retrieval of similar token-level embeddings.</li>
    </ul>
  </li>
  <li><strong>Optimized Retrieval Process</strong>:
    <ul>
      <li><strong>Candidate Generation</strong>: For each vector in the query, the nearest centroids are found, and inverted lists are used to identify paragraph embeddings close to these centroids. These embeddings are decompressed, and cosine similarity with query vectors is computed.</li>
      <li><strong>Scoring and Maximization</strong>: Scores are grouped by paragraph ID, and scores for the same paragraph are maximized and reduced. This is similar to finding the best matching pieces in a puzzle.</li>
    </ul>
  </li>
</ol>

<h3 id="full-process-example-of-colbert">Full Process Example of [ColBERT</h3>

<p>v2](https://arxiv.org/pdf/2112.01488)</p>

<h3 id="background-1">Background</h3>

<p>Consider a scenario where you are searching for recent research papers on “machine learning advancements” in a database. Here’s how <a href="https://arxiv.org/pdf/2112.01488">ColBERTv2</a> would enhance this process:</p>

<h3 id="1-offline-preprocessing-and-encoding-of-documents-1">1. Offline Preprocessing and Encoding of Documents</h3>

<ol>
  <li><strong>Preprocessing and Encoding</strong>:
    <ul>
      <li><strong>Segmentation and Encoding</strong>: Similar to <a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>, segment and encode each paper, but with residual compression for space efficiency.</li>
      <li><strong>Storing Quantized Embeddings</strong>: Store embeddings efficiently using centroids and quantization residuals.</li>
    </ul>
  </li>
  <li><strong>Index Construction</strong>:
    <ul>
      <li><strong>Optimized Indexing</strong>: Create an inverted index grouping embeddings by their nearest centroids for fast retrieval.</li>
    </ul>
  </li>
</ol>

<h3 id="2-query-processing-and-encoding">2. Query Processing and Encoding</h3>

<ol>
  <li><strong>Encoding</strong>: Encode the query with BERT, and perform similar normalization and quantization as with documents.</li>
</ol>

<h3 id="3-efficient-retrieval">3. Efficient Retrieval</h3>

<ol>
  <li><strong>Centroid-based Candidate Generation</strong>:
    <ul>
      <li><strong>Find Nearest Centroids</strong>: Identify centroids closest to the query vectors.</li>
      <li><strong>Retrieve Embeddings</strong>: Use inverted lists to gather candidate embeddings, decompress, and score them based on similarity to the query.</li>
    </ul>
  </li>
  <li><strong>Scoring and Ranking</strong>:
    <ul>
      <li><strong>Maximizing Scores</strong>: Rank and maximize scores to retrieve the most relevant papers efficiently.</li>
    </ul>
  </li>
</ol>

<h3 id="vivid-example-1">Vivid Example</h3>

<p>Imagine you are a researcher looking for the latest papers on “machine learning advancements.” Instead of manually reviewing each paper, <a href="https://arxiv.org/pdf/2112.01488">ColBERTv2</a> preprocesses and indexes the papers in advance, using efficient storage techniques. When you make a query, it quickly finds and retrieves the most relevant papers by leveraging optimized indexing and centroid-based retrieval.</p>

<p><a href="https://arxiv.org/pdf/2112.01488">ColBERTv2</a> further refines the retrieval process, improving both efficiency and accuracy while reducing storage requirements, thus providing a highly effective tool for handling extensive document collections.</p>

<h2 id="dpr">DPR</h2>

<p><a href="https://arxiv.org/pdf/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a></p>

<h3 id="example-scenario">Example Scenario</h3>

<p>Suppose a user asks the system a question: “What is photosynthesis?” The following steps illustrate how DPR retrieves relevant information:</p>

<h3 id="step-1-query-encoding">Step 1: Query Encoding</h3>

<ul>
  <li><strong>Input</strong>: User’s question “What is photosynthesis?”</li>
  <li><strong>Processing</strong>: The question is first fed into a pre-trained Transformer model (e.g., BERT). This model transforms the text into a high-dimensional vector (typically 768 dimensions or more, depending on the model architecture).</li>
  <li><strong>Output</strong>: Dense vector representation of the question.</li>
</ul>

<h3 id="step-2-document-encoding">Step 2: Document Encoding</h3>

<ul>
  <li><strong>Preprocessing</strong>: Prior to this step, the system has already encoded potential answers or information sources (e.g., Wikipedia entries, textbook paragraphs) into vectors and stored them in a vector database.</li>
  <li><strong>Database</strong>: Contains precomputed vector representations of numerous documents.</li>
</ul>

<h3 id="step-3-vector-similarity-calculation">Step 3: Vector Similarity Calculation</h3>

<ul>
  <li><strong>Comparison</strong>: The system now compares the query vector with each document vector in the database. Comparison typically uses cosine similarity.</li>
  <li><strong>Ranking</strong>: Documents are ranked based on similarity scores, from highest to lowest.</li>
</ul>

<h3 id="step-4-selecting-top-documents">Step 4: Selecting Top Documents</h3>

<ul>
  <li><strong>Selection</strong>: The system typically selects the top N documents (e.g., the top 5 or 10) with the highest similarity scores, considering these documents to be the most relevant to the query.</li>
  <li><strong>Output</strong>: The text content of these top documents is sent to a generation model for the next step in answer generation.</li>
</ul>

<h3 id="step-5-answer-generation">Step 5: Answer Generation</h3>

<ul>
  <li><strong>Generation Model Input</strong>: The selected document contents are used as context for a generation model (e.g., GPT).</li>
  <li><strong>Generating Answer</strong>: The generation model synthesizes these textual inputs to produce a comprehensive and relevant answer.</li>
</ul>

<h3 id="step-6-output-final-answer">Step 6: Output Final Answer</h3>

<ul>
  <li><strong>User Reception</strong>: The system outputs the answer to the user, for example: “Photosynthesis is the process by which plants, algae, and some bacteria use sunlight to convert water and carbon dioxide into oxygen and glucose.”</li>
</ul>

<p>This example demonstrates how DPR precisely retrieves relevant content from a vast amount of information in a RAG system and assists the generation model in providing accurate and useful answers.</p>

<p><a href="https://arxiv.org/abs/2405.01585">Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications</a></p>

<blockquote>
  <p>No code</p>
</blockquote>

<h2 id="raft">RAFT</h2>

<p><a href="https://arxiv.org/abs/2403.10131">RAFT: Adapting Language Model to Domain-Specific RAG</a></p>

<p>The paper introduces a method called <strong>Retrieval Augmented Fine Tuning (RAFT)</strong> designed to enhance pre-trained language models’ retrieval-augmented generation (RAG) capabilities, particularly in domain-specific settings under the “open-book” scenario. This approach integrates fine-tuning with RAG to improve model performance on domain-specific question answering tasks.</p>

<p><strong>Data Preparation</strong>: The RAFT method prepares a set of documents for each question, including “oracle” documents containing the answers and irrelevant distractor documents. For questions with correct documents, RAFT generates Chain-of-Thought (CoT) style answers, directly referencing relevant fragments of the documents to minimize hallucination issues during the generation process. This data structure trains the model to better recognize and utilize relevant information.</p>

<p><strong>Training Strategy</strong>: During training, the model is fine-tuned to handle scenarios with distractor documents. The model is tasked with accurately extracting useful information and generating answers amidst these distractors. Additionally, some training questions contain only distractor documents to encourage the model to rely on learned domain knowledge for answering. This strategy not only strengthens the model’s domain-specific knowledge but also enhances its ability to respond effectively in noisy contexts.</p>

<p><strong>Integration of Fine-Tuning and RAG</strong>: RAFT improves the model’s effectiveness in open-book settings by fine-tuning it to efficiently handle domain-specific documents. The model learns to ignore irrelevant information and accurately cite relevant document content when generating answers. Unlike traditional RAG methods, RAFT focuses on domain-specific applications, further enhancing retrieval and generation capabilities.</p>

<p>Experimental results show that RAFT significantly outperforms other baseline models on several datasets (e.g., PubMed, HotpotQA, Gorilla API Bench), demonstrating its strong potential for domain-specific question answering tasks. The paper provides an effective training strategy for achieving high performance in domain-specific QA tasks and showcases the benefits of combining fine-tuning with RAG to enhance model performance.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Lu, W., Zhang, J., Zhang, J. and Chen, Y., 2024. Large language model for table processing: A survey. arXiv preprint arXiv:2402.05121.</li>
  <li>Fang, X., Xu, W., Tan, F.A., Zhang, J., Hu, Z., Qi, Y., Nickleach, S., Socolinsky, D., Sengamedu, S. and Faloutsos, C., 2024. Large Language Models on Tabular Data–A Survey. arXiv preprint arXiv:2402.17944.</li>
  <li>Zhao, Y., Long, Y., Liu, H., Nan, L., Chen, L., Kamoi, R., Liu, Y., Tang, X., Zhang, R. and Cohan, A., 2023. Docmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data. arXiv preprint arXiv:2311.09805.</li>
  <li>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023. Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. arXiv preprint arXiv:2312.09039.</li>
  <li>Dong, X., Zhang, C., Ge, Y., Mao, Y., Gao, Y., Lin, J. and Lou, D., 2023. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306.</li>
  <li>Sundar, A.S. and Heck, L., 2023. cTBLS: Augmenting large language models with conversational tables. arXiv preprint arXiv:2303.12024.</li>
  <li>Gao, D., Wang, H., Li, Y., Sun, X., Qian, Y., Ding, B. and Zhou, J., 2023. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363.</li>
  <li>Lin, W., Blloshmi, R., Byrne, B., de Gispert, A. and Iglesias, G., 2023, July. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 1557-1566).</li>
  <li>Khattab, O. and Zaharia, M., 2020, July. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (pp. 39-48).</li>
  <li>Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C. and Zaharia, M., 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488.</li>
  <li>Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</li>
  <li>Khanna, S. and Subedi, S., 2024. Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications. arXiv preprint arXiv:2405.01585.</li>
  <li>Zhang, T., Patil, S.G., Jain, N., Shen, S., Zaharia, M., Stoica, I. and Gonzalez, J.E., 2024. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131.</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[With the widespread application of large language models (LLMs) in the field of natural language processing (NLP), these models have demonstrated remarkable capabilities across various tasks, including text generation, question answering systems, and sentiment analysis. However, despite their outstanding performance in handling unstructured data, LLMs still face numerous challenges when processing structured data, particularly tabular data. The structured nature of tabular data and its rich semantic information impose higher demands on LLMs, rendering traditional text processing methods often inapplicable to such data.]]></summary></entry><entry><title type="html">Dataset-for-Question-Answering(CN)</title><link href="http://localhost:4000/Dataset-for-Question-Answering-CN/" rel="alternate" type="text/html" title="Dataset-for-Question-Answering(CN)" /><published>2024-08-15T09:02:00+01:00</published><updated>2024-08-15T09:02:00+01:00</updated><id>http://localhost:4000/Dataset-for-Question-Answering-CN</id><content type="html" xml:base="http://localhost:4000/Dataset-for-Question-Answering-CN/"><![CDATA[<p><strong>Title:</strong> Dataset for Question Answering (CN) [To be continued…]<br />
<strong>Date:</strong> 2024-08-14 13:21:39<br />
<strong>Tags:</strong> Dataset</p>

<h1 id="qa数据集概览">QA数据集概览</h1>

<p>问答系统（QA）是自然语言处理领域的核心研究方向之一，旨在从多种信息源中提取精确答案。为满足不同的研究需求和应用场景，各类问答数据集应运而生，并根据数据形式和任务特性进行了系统分类。以下概览介绍了一些重要的问答数据集，涵盖了多种数据类型和任务要求。这些数据集为研究人员提供了丰富的测试和训练资源，以推动在不同信息格式下问答系统的性能提升和技术进步。</p>

<h3 id="开放领域问答">开放领域问答</h3>

<ul>
  <li><strong><a href="https://hybridqa.github.io/">HybridQA</a></strong>: 每个问题与一个维基百科表格及多个段落对齐，表格单元格与段落链接。</li>
  <li><strong><a href="https://ott-qa.github.io/">OTT-QA</a></strong>: 基于HybridQA的数据集，结合了文本和表格证据，构建了一个开放领域问答基准。</li>
  <li><strong><a href="https://arxiv.org/pdf/2103.12011">NQ-Table</a></strong>: 开放领域问答数据集，结合了表格和文本证据。</li>
  <li><strong><a href="https://nextplusplus.github.io/TAT-QA/">TAT-QA</a></strong>: 类似于FinQA，但包含了既有算术问题又有范围回答的问题。</li>
  <li><strong><a href="https://https://finqasite.github.io/">FinQA</a></strong>: 基于HybridQA的财务报告数据集，仅包含算术回答的问题（忽略范围回答问题）。</li>
  <li><strong><a href="https://github.com/psunlpgroup/MultiHiertt">MultiHiertt</a></strong>: 包含层次表格和文本的数值推理问题数据集。</li>
  <li><strong><a href="https://nlp.cs.washington.edu/triviaqa/">TriviaQA</a></strong>: 包含多个领域的开放问答数据集，提供了丰富的证据段落用于回答问题。</li>
  <li><strong><a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></strong>: 提供自然语言问题及其对应的维基百科段落，支持开放领域问答研究。</li>
  <li><strong><a href="https://hotpotqa.github.io/">HotpotQA</a></strong>: 提供需要多跳推理的问题和答案，要求在多个文档中找出信息以回答问题。</li>
  <li><strong><a href="https://fever.ai/">FEVER</a></strong>: 主要用于事实验证任务，提供了新闻文章和对应的真值判断问题。</li>
</ul>

<h3 id="表格问答">表格问答</h3>

<ul>
  <li><strong><a href="https://ppasupat.github.io/WikiTableQuestions/">WikiTableQuestions (WTQ)</a></strong>: 包含22033个问题-答案对，配有2108个表格；人工标注的问题和答案。</li>
  <li><strong><a href="https://github.com/salesforce/WikiSQL">WikiSQL</a></strong>: 对Wiki表格进行SQL标注，包含81,000个问题和24,000个表格。</li>
  <li><strong><a href="https://github.com/IBM/AITQA">AIT-QA</a></strong>: 涉及层次表格的数据集，包含116个关于美国航班的表格和515个问题。</li>
  <li><strong><a href="https://arxiv.org/pdf/2108.06712v3">HiTab</a></strong>: 针对层次表格的数据集。</li>
  <li><strong><a href="https://github.com/google-research/tapas">TAPAS</a></strong>: 针对表格的问答数据集，结合了SQL查询和自然语言问题，覆盖多种表格格式。</li>
  <li><strong><a href="https://github.com/wtmo/TabFact">TabFact</a></strong>: 包含表格的真值判断数据集，提供了大量的表格和对应的事实验证问题。</li>
  <li><strong><a href="https://github.com/ABSA-HKUST/TableQA">TableQA</a></strong>: 涵盖各种表格结构的问答任务，重点关注从表格中提取信息。</li>
  <li><strong><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></strong>: 专注于结构化数据的问答任务，通过SQL查询从表格中提取信息。</li>
</ul>

<h3 id="知识图谱问答">知识图谱问答</h3>

<ul>
  <li><strong>MetaQA (2018)</strong>: 知识图谱问答数据集。</li>
  <li><strong>GRAPHQ (2016)</strong>: 知识图谱问答数据集。</li>
  <li><strong>WebQSP (2016)</strong>: 基于Freebase的问答数据集。</li>
  <li><strong>CWQ (2018)</strong>: 基于WebQSP的扩展数据集，使用SPARQL查询标注来研究语义解析。</li>
  <li><strong>LC-QuAD (2017)</strong> 和 <strong>LC-QuAD 2.0 (2019)</strong>: 基于Wikidata和DBpedia的知识图谱数据集，包含30,000个问题，配有SPARQL查询。</li>
  <li><strong>GrailQA (2021)</strong>: 基于Freebase的知识图谱问答数据集，包含64,000个问题，并进行查询标注。</li>
  <li><strong>KQA Pro (2022)</strong>: NL问题与SPARQL注释的数据集，知识图谱包括FB15k-237和对齐的Wikidata实体及3,000个相同名称的其他实体。</li>
  <li><strong>Freebase QA (2014)</strong>: 使用Freebase知识图谱的早期问答数据集，提供了大量关于知识图谱的问答对。</li>
</ul>

<h3 id="知识图谱文本问答">知识图谱+文本问答</h3>

<ul>
  <li>评估开放领域文本问答数据集：TriviaQA (2017)、WebQuestion (2013)、CWQ (2018)、WebQAP (2016)，使用WordNet、Freebase、ConceptNet等知识图谱。</li>
  <li><strong>WebQuestion (2013)</strong>: 包含针对Freebase的自然语言问题和对应的答案，广泛用于知识图谱和文本问答的研究。</li>
  <li><strong>ComplexWebQuestions (CWQ) (2018)</strong>: 扩展了WebQuestion的数据集，使用复杂的SPARQL查询以测试知识图谱问答的能力。</li>
</ul>

<h3 id="文档问答">文档问答</h3>

<ul>
  <li><strong>SQuAD (2018)</strong>: 文档问答数据集，提供了大规模的段落和问题对，用于评估模型的理解能力。 <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a></li>
  <li><strong>TrivialQA (2017)</strong>: 文档问答数据集，侧重于简单的问题，主要用于基线模型的评估。</li>
  <li><strong>Natural Questions (NQ) (2019)</strong>: 提供自然语言问题及其对应的维基百科段落，支持开放领域问答研究。 <a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></li>
  <li><strong>SearchQA (2017)</strong>: 提供了基于搜索引擎的问答数据集，包含从网络搜索结果中提取的问题和答案。</li>
  <li><strong>DocVQA (2020)</strong>: 文档问答数据集，专注于从复杂的文档中提取信息，包括扫描文档和表格。</li>
  <li><strong>FEVEROUS (2020)</strong>: 事实验证问答数据集，扩展了FEVER数据集，包含了更多文档和验证任务。</li>
  <li><strong>HotpotQA (2018)</strong>: 提供需要多跳推理的问题和答案，要求在多个文档中找出信息以回答问题。 <a href="https://hotpotqa.github.io/">HotpotQA</a></li>
</ul>

<h1 id="问答数据集概览">问答数据集概览</h1>

<h2 id="开放领域问答数据集">开放领域问答数据集</h2>

<h3 id="hybridqa">HybridQA</h3>

<p><strong>介绍</strong>：HybridQA由Chen等人在《HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data》中提出。现有的问答数据集大多专注于处理同质信息，基于文本或知识库/表格信息。但由于人类知识分布在异质形式中，仅使用同质信息可能会导致严重的覆盖问题。为填补这一空白，我们提出了HybridQA，一个新的大规模问答数据集，要求在异质信息上进行推理。每个问题与一个维基百科表格及多个与表格中实体相关的自由格式语料库对齐。问题设计为需要同时汇总表格信息和文本信息，即缺少任何一种形式都会使问题无法回答。我们测试了三种不同的模型：1) 仅表格模型；2) 仅文本模型；3) 结合异质信息的混合模型。实验结果表明，两种基线模型的EM分数均低于20%，而混合模型的EM分数超过40%。这一差距表明，在HybridQA中汇总异质信息的必要性。然而，混合模型的分数仍远低于人类表现。因此，HybridQA可以作为一个挑战性的基准，用于研究异质信息的问答任务。</p>

<p><strong>数据集及代码</strong>：公开可用：<a href="https://github.com/wenhuchen/HybridQA">GitHub - HybridQA</a>。</p>

<h3 id="tat-qa">TAT-QA</h3>

<p><strong>介绍</strong>：TAT-QA（Tabular And Textual dataset for Question Answering）由Zhu等人在《TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance》中引入。TAT-QA是一个大规模的问答数据集，旨在推动对更复杂且现实的表格和文本数据进行问答研究，特别是那些需要数值推理的问题。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>上下文为混合型，包括一个半结构化的表格和至少两个相关段落，这些段落描述、分析或补充表格内容；</li>
  <li>问题由具备丰富金融知识的专家生成，大多数为实际应用问题；</li>
  <li>答案形式多样，包括单一跨度、多个跨度和自由格式；</li>
  <li>回答问题通常需要各种数值推理能力，包括加法（+）、减法（-）、乘法（x）、除法（/）、计数、比较、排序及其组合；</li>
  <li>除了真实答案外，还提供了对应的推导过程和规模（如有）。</li>
</ul>

<p><strong>数据量</strong>：TAT-QA包含16,552个问题，涉及2,757个来自真实金融报告的混合上下文。</p>

<h3 id="finqa">FinQA</h3>

<p><strong>介绍</strong>：FinQA由Chen等人在《FinQA: A Dataset of Numerical Reasoning over Financial Data》中提出。该数据集专注于对金融数据进行深入问答，旨在自动化分析大量金融文档。与现有的通用领域任务相比，金融领域涉及复杂的数值推理和异质表示理解。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>提供了金融专家编写的问答对，并注释了黄金推理程序，以确保完整的可解释性；</li>
  <li>引入了基线模型并进行了全面实验，结果显示，流行的大型预训练模型在获取金融知识和复杂的多步骤数值推理上远不及专家；</li>
  <li>数据集及代码公开可用：<a href="https://github.com/czyssrs/FinQA">GitHub - FinQA</a>。</li>
</ul>

<h3 id="multihiertt">MultiHiertt</h3>

<p><strong>介绍</strong>：MultiHiertt由Zhao等人在《Numerical Reasoning over Multi Hierarchical Tabular and Textual Data》中提出。该数据集针对包含文本和表格内容的混合数据进行数值推理，特别是多层次表格的情况。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>每个文档包含多个表格和较长的非结构化文本；</li>
  <li>多数表格为层次结构；</li>
  <li>问题所需的推理过程比现有基准更加复杂且具有挑战性；</li>
  <li>提供了详细的推理过程和支持事实的注释。</li>
</ul>

<p><strong>模型</strong>：引入了一种新型问答模型MT2Net，该模型首先应用事实检索从表格和文本中提取相关支持事实，然后使用推理模块对检索到的事实进行符号推理。实验结果表明，MultiHiertt对现有基线模型提出了强有力的挑战，其表现远落后于人类专家。</p>

<p><strong>数据集及代码</strong>：公开可用：<a href="https://github.com/psunlpgroup/MultiHiertt">GitHub - MultiHiertt</a>。</p>

<h3 id="hotpotqa">HotpotQA</h3>

<p><strong>介绍</strong>：HotpotQA由Yang等人在《HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering》中引入。该数据集基于英语维基百科，包含约113K个众包问题，这些问题需要引入两个维基百科文章的介绍段落来回答。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>涵盖多种推理策略，包括涉及缺失实体的问题、交集问题（满足属性A和属性B的是什么？）、比较问题等；</li>
  <li>提供了十个段落，其中包含了黄金段落；在开放领域的全维基百科设置中，模型仅给出问题和整个维基百科；</li>
  <li>模型评估包括答案准确性（通过精确匹配（EM）和单词F1测量）和可解释性（评估预测的支持事实句子与人工注释的匹配度）。</li>
</ul>

<h3 id="fever"><a href="https://fever.ai/">FEVER</a></h3>

<p><strong>介绍</strong>：FEVER（Fact Extraction and VERification）由Thorne等人在《FEVER: a large-scale dataset for Fact Extraction and VERification》中提出。该数据集用于验证文本源的事实，包含185,445个通过改变维基百科句子生成的声明，随后被验证。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>声明被分类为“支持”、“反驳”或“信息不足”；</li>
  <li>对于前两类，注释人员记录了形成判断所需的句子；</li>
  <li>发展了一个流水线方法，并与设计良好的预言者进行了比较，表明FEVER是一个具有挑战性的测试平台，有助于推动文本源的声明验证研究。</li>
</ul>

<p><strong>数据集及代码</strong>：<a href="https://fever.ai/">FEVER</a></p>

<h2 id="表格问答数据集">表格问答数据集</h2>

<h3 id="wikitablequestions">WikiTableQuestions</h3>

<p><strong>介绍</strong>：WikiTableQuestions由Pasupat等人在《Compositional Semantic Parsing on Semi-Structured Tables》中提出。该数据集基于HTML表格，包括22,033个问题-答案对，问题由Amazon Mechanical Turk工人编写，表格来自维基百科，包含至少8行和5列。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>问题不是通过预定义模板设计，而是由用户手工编写，展现出高度的语言变异性；</li>
  <li>相较于之前的知识库数据集，涵盖了近4,000个独特的列标题，涉及的关系比封闭领域数据集更多；</li>
  <li>问题涵盖广泛领域，需要进行表格查找、聚合、超类（如最大值、最小值）、算术运算、连接和合并等操作。</li>
</ul>

<h3 id="ait-qa">AIT-QA</h3>

<p><strong>介绍</strong>：AIT-QA（Airline Industry Table QA）由Katsis等人在《AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry》中提出。AIT-QA是一个特定于航空行业的表格问答数据集。该数据集包含515个问题，这些问题由人工注释者编写，基于从美国证券交易委员会（SEC）公开的主要航空公司2017-2019财年的报告中提取的116个表格。数据集还包含关于问题性质的注释，标记出那些需要层次化标题、特定领域术语和同义改写形式的问题。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>表格布局更加复杂，相较于传统的表格问答数据集具有更高的挑战性；</li>
  <li>包含注释，标记出需要层次化标题、特定领域术语和同义改写形式的问题。</li>
</ul>

<p><strong>数据集及代码</strong>：公开可用：<a href="https://github.com/IBM/AITQA">GitHub - AIT-QA</a>.</p>

<h3 id="tabfact">TabFact</h3>

<p><strong>介绍</strong>：TabFact由Chen等人在《TabFact: A Large-scale Dataset for Table-based Fact Verification》中提出。TabFact是一个大规模数据集，包含117,854个手工注释的陈述，涉及16,573个维基百科表格。这些陈述的关系被分类为“支持”（ENTAILED）和“反驳”（REFUTED）。TabFact是第一个用于评估结构化数据上语言推理的数据集，涉及符号和语言方面的混合推理技能。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>提供大规模的表格基础事实验证数据；</li>
  <li>关系分类为“支持”或“反驳”，挑战语言推理和结构化数据处理能力。</li>
</ul>

<p><strong>数据集及代码</strong>：公开可用：<a href="https://tabfact.github.io/">TabFact</a></p>

<h3 id="sqa-sequentialqa">SQA (SequentialQA)</h3>

<p><strong>介绍</strong>：SQA由Iyyer等人在《Search-based Neural Structured Learning for Sequential Question Answering》中提出。SQA数据集旨在探讨在HTML表格上回答一系列相关问题的任务。该数据集包含6,066个序列，总计17,553个问题。</p>

<p><strong>特点</strong>：</p>
<ul>
  <li>专注于在HTML表格中回答相关问题的序列；</li>
  <li>提供了丰富的序列问题集，涵盖了多个问题的顺序和关系。</li>
</ul>

<p><strong>数据集及代码</strong>：公开可用：<a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></p>

<h1 id="to-be-continued">(To be continued…)</h1>

<h2 id="acknowledgment">Acknowledgment</h2>
<p>I would like to express my sincere gratitude to my advisor, Jiaoyan Chen, for his invaluable guidance throughout this research. His generous sharing of datasets and resources has been instrumental in the development of this study.</p>

<h2 id="reference">Reference</h2>
<ol>
  <li>Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H. and Wang, W., 2020. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. arXiv preprint arXiv:2004.07347.</li>
  <li>Chen, W., Chang, M.W., Schlinger, E., Wang, W. and Cohen, W.W., 2020. Open question answering over tables and text. arXiv preprint arXiv:2010.10439.</li>
  <li>Herzig, J., Müller, T., Krichene, S. and Eisenschlos, J.M., 2021. Open domain question answering over tables via dense retrieval. arXiv preprint arXiv:2103.12011.</li>
  <li>Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B. and Wang, W.Y., 2021. Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122.</li>
  <li>Zhao, Y., Li, Y., Li, C. and Zhang, R., 2022. MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data. arXiv preprint arXiv:2206.01347.</li>
  <li>Qi, P., Lin, X., Mehr, L., Wang, Z. and Manning, C.D., 2019. Answering complex open-domain questions through iterative query generation. arXiv preprint arXiv:1910.07000.</li>
  <li>Joshi, M., Choi, E., Weld, D.S. and Zettlemoyer, L., 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</li>
  <li>Thorne, J., Vlachos, A., Christodoulopoulos, C. and Mittal, A., 2018. FEVER: a large-scale dataset for fact extraction and VERification. arXiv preprint arXiv:1803.05355.</li>
  <li>Pasupat, P. and Liang, P., 2015. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305.</li>
  <li>Katsis, Y., Chemmengath, S., Kumar, V., Bharadwaj, S., Canim, M., Glass, M., Gliozzo, A., Pan, F., Sen, J., Sankaranarayanan, K. and Chakrabarti, S., 2021. AIT-QA: Question answering dataset over complex tables in the airline industry. arXiv preprint arXiv:2106.12944.</li>
  <li>Cheng, Z., Dong, H., Wang, Z., Jia, R., Guo, J., Gao, Y., Han, S., Lou, J.G. and Zhang, D., 2021. Hitab: A hierarchical table dataset for question answering and natural language generation. arXiv preprint arXiv:2108.06712.</li>
  <li>Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X. and Wang, W.Y., 2019. Tabfact: A large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164.</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[Title: Dataset for Question Answering (CN) [To be continued…] Date: 2024-08-14 13:21:39 Tags: Dataset]]></summary></entry><entry><title type="html">TableRAG(CN)</title><link href="http://localhost:4000/Dataset-for-Question-Answering/" rel="alternate" type="text/html" title="TableRAG(CN)" /><published>2024-08-15T07:42:00+01:00</published><updated>2024-08-15T07:42:00+01:00</updated><id>http://localhost:4000/Dataset-for-Question-Answering</id><content type="html" xml:base="http://localhost:4000/Dataset-for-Question-Answering/"><![CDATA[<p><strong>Title:</strong> Dataset for Question Answering (CN) [To be continued…]<br />
<strong>Date:</strong> 2024-08-14 13:21:39<br />
<strong>Tags:</strong> Dataset</p>

<p>© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from the author. For inquiries, please contact me at yuhang.wu-4 [at] postgrad.manchester.ac.uk.</p>

<h1 id="overview-of-qa-datasets">Overview of QA Datasets</h1>

<p>Question Answering (QA) systems are a core area of research in natural language processing, aimed at extracting precise answers from various information sources. To meet diverse research needs and application scenarios, various QA datasets have emerged, systematically classified according to data format and task characteristics. This overview introduces several important QA datasets, covering a range of data types and task requirements. These datasets provide rich resources for testing and training, driving performance improvements and technological advancements in QA systems across different information formats.</p>

<h2 id="open-domain-qa">Open-Domain QA</h2>

<ul>
  <li><strong><a href="https://hybridqa.github.io/">HybridQA</a></strong>: Each question is aligned with a Wikipedia table and multiple paragraphs, linking table cells to paragraphs.</li>
  <li><strong><a href="https://ott-qa.github.io/">OTT-QA</a></strong>: Based on the HybridQA dataset, combining text and table evidence to create an open-domain QA benchmark.</li>
  <li><strong><a href="https://arxiv.org/pdf/2103.12011">NQ-Table</a></strong>: An open-domain QA dataset that combines table and text evidence.</li>
  <li><strong><a href="https://nextplusplus.github.io/TAT-QA/">TAT-QA</a></strong>: Similar to FinQA but includes both arithmetic questions and range questions.</li>
  <li><strong><a href="https://finqasite.github.io/">FinQA</a></strong>: A financial report dataset based on HybridQA, including only arithmetic questions (excluding range questions).</li>
  <li><strong><a href="https://github.com/psunlpgroup/MultiHiertt">MultiHiertt</a></strong>: A numerical reasoning dataset with hierarchical tables and text.</li>
  <li><strong><a href="https://nlp.cs.washington.edu/triviaqa/">TriviaQA</a></strong>: An open-domain QA dataset with multiple domains, providing rich evidence paragraphs for answering questions.</li>
  <li><strong><a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></strong>: Provides natural language questions and their corresponding Wikipedia paragraphs, supporting open-domain QA research.</li>
  <li><strong><a href="https://hotpotqa.github.io/">HotpotQA</a></strong>: Provides multi-hop reasoning questions and answers, requiring information retrieval from multiple documents to answer.</li>
  <li><strong><a href="https://fever.ai/">FEVER</a></strong>: Primarily used for fact verification tasks, offering news articles and corresponding truth judgment questions.</li>
</ul>

<h2 id="table-based-qa">Table-Based QA</h2>

<ul>
  <li><strong><a href="https://ppasupat.github.io/WikiTableQuestions/">WikiTableQuestions (WTQ)</a></strong>: Contains 22,033 question-answer pairs with 2,108 tables; manually annotated questions and answers.</li>
  <li><strong><a href="https://github.com/salesforce/WikiSQL">WikiSQL</a></strong>: SQL-annotated tables from Wikipedia, including 81,000 questions and 24,000 tables.</li>
  <li><strong><a href="https://github.com/IBM/AITQA">AIT-QA</a></strong>: Involves hierarchical tables with 116 tables about U.S. flights and 515 questions.</li>
  <li><strong><a href="https://arxiv.org/pdf/2108.06712v3">HiTab</a></strong>: A dataset for hierarchical tables.</li>
  <li><strong><a href="https://github.com/google-research/tapas">TAPAS</a></strong>: A table-based QA dataset combining SQL queries and natural language questions, covering various table formats.</li>
  <li><strong><a href="https://github.com/wtmo/TabFact">TabFact</a></strong>: A fact verification dataset with tables, providing a large number of tables and corresponding fact-checking questions.</li>
  <li><strong><a href="https://github.com/ABSA-HKUST/TableQA">TableQA</a></strong>: Covers QA tasks across various table structures, focusing on extracting information from tables.</li>
  <li><strong><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></strong>: Focuses on QA tasks involving structured data through SQL queries.</li>
</ul>

<h2 id="knowledge-graph-qa">Knowledge Graph QA</h2>

<ul>
  <li><strong>MetaQA (2018)</strong>: A knowledge graph QA dataset.</li>
  <li><strong>GRAPHQ (2016)</strong>: A knowledge graph QA dataset.</li>
  <li><strong>WebQSP (2016)</strong>: A QA dataset based on Freebase.</li>
  <li><strong>CWQ (2018)</strong>: An extended dataset based on WebQSP, using SPARQL query annotations to study semantic parsing.</li>
  <li><strong>LC-QuAD (2017)</strong> and <strong>LC-QuAD 2.0 (2019)</strong>: Knowledge graph datasets based on Wikidata and DBpedia, including 30,000 questions with SPARQL query annotations.</li>
  <li><strong>GrailQA (2021)</strong>: A knowledge graph QA dataset based on Freebase, with 64,000 questions and query annotations.</li>
  <li><strong>KQA Pro (2022)</strong>: A dataset with NL questions and SPARQL annotations, including knowledge graphs from FB15k-237 and aligned Wikidata entities and 3,000 additional entities.</li>
  <li><strong>Freebase QA (2014)</strong>: An early QA dataset using the Freebase knowledge graph, providing a large number of QA pairs about the knowledge graph.</li>
</ul>

<h2 id="knowledge-graph--text-qa">Knowledge Graph + Text QA</h2>

<ul>
  <li>Evaluates open-domain text QA datasets: TriviaQA (2017), WebQuestion (2013), CWQ (2018), WebQAP (2016), using knowledge graphs like WordNet, Freebase, ConceptNet.</li>
  <li><strong>WebQuestion (2013)</strong>: Contains natural language questions targeting Freebase and corresponding answers, widely used in knowledge graph and text QA research.</li>
  <li><strong>ComplexWebQuestions (CWQ) (2018)</strong>: Extends WebQuestion with complex SPARQL queries to test knowledge graph QA capabilities.</li>
</ul>

<h2 id="document-based-qa">Document-Based QA</h2>

<ul>
  <li><strong>SQuAD (2018)</strong>: A document QA dataset providing a large-scale collection of paragraphs and question pairs for assessing model comprehension. <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a></li>
  <li><strong>TrivialQA (2017)</strong>: A document QA dataset focusing on simple questions, mainly used for baseline model evaluation.</li>
  <li><strong>Natural Questions (NQ) (2019)</strong>: Provides natural language questions and their corresponding Wikipedia paragraphs, supporting open-domain QA research. <a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></li>
  <li><strong>SearchQA (2017)</strong>: A search engine-based QA dataset, including questions and answers extracted from web search results.</li>
  <li><strong>DocVQA (2020)</strong>: A document QA dataset focusing on extracting information from complex documents, including scanned documents and tables.</li>
  <li><strong>FEVEROUS (2020)</strong>: A fact verification QA dataset, expanding the FEVER dataset with additional documents and verification tasks.</li>
  <li><strong>HotpotQA (2018)</strong>: Provides multi-hop reasoning questions and answers, requiring information retrieval from multiple documents to answer. <a href="https://hotpotqa.github.io/">HotpotQA</a></li>
</ul>

<p>Sure, here is the refined text in natural, academic English and formatted in Markdown:</p>

<hr />

<p><strong>Title:</strong> Dataset for Question Answering (CN) [To be continued…]<br />
<strong>Date:</strong> 2024-08-14 13:21:39<br />
<strong>Tags:</strong> Dataset</p>

<p>© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from the author. For inquiries, please contact me at yuhang.wu-4 [at] postgrad.manchester.ac.uk.</p>

<h1 id="overview-of-qa-datasets-1">Overview of QA Datasets</h1>

<p>Question Answering (QA) systems are a core area of research in natural language processing, aimed at extracting precise answers from various information sources. To meet diverse research needs and application scenarios, various QA datasets have emerged, systematically classified according to data format and task characteristics. This overview introduces several important QA datasets, covering a range of data types and task requirements. These datasets provide rich resources for testing and training, driving performance improvements and technological advancements in QA systems across different information formats.</p>

<h2 id="open-domain-qa-1">Open-Domain QA</h2>

<ul>
  <li><strong><a href="https://hybridqa.github.io/">HybridQA</a></strong>: Each question is aligned with a Wikipedia table and multiple paragraphs, linking table cells to paragraphs.</li>
  <li><strong><a href="https://ott-qa.github.io/">OTT-QA</a></strong>: Based on the HybridQA dataset, combining text and table evidence to create an open-domain QA benchmark.</li>
  <li><strong><a href="https://arxiv.org/pdf/2103.12011">NQ-Table</a></strong>: An open-domain QA dataset that combines table and text evidence.</li>
  <li><strong><a href="https://nextplusplus.github.io/TAT-QA/">TAT-QA</a></strong>: Similar to FinQA but includes both arithmetic questions and range questions.</li>
  <li><strong><a href="https://finqasite.github.io/">FinQA</a></strong>: A financial report dataset based on HybridQA, including only arithmetic questions (excluding range questions).</li>
  <li><strong><a href="https://github.com/psunlpgroup/MultiHiertt">MultiHiertt</a></strong>: A numerical reasoning dataset with hierarchical tables and text.</li>
  <li><strong><a href="https://nlp.cs.washington.edu/triviaqa/">TriviaQA</a></strong>: An open-domain QA dataset with multiple domains, providing rich evidence paragraphs for answering questions.</li>
  <li><strong><a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></strong>: Provides natural language questions and their corresponding Wikipedia paragraphs, supporting open-domain QA research.</li>
  <li><strong><a href="https://hotpotqa.github.io/">HotpotQA</a></strong>: Provides multi-hop reasoning questions and answers, requiring information retrieval from multiple documents to answer.</li>
  <li><strong><a href="https://fever.ai/">FEVER</a></strong>: Primarily used for fact verification tasks, offering news articles and corresponding truth judgment questions.</li>
</ul>

<h2 id="table-based-qa-1">Table-Based QA</h2>

<ul>
  <li><strong><a href="https://ppasupat.github.io/WikiTableQuestions/">WikiTableQuestions (WTQ)</a></strong>: Contains 22,033 question-answer pairs with 2,108 tables; manually annotated questions and answers.</li>
  <li><strong><a href="https://github.com/salesforce/WikiSQL">WikiSQL</a></strong>: SQL-annotated tables from Wikipedia, including 81,000 questions and 24,000 tables.</li>
  <li><strong><a href="https://github.com/IBM/AITQA">AIT-QA</a></strong>: Involves hierarchical tables with 116 tables about U.S. flights and 515 questions.</li>
  <li><strong><a href="https://arxiv.org/pdf/2108.06712v3">HiTab</a></strong>: A dataset for hierarchical tables.</li>
  <li><strong><a href="https://github.com/google-research/tapas">TAPAS</a></strong>: A table-based QA dataset combining SQL queries and natural language questions, covering various table formats.</li>
  <li><strong><a href="https://github.com/wtmo/TabFact">TabFact</a></strong>: A fact verification dataset with tables, providing a large number of tables and corresponding fact-checking questions.</li>
  <li><strong><a href="https://github.com/ABSA-HKUST/TableQA">TableQA</a></strong>: Covers QA tasks across various table structures, focusing on extracting information from tables.</li>
  <li><strong><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></strong>: Focuses on QA tasks involving structured data through SQL queries.</li>
</ul>

<h2 id="knowledge-graph-qa-1">Knowledge Graph QA</h2>

<ul>
  <li><strong>MetaQA (2018)</strong>: A knowledge graph QA dataset.</li>
  <li><strong>GRAPHQ (2016)</strong>: A knowledge graph QA dataset.</li>
  <li><strong>WebQSP (2016)</strong>: A QA dataset based on Freebase.</li>
  <li><strong>CWQ (2018)</strong>: An extended dataset based on WebQSP, using SPARQL query annotations to study semantic parsing.</li>
  <li><strong>LC-QuAD (2017)</strong> and <strong>LC-QuAD 2.0 (2019)</strong>: Knowledge graph datasets based on Wikidata and DBpedia, including 30,000 questions with SPARQL query annotations.</li>
  <li><strong>GrailQA (2021)</strong>: A knowledge graph QA dataset based on Freebase, with 64,000 questions and query annotations.</li>
  <li><strong>KQA Pro (2022)</strong>: A dataset with NL questions and SPARQL annotations, including knowledge graphs from FB15k-237 and aligned Wikidata entities and 3,000 additional entities.</li>
  <li><strong>Freebase QA (2014)</strong>: An early QA dataset using the Freebase knowledge graph, providing a large number of QA pairs about the knowledge graph.</li>
</ul>

<h2 id="knowledge-graph--text-qa-1">Knowledge Graph + Text QA</h2>

<ul>
  <li>Evaluates open-domain text QA datasets: TriviaQA (2017), WebQuestion (2013), CWQ (2018), WebQAP (2016), using knowledge graphs like WordNet, Freebase, ConceptNet.</li>
  <li><strong>WebQuestion (2013)</strong>: Contains natural language questions targeting Freebase and corresponding answers, widely used in knowledge graph and text QA research.</li>
  <li><strong>ComplexWebQuestions (CWQ) (2018)</strong>: Extends WebQuestion with complex SPARQL queries to test knowledge graph QA capabilities.</li>
</ul>

<h2 id="document-based-qa-1">Document-Based QA</h2>

<ul>
  <li><strong><a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a></strong>: A document QA dataset providing a large-scale collection of paragraphs and question pairs for assessing model comprehension.</li>
  <li><strong>TrivialQA (2017)</strong>: A document QA dataset focusing on simple questions, mainly used for baseline model evaluation.</li>
  <li><strong><a href="https://ai.google.com/research/NaturalQuestions">Natural Questions</a></strong>: Provides natural language questions and their corresponding Wikipedia paragraphs, supporting open-domain QA research.</li>
  <li><strong>SearchQA (2017)</strong>: A search engine-based QA dataset, including questions and answers extracted from web search results.</li>
  <li><strong>DocVQA (2020)</strong>: A document QA dataset focusing on extracting information from complex documents, including scanned documents and tables.</li>
  <li><strong>FEVEROUS (2020)</strong>: A fact verification QA dataset, expanding the FEVER dataset with additional documents and verification tasks.</li>
  <li><strong><a href="https://hotpotqa.github.io/">HotpotQA</a></strong>: Provides multi-hop reasoning questions and answers, requiring information retrieval from multiple documents to answer.</li>
</ul>

<h1 id="detailed-qa-dataset-descriptions">Detailed QA Dataset Descriptions</h1>

<h2 id="open-domain-qa-datasets">Open-Domain QA Datasets</h2>

<h3 id="hybridqa"><a href="https://hybridqa.github.io/">HybridQA</a></h3>

<p><strong>Introduction</strong>: <a href="https://hybridqa.github.io/">HybridQA</a>, proposed by Chen et al. in “HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data,” addresses the challenge of reasoning over heterogeneous information. Unlike existing QA datasets that primarily focus on homogeneous information, HybridQA requires reasoning over both tables and text. Each question is associated with a Wikipedia table and multiple paragraphs related to the table’s entities. The questions are designed to require synthesizing both types of information, making the absence of either type insufficient for answering. Experiments show that while baseline models using only tables or text perform poorly, a hybrid model that integrates both types achieves significantly better results, though still trailing behind human performance. This dataset serves as a challenging benchmark for studying QA tasks involving heterogeneous information.</p>

<p><strong>Dataset and Code</strong>: Available publicly at <a href="https://github.com/wenhuchen/HybridQA">GitHub - HybridQA</a>.</p>

<h3 id="tat-qa">TAT-QA</h3>

<p><strong>Introduction</strong>: TAT-QA, introduced by Zhu et al. in “TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,” is a large-scale QA dataset aimed at advancing research on complex financial data that combines tables and text. It involves questions requiring numerical reasoning over financial reports.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Context includes a semi-structured table and at least two related paragraphs describing or supplementing the table content.</li>
  <li>Questions are generated by experts with rich financial knowledge, most being real-world application questions.</li>
  <li>Answers vary in format, including single span, multiple spans, and free-form responses.</li>
  <li>Answering often requires diverse numerical reasoning abilities such as addition, subtraction, multiplication, division, counting, comparison, sorting, and combinations thereof.</li>
  <li>Along with real answers, the dataset includes the corresponding reasoning processes and scales, when applicable.</li>
</ul>

<p><strong>Size</strong>: TAT-QA contains 16,552 questions based on 2,757 mixed-context financial reports.</p>

<h3 id="finqa">FinQA</h3>

<p><strong>Introduction</strong>: FinQA, proposed by Chen et al. in “FinQA: A Dataset of Numerical Reasoning over Financial Data,” focuses on deep QA over financial data, aiming to automate the analysis of extensive financial documents. Compared to general domain tasks, the financial domain involves complex numerical reasoning and understanding of heterogeneous representations.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Provides QA pairs created by financial experts, with annotated gold reasoning programs to ensure full interpretability.</li>
  <li>Introduces baseline models and conducts extensive experiments, showing that popular large pre-trained models significantly lag behind experts in acquiring financial knowledge and performing complex multi-step numerical reasoning.</li>
  <li>Dataset and code are available at <a href="https://github.com/czyssrs/FinQA">GitHub - FinQA</a>.</li>
</ul>

<h3 id="multihiertt"><a href="https://github.com/psunlpgroup/MultiHiertt">MultiHiertt</a></h3>

<p><strong>Introduction</strong>: <a href="https://github.com/psunlpgroup/MultiHiertt">MultiHiertt</a>, introduced by Wang et al. in “MultiHiertt: A Multi-Modal Dataset for Hierarchical Table-Based Numerical Reasoning,” provides a dataset specifically for numerical reasoning over hierarchical tables. Unlike other table-based datasets that focus on flat or moderately complex tables, <a href="https://github.com/psunlpgroup/MultiHiertt">MultiHiertt</a> features hierarchical tables that reflect the multi-level nature of real-world financial documents.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Contains hierarchical tables with multiple levels of nesting, reflecting complex financial data structures.</li>
  <li>Focuses on multi-modal reasoning, requiring combining information from different hierarchical levels and types.</li>
</ul>

<p><strong>Size</strong>: <a href="https://github.com/psunlpgroup/MultiHiertt">MultiHiertt</a> includes a range of hierarchical tables and associated questions to test different levels of numerical reasoning.</p>

<p>Certainly! Here’s the refined version of the content in academic English with Markdown formatting:</p>

<hr />

<h3 id="hotpotqa"><a href="https://hotpotqa.github.io/">HotpotQA</a></h3>

<p><strong>Introduction</strong>: <a href="https://hotpotqa.github.io/">HotpotQA</a> was introduced by Yang et al. in “<a href="https://hotpotqa.github.io/">HotpotQA</a>: A Dataset for Diverse, Explainable Multi-hop Question Answering”. This dataset, based on English Wikipedia, includes approximately 113,000 crowd-sourced questions that require information from two introductory paragraphs of Wikipedia articles to answer.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Covers various reasoning strategies, including questions involving missing entities, intersection questions (e.g., “What satisfies both attribute A and attribute B?”), and comparison questions.</li>
  <li>Provides ten paragraphs, including golden paragraphs. In the open-domain full Wikipedia setting, models are given only the question and the entire Wikipedia.</li>
  <li>Evaluation metrics include answer accuracy (measured by Exact Match (EM) and word F1 score) and explainability (assessing the alignment of predicted supporting sentences with human-annotated sentences).</li>
</ul>

<h3 id="fever"><a href="https://fever.ai/">FEVER</a></h3>

<p><strong>Introduction</strong>: FEVER (Fact Extraction and VERification) was proposed by Thorne et al. in “FEVER: a large-scale dataset for Fact Extraction and VERification”. This dataset is used for verifying facts from textual sources and includes 185,445 statements generated by altering sentences from Wikipedia, which are subsequently verified.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Statements are classified as “support”, “refute”, or “not enough information”.</li>
  <li>For the first two categories, annotators record the sentences required to make a judgment.</li>
  <li>Developed a pipeline approach and compared it with well-designed predictors, demonstrating that FEVER is a challenging test platform that advances research in statement verification from textual sources.</li>
</ul>

<p><strong>Dataset and Code</strong>: <a href="https://fever.ai/">FEVER</a></p>

<h2 id="table-based-question-answering-datasets">Table-Based Question Answering Datasets</h2>

<h3 id="wikitablequestions-wtq"><a href="https://ppasupat.github.io/WikiTableQuestions/">WikiTableQuestions (WTQ)</a></h3>

<p><strong>Introduction</strong>: <a href="https://ppasupat.github.io/WikiTableQuestions/">WikiTableQuestions (WTQ)</a>, introduced by Pasupat et al. in “Compositional Semantic Parsing on Semi-Structured Tables”, is based on HTML tables and includes 22,033 question-answer pairs. The questions were crafted by Amazon Mechanical Turk workers, and the tables were sourced from Wikipedia, each containing at least 8 rows and 5 columns.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Questions are manually crafted by users rather than through predefined templates, showcasing significant linguistic variability.</li>
  <li>Compared to previous knowledge base datasets, it covers nearly 4,000 unique column headers and involves more relationships than closed-domain datasets.</li>
  <li>Questions span a wide range of domains and require various operations, including table retrieval, aggregation, superlatives (e.g., maximum, minimum), arithmetic calculations, joining, and merging.</li>
</ul>

<h3 id="ait-qa"><a href="https://github.com/IBM/AITQA">AIT-QA</a></h3>

<p><strong>Introduction</strong>: <a href="https://github.com/IBM/AITQA">AIT-QA</a> (Airline Industry Table QA), proposed by Katsis et al. in “AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry”, is a domain-specific table-based question answering dataset. It includes 515 questions crafted by human annotators based on 116 tables extracted from the annual reports of major airlines from the SEC filings (2017-2019). The dataset also contains annotations on the nature of questions, highlighting those requiring hierarchical headers, domain-specific terminology, and synonym variations.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>The table layouts are more complex, presenting greater challenges compared to traditional table-based QA datasets.</li>
  <li>Includes annotations indicating questions that require hierarchical headers, domain-specific terminology, and synonym variations.</li>
</ul>

<p><strong>Dataset and Code</strong>: Available publicly: <a href="https://github.com/IBM/AITQA">GitHub - AIT-QA</a></p>

<h3 id="tabfact"><a href="https://tabfact.github.io/">TabFact</a></h3>

<p><strong>Introduction</strong>: <a href="https://tabfact.github.io/">TabFact</a>, introduced by Chen et al. in “TabFact: A Large-scale Dataset for Table-based Fact Verification”, is a large-scale dataset containing 117,854 manually annotated statements involving 16,573 Wikipedia tables. The relationships in these statements are classified as “entailed” or “refuted”. <a href="https://tabfact.github.io/">TabFact</a> is the first dataset designed to evaluate language reasoning over structured data, involving both symbolic and linguistic reasoning skills.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Provides a large-scale dataset for fact verification based on tables.</li>
  <li>Relationship classification as “entailed” or “refuted”, challenging the model’s language reasoning and structured data processing capabilities.</li>
</ul>

<p><strong>Dataset and Code</strong>: Available publicly: <a href="https://tabfact.github.io/">TabFact</a></p>

<h3 id="sqa-sequentialqa"><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a> (SequentialQA)</h3>

<p><strong>Introduction</strong>: <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a>, proposed by Iyyer et al. in “Search-based Neural Structured Learning for Sequential Question Answering”, explores the task of answering a sequence of related questions over HTML tables. The dataset comprises 6,066 sequences, totaling 17,553 questions.</p>

<p><strong>Features</strong>:</p>
<ul>
  <li>Focuses on answering a series of related questions within HTML tables.</li>
  <li>Provides a rich set of sequential questions, covering various problem orders and interrelationships.</li>
</ul>

<p><strong>Dataset and Code</strong>: Available publicly: <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></p>

<h1 id="to-be-continued">(To be continued…)</h1>

<h2 id="acknowledgment">Acknowledgment</h2>
<p>I would like to express my sincere gratitude to my advisor, Jiaoyan Chen, for his invaluable guidance throughout this research. His generous sharing of datasets and resources has been instrumental in the development of this study.</p>

<h2 id="reference">Reference</h2>
<ol>
  <li>Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H. and Wang, W., 2020. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. arXiv preprint arXiv:2004.07347.</li>
  <li>Chen, W., Chang, M.W., Schlinger, E., Wang, W. and Cohen, W.W., 2020. Open question answering over tables and text. arXiv preprint arXiv:2010.10439.</li>
  <li>Herzig, J., Müller, T., Krichene, S. and Eisenschlos, J.M., 2021. Open domain question answering over tables via dense retrieval. arXiv preprint arXiv:2103.12011.</li>
  <li>Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B. and Wang, W.Y., 2021. Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122.</li>
  <li>Zhao, Y., Li, Y., Li, C. and Zhang, R., 2022. MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data. arXiv preprint arXiv:2206.01347.</li>
  <li>Qi, P., Lin, X., Mehr, L., Wang, Z. and Manning, C.D., 2019. Answering complex open-domain questions through iterative query generation. arXiv preprint arXiv:1910.07000.</li>
  <li>Joshi, M., Choi, E., Weld, D.S. and Zettlemoyer, L., 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</li>
  <li>Thorne, J., Vlachos, A., Christodoulopoulos, C. and Mittal, A., 2018. FEVER: a large-scale dataset for fact extraction and VERification. arXiv preprint arXiv:1803.05355.</li>
  <li>Pasupat, P. and Liang, P., 2015. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305.</li>
  <li>Katsis, Y., Chemmengath, S., Kumar, V., Bharadwaj, S., Canim, M., Glass, M., Gliozzo, A., Pan, F., Sen, J., Sankaranarayanan, K. and Chakrabarti, S., 2021. AIT-QA: Question answering dataset over complex tables in the airline industry. arXiv preprint arXiv:2106.12944.</li>
  <li>Cheng, Z., Dong, H., Wang, Z., Jia, R., Guo, J., Gao, Y., Han, S., Lou, J.G. and Zhang, D., 2021. Hitab: A hierarchical table dataset for question answering and natural language generation. arXiv preprint arXiv:2108.06712.</li>
  <li>Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X. and Wang, W.Y., 2019. Tabfact: A large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164.</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[Title: Dataset for Question Answering (CN) [To be continued…] Date: 2024-08-14 13:21:39 Tags: Dataset]]></summary></entry><entry><title type="html">LLM for table data enhancement-CN</title><link href="http://localhost:4000/LLM-for-table-data-enhancement-CN/" rel="alternate" type="text/html" title="LLM for table data enhancement-CN" /><published>2024-08-15T00:00:00+01:00</published><updated>2024-08-15T00:00:00+01:00</updated><id>http://localhost:4000/LLM-for-table-data-enhancement-CN</id><content type="html" xml:base="http://localhost:4000/LLM-for-table-data-enhancement-CN/"><![CDATA[<p>随着大型语言模型（LLM）在自然语言处理（NLP）领域的广泛应用，它们在各种任务中展现了显著的能力，包括文本生成、问答系统和情感分析。然而，尽管LLM在处理非结构化数据方面表现优异，它们在处理结构化数据，特别是表格数据时，仍面临诸多挑战。表格数据的结构化特性和丰富的语义信息对LLM提出了更高的要求，传统的文本处理方法往往无法直接适用于此类数据。</p>

<p>本文旨在总结和讨论处理表格数据的关键技术和方法。我们深入分析了LLM在处理表格数据时重要的文献和方法，这些研究尝试解决LLM在处理表格数据时遇到的难题，包括表格数据的编码、查询和生成等方面。通过对ColBERT、ColBERTv2、DPR以及RAFT等技术的详细探讨，我们展示了当前LLM在表格数据处理领域的主要进展和创新。这些技术不仅提升了表格数据的理解和检索能力，也为未来的研究提供了重要的参考。</p>
<h2 id="llm-for-table">LLM for Table</h2>

<p><a href="https://arxiv.org/pdf/2402.05121">Large Language Model for Table Processing</a></p>

<h3 id="1-llms表格处理方法分类">1. LLMs表格处理方法分类</h3>

<p>在处理表格数据时，研究主要集中在训练和提示这两大方法上，具体包括：</p>

<ol>
  <li><strong>基于训练的方法</strong>：
    <ul>
      <li><strong>任务特定微调</strong>：例如TaPas和TaBERT，通过调整模型结构和训练目标来提升表格任务的性能。</li>
      <li><strong>指令微调</strong>：如TableLlama和Table-GPT，通过在多个数据集上的微调，改进模型在未见任务上的表现。</li>
      <li><strong>检索增强方法</strong>：如ITR和LI-RAGE，将大型表格分割成子表，并联合训练检索器和阅读器。</li>
    </ul>
  </li>
  <li><strong>基于提示的方法</strong>：
    <ul>
      <li><strong>表格序列化</strong>：将表格转换为线性文本格式，使LLMs更容易处理。</li>
      <li><strong>少样本学习的示例选择</strong>：挑选与目标任务最相关的示例，以提高模型性能。</li>
    </ul>
  </li>
  <li><strong>基于代理的方法</strong>：
    <ul>
      <li><strong>复杂任务分解</strong>：例如DIN-SQL，通过将复杂任务分解为更小的子任务来提升准确性。</li>
      <li><strong>动作定义</strong>：将软件工具的API抽象为动作，方便LLMs调用。</li>
      <li><strong>反思和修正</strong>：通过生成多个推理路径并选择最一致的答案，或通过自我修正来提高模型的准确性。</li>
      <li><strong>多任务框架</strong>：例如StructGPT，能够处理多种表格任务。</li>
    </ul>
  </li>
</ol>

<h3 id="具体方法归纳">具体方法归纳</h3>

<ol>
  <li><strong>任务特定微调</strong>：
    <ul>
      <li><strong>TaPas</strong>：扩展BERT模型结构，进行表格预训练和微调。</li>
      <li><strong>TaBERT</strong>：编码与输入语句相关的表格内容，并使用垂直注意力机制。</li>
      <li><strong>TURL</strong>：将表格组件的信息编码为单独的输入嵌入，并将其融合在一起。</li>
    </ul>
  </li>
  <li><strong>指令微调</strong>：
    <ul>
      <li><strong>Table-GPT</strong>：使用合成-增强的方法构建指令微调数据集。</li>
      <li><strong>TableLlama</strong>：利用现有数据集的真实数据进行指令微调。</li>
      <li><strong>Magicoder</strong>：收集开源代码片段，生成编程问题和解决方案进行指令微调。</li>
    </ul>
  </li>
  <li><strong>检索增强方法</strong>：
    <ul>
      <li><strong>ITR</strong>：将大型表格分割成子表，并联合训练检索器和阅读器。</li>
      <li><strong>DB-GPT</strong>：支持多种功能，如检索增强生成、微调和代理。</li>
    </ul>
  </li>
  <li><strong>表格序列化</strong>：
    <ul>
      <li>将表格内容线性化，插入列分隔符。</li>
      <li>表格模式可以用普通文本或CREATE TABLE语句表示。</li>
    </ul>
  </li>
  <li><strong>少样本学习的示例选择</strong>：
    <ul>
      <li>选择与目标任务最相关的示例，平衡质量和数量。</li>
    </ul>
  </li>
  <li><strong>复杂任务分解</strong>：
    <ul>
      <li><strong>DIN-SQL</strong>：将text-to-SQL任务分解为子任务，生成中间子查询。</li>
    </ul>
  </li>
  <li><strong>动作定义</strong>：
    <ul>
      <li><strong>SheetCopilot</strong>：将现有电子表格软件API建模为原子动作，通过嵌入和聚类方法设计。</li>
      <li><strong>ReAcTable</strong>：扩展ReAct框架，定义三种动作：生成SQL查询、生成Python代码和直接回答问题。</li>
    </ul>
  </li>
  <li><strong>反思和修正</strong>：
    <ul>
      <li>生成多个推理路径并选择最一致的答案。</li>
      <li>采用提案和修正机制，反思并改进过去的动作。</li>
    </ul>
  </li>
  <li><strong>多任务框架</strong>：
    <ul>
      <li><strong>StructGPT</strong>：通过开发针对网页表格、数据库和知识图谱的三种动作，解决多种表格任务。</li>
    </ul>
  </li>
</ol>

<p>这篇综述系统地总结了LLMs在表格处理任务中的最新进展和具体方法，并为未来的研究和应用提供了参考。</p>

<h2 id="llm-on-tabular-data-retriever">LLM on Tabular Data (Retriever)</h2>

<p><a href="https://arxiv.org/pdf/2402.17944">Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding</a></p>

<h3 id="52-大型语言模型在问答任务中的普遍能力">5.2 大型语言模型在问答任务中的普遍能力</h3>

<p>表8列出了研究大型语言模型（LLMs）在问答和推理任务中的效果的论文及所探讨的模型。虽然GPT-3.5和GPT-4是最受欢迎的LLM，但这些模型并未特别优化用于表格任务。然而，结合一些提示工程（prompt engineering）技巧（如Chain of Thought，CoT），它们在执行复杂的表格推理任务时表现出色。</p>

<h3 id="数值问答numerical-qa">数值问答（Numerical QA）</h3>

<p>数值问答任务涉及数学推理，例如，“美国运通的每笔交易的平均支付量是多少？”在许多实际应用中，如处理财务文件和年报，这种数学推理任务非常常见。Akhtar等（2023）发现，FlanT5和GPT-3.5在各种数值推理任务中表现优于其他模型。在DOCMATH-EVAL（Zhao等，2023d）数据集上，使用CoT的GPT-4显著优于其他LLM，而开源LLM（如LLaMa-2、Vicuna、Mistral、Starcoder、MPT、Qwen、AquilaChat2等）则表现较差。</p>

<h3 id="text2sql">Text2SQL</h3>

<p>Liu等（2023c）设计了一个问题匹配器，识别三种关键词类型：1）列名相关术语，2）限制相关短语（如“前十名”），3）算法或模块关键词。一旦识别了这些关键词，该模块将与每列相关的具体限制合并成统一的组合，然后将其与第三种关键词所指示的SQL算法或模块匹配。Zhang等（2023d）选择了一种更直接的方法，即让LLaMa-2基于问题和表模式生成SQL语句。Sun等（2023b）在Text2SQL任务上对PaLM-2进行了微调，在Spider数据集上取得了显著成绩。OpenTab（Kong等，2024）开发了一个基于LLM的开放域表格问答框架，结合了一个SQL生成模块。今天在Spider上的顶级模型包括Dong等（2023）、Gao等（2024）和Pourreza &amp; Rafiei（2023），他们都建立在OpenAI的GPT模型之上。SQL生成在业界很受欢迎，许多开源微调模型也可用。</p>

<h3 id="模型大小对性能的影响">模型大小对性能的影响</h3>

<p>Chen（2023）发现模型大小确实重要：在WebTableQuestions上，比较6.7B和175B的GPT-3模型时，较小的模型仅达到了较大模型得分的一半。在TabFact上，他们发现较小的模型（&lt;=6.7B）的准确性几乎是随机的。</p>

<h3 id="微调还是不微调">微调还是不微调？</h3>

<p>一些较大的模型在各种表格任务（包括问答和事实验证任务）上进行了微调。Li等（2023d）发现，微调总是有助于提高各种表格任务的性能，尤其是在零样本设置中改进最为显著。Ye等（2023b）使用PASTA（Gu等，2022）模型在TabFact上得分更高（93.00%），相比之下，GPT-3 Codex（code-davinci-002）得分为85.60%。PASTA在一个由维基百科表格组成的120万条目合成语料库上进行了预训练，用于六种类型的句子-表格填空任务。这表明在使用微调的LLM在表格任务上仍有一些好处。</p>

<p>然而，与在预测和生成任务上工作的其他方法相比，微调并不常见。这可能是因为LLMs（如GPT-3.5，GPT-4）在开箱即用的问答任务中表现良好。在Spider上的SQL生成中，DIN-SQL（Pourreza &amp; Rafiei，2023）和DAIL-SQL（Gao等，2024）是使用GPT-4的推理技术，超越了以前微调的较小模型。有趣的是，在Gao等（2024）的论文中，作者在Text2SQL任务上微调了一个Llama 2 13B模型，但该模型并没有超过未微调的GPT-4模型。相反，许多使用LLM进行表格理解任务的论文专注于调整序列化、提示工程、搜索和检索以及端到端管道（用户界面）等方面的内容，我们将在下一部分进一步描述。</p>

<h3 id="53-表格问答的关键组成部分">5.3 表格问答的关键组成部分</h3>

<p>在最简单的问答架构中，一个LLM接收一个输入提示（查询和序列化表格），并返回一个答案。在更复杂的架构中，系统可能连接到外部数据库或程序。大多数情况下，知识库可能无法适应LLM的上下文长度或内存。因此，LLM在表格问答</p>

<p>中的独特挑战包括：查询意图的消歧、搜索和检索、输出类型和格式，以及需要程序之间迭代调用的多轮设置。在本节中，我们将进一步描述这些组件。</p>

<h3 id="531-查询意图消歧">5.3.1 查询意图消歧</h3>

<p>Zha等（2023）引入了命令链（Chain-of-command，CoC）的概念，将用户输入转换为一系列中间命令操作。例如，一个LLM需要首先检查任务是否需要检索、数学推理、表格操作，以及如果指令太模糊无法回答问题。他们构建了一个命令链指令数据集，以微调LLMs生成这些命令。Deng等（2022b）建议将问答任务分为三个子任务：澄清需求预测（CNP）以确定是否需要提出澄清不确定性的问；澄清问题生成（CQG），如果CNP检测到需要澄清，则生成一个澄清问题作为响应；以及对话问答（CQA），如果不需要澄清，则直接生成答案作为响应。他们训练了一个UniPCQA模型，通过多任务学习统一了问答中的所有子任务。</p>

<hr />

<h3 id="搜索和检索方法归纳总结">搜索和检索方法归纳总结</h3>

<h3 id="1-检索模块性能提升">1. 检索模块性能提升</h3>

<ul>
  <li><strong>Retriever Module</strong>: 返回最相关的前n个文档，性能越好，最终准确性越高（Zhao et al., 2023d）。<a href="https://arxiv.org/pdf/2311.09805">论文链接</a></li>
</ul>

<h3 id="2-表格采样方法">2. 表格采样方法</h3>

<p><a href="https://arxiv.org/abs/2312.09039">TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning</a></p>

<ul>
  <li><strong>多种采样方法</strong>: 包括行和列的采样以及表格打包方法（Sui et al., 2023c）。
    <ul>
      <li><strong>基于查询的采样</strong>: 检索与问题语义相似度最高的行，效果最好。</li>
      <li><strong>无采样、聚类、随机、均匀采样、内容快照方法</strong>: 相对较弱。</li>
    </ul>
  </li>
</ul>

<h3 id="3-表格排名方法">3. 表格排名方法</h3>

<p><a href="https://arxiv.org/abs/2307.07306">C3: Zero-shot Text-to-SQL with ChatGPT</a></p>

<ul>
  <li><strong>ChatGPT 排名表格</strong>: 生成十组检索结果，每组包含前四个表格，然后选择出现最频繁的一组（Dong et al., 2023）。</li>
  <li><strong>列排名</strong>: 根据与问题的相关性对所有列进行排名，匹配列名与问题词，或优先处理外键以帮助更准确的召回（Dong et al., 2023）。</li>
</ul>

<h3 id="4-三步检索架构">4. 三步检索架构</h3>

<ul>
  <li><strong>Dense Table Retrieval (DTR)</strong>: 基于RoBERTa的双编码器模型，识别与查询最相关的表格（Sundar &amp; Heck, 2023）。
<a href="https://aclanthology.org/2023.nlp4convai-1.6.pdf">cTBLS: Augmenting Large Language Models with Conversational Tables</a></li>
  <li><strong>Coarse System State Tracking</strong>: 使用三元组损失训练，对单元格进行排名（Sundar &amp; Heck, 2023）。</li>
  <li><strong>自然语言响应生成</strong>: 基于GPT-3.5，结合对话历史、排名的知识来源和查询生成响应（Sundar &amp; Heck, 2023）。</li>
</ul>

<h3 id="5-稠密和稀疏检索器结合">5. 稠密和稀疏检索器结合</h3>

<ul>
  <li><strong>Ada Embedding4 和 Contriever</strong>: 作为稠密检索器（Zhao et al., 2023d）。</li>
  <li><strong>BM25</strong>: 作为稀疏检索器（Zhao et al., 2023d）。</li>
  <li><strong>综合使用</strong>: 提取最相关的文本和表格证据。</li>
</ul>

<h3 id="6-获取额外信息的方法">6. 获取额外信息的方法</h3>

<p><a href="https://www.vldb.org/pvldb/vol17/p1132-gao.pdf">Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation</a></p>

<p>Code: <a href="https://github.com/BeachWang/DAIL-SQL">Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation</a></p>

<ul>
  <li><strong>策划样本用于上下文学习</strong>（Gao et al., 2024）
    <ul>
      <li><strong>随机选择</strong>: 随机选择k个示例。</li>
      <li><strong>问题相似度选择</strong>: 基于与问题Q的语义相似度选择k个示例。</li>
      <li><strong>掩蔽问题相似度选择</strong>: 掩蔽问题中的领域特定信息后进行相似度选择。</li>
      <li><strong>查询相似度选择</strong>: 选择与目标SQL查询s*相似的k个示例，使用另一个模型生成近似SQL查询s’。</li>
    </ul>
  </li>
</ul>

<h3 id="7-手动和随机示例选择">7. 手动和随机示例选择</h3>

<ul>
  <li><strong>手动策划和随机选择</strong>（Narayan et al., 2022）</li>
</ul>

<h2 id="li-rage">LI-RAGE</h2>

<p><a href="https://aclanthology.org/2023.acl-short.133.pdf">LI-RAGE: L ate I nteraction R etrieval A ugmented Generation with Explicit Signals for Open-Domain Table Question Answering</a></p>

<h3 id="文章内容简短总结">文章内容简短总结</h3>

<p>LI-RAGE框架是一种针对开放领域表格问答（TableQA）的新方法。该方法通过结合后期交互检索（Late Interaction，LI）模型和显式信号的检索增强生成（RAGE）损失，显著提升了表格问答的性能。与传统的检索-阅读器（Retriever-Reader）管道相比，LI-RAGE通过以下改进提供了更高的准确性和可靠性：</p>

<ol>
  <li><strong>后期交互模型（LI）</strong>：使用ColBERT模型对问题和表格进行逐词编码，捕捉更细粒度的交互信息，从而提升表格检索效果。</li>
  <li><strong>联合训练（RAGE损失）</strong>：将检索器和阅读器的信号结合进行联合训练，优化表格检索与答案生成的效果。</li>
  <li><strong>二进制相关性标记</strong>：在生成答案前添加二进制相关性标记（yes/no），以指示表格是否与问题相关，从而提高答案的可靠性。</li>
</ol>

<h3 id="示例问题">示例问题</h3>

<p>考虑到问题“哪个国家的人口最多？”</p>

<h3 id="表格数据集">表格数据集</h3>

<p>假设有如下表格数据：</p>

<p>表格1：</p>

<table>
  <thead>
    <tr>
      <th>国家</th>
      <th>人口</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>中国</td>
      <td>1,411百万</td>
    </tr>
    <tr>
      <td>印度</td>
      <td>1,366百万</td>
    </tr>
    <tr>
      <td>美国</td>
      <td>331百万</td>
    </tr>
  </tbody>
</table>

<p>表格2：</p>

<table>
  <thead>
    <tr>
      <th>国家</th>
      <th>面积</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>俄罗斯</td>
      <td>17百万 km²</td>
    </tr>
    <tr>
      <td>加拿大</td>
      <td>9.98百万 km²</td>
    </tr>
    <tr>
      <td>中国</td>
      <td>9.6百万 km²</td>
    </tr>
  </tbody>
</table>

<p>表格3：</p>

<table>
  <thead>
    <tr>
      <th>城市</th>
      <th>人口</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>纽约</td>
      <td>8百万</td>
    </tr>
    <tr>
      <td>东京</td>
      <td>14百万</td>
    </tr>
    <tr>
      <td>上海</td>
      <td>24百万</td>
    </tr>
  </tbody>
</table>

<h3 id="全流程示例">全流程示例</h3>

<h4 id="1-表格检索retriever">1. 表格检索（Retriever）</h4>

<p>检索器从表格语料库中选择与问题最相关的表格。在此示例中，检索器可能会选择表格1，因为它包含了国家与人口的相关信息。</p>

<p><strong>检索结果：</strong><br />
选择表格1：</p>

<table>
  <thead>
    <tr>
      <th>国家</th>
      <th>人口</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>中国</td>
      <td>1,411百万</td>
    </tr>
    <tr>
      <td>印度</td>
      <td>1,366百万</td>
    </tr>
    <tr>
      <td>美国</td>
      <td>331百万</td>
    </tr>
  </tbody>
</table>

<h4 id="2-答案生成reader">2. 答案生成（Reader）</h4>

<p>答案生成器模型将问题和检索到的表格作为输入，并生成答案。在此例中，答案生成器将问题“哪个国家的人口最多？”与表格1结合，通过识别关键词“国家”和“人口最多”，在表格中查找最大值对应的国家，生成答案：“中国”。</p>

<h4 id="3-二进制相关性标记binary-relevance-token">3. 二进制相关性标记（Binary Relevance Token）</h4>

<p>为确保答案生成器选择的表格是可靠的，在生成答案前添加二进制相关性标记。在训练过程中，系统学习到从黄金表格生成的答案前有“yes”，而从非黄金表格生成的答案前有“no”。此例中，生成的答案是从黄金表格（表格1）中得出的，因此答案前有“yes”。</p>

<p><strong>最终输出：</strong><br />
答案生成器输出：“yes 中国”。</p>

<h4 id="4-过滤与确定最终答案">4. 过滤与确定最终答案</h4>

<p>在推理阶段，如果答案生成器输出的前缀为“yes”，则表明该答案可靠。系统将优先选择标记为“yes”的答案；如果所有候选答案的前缀均为“no”，系统将依据答案生成器的置信度得分来选择最终答案。在此例中，系统识别到“yes”标记，确认该答案可靠，最终输出答案：“中国”。</p>

<h3 id="总结">总结</h3>

<p>上述流程展示了开放领域表格问答系统从输入问题到生成最终答案的完整过程。通过LI-RAGE框架，系统不仅能够从大量表格数据中有效检索相关信息，还能通过二进制相关性标记确保答案的可靠性。</p>

<h2 id="tap4llm">TAP4LLM</h2>

<p><a href="https://arxiv.org/pdf/2312.09039">TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning</a></p>

<h3 id="tap4llm方法详细总结">TAP4LLM方法详细总结</h3>

<h3 id="大型语言模型在表格数据中的应用">大型语言模型在表格数据中的应用</h3>

<p>随着自然语言处理领域大型语言模型（LLMs）的进展，研究者开始探索将这些模型应用于其他模态，如视觉和语音。然而，直接在表格领域使用传统LLMs面临两个主要挑战：</p>

<ol>
  <li><strong>全局表理解</strong>：LLMs如GPT存在令牌长度限制，难以读取并理解整个大表格，从而限制了其对表格全局信息的理解。</li>
  <li><strong>对表格领域的泛化</strong>：这些模型的训练过程主要针对自然语言，因此在处理表格数据时泛化能力较弱。</li>
</ol>

<p>尽管一些研究已经尝试将自然语言处理与表格数据分析相结合，LLMs在表格问答中的表现仍然受限。</p>

<h3 id="表格增强技术">表格增强技术</h3>

<p>表格增强技术旨在提高机器学习模型的泛化性能和鲁棒性。为了提升LLMs在表格领域的表现，研究者进行了多种增强方法的探索，包括结构化知识、常识知识和分析性知识的结合。研究表明，利用领域特定的元数据或知识图能够显著提高LLMs对表格数据的理解能力。例如:</p>
<ul>
  <li><strong>Jena et al. (2022)</strong> 提出了半自动地转换现有表格数据，创建多样化的自然语言推理实例，以提高零样本性能。</li>
  <li><strong>He et al. (2023)</strong> 提出了一个多任务元数据模型，利用字段分布和知识图信息，准确推断表格的分析元数据，并展示了其在智能数据分析产品中的应用。</li>
</ul>

<h3 id="tap4llm的核心组件">TAP4LLM的核心组件</h3>

<p>TAP4LLM通过以下三个核心组件解决了综合表格理解中的主要挑战：</p>

<ol>
  <li><strong>表格采样</strong>：从表格中选择和提取与查询最相关的行和列。
    <ul>
      <li><strong>基于规则的采样</strong>：通过预定义标准或规则，如随机采样、均匀采样和内容快照采样，进行表格采样。</li>
      <li><strong>基于嵌入的采样</strong>：通过语义和上下文表示选择行和列，采用基于语义的采样、基于中心点的采样等方法。</li>
      <li><strong>基于LLM的采样</strong>：利用强大的LLM来预测表格行和列的索引，但这种方法计算成本较高。</li>
    </ul>
  </li>
  <li><strong>表格增强</strong>：通过添加外部知识和元数据丰富表格信息。
    <ul>
      <li><strong>基于元数据的增强</strong>：包括维度/度量、语义字段类型、表格大小、统计特征和标题层次结构等信息的添加。</li>
      <li><strong>基于检索的增强</strong>：通过文档检索系统，从外部知识库中获取相关内容以减少幻觉或事实性错误。</li>
      <li><strong>基于自一致性的增强</strong>：通过迭代生成和改进查询与响应，提高模型的推理能力。</li>
    </ul>
  </li>
  <li><strong>表格打包与序列化</strong>：控制令牌分配，将表格和增强信息打包成适用于LLMs的序列。
    <ul>
      <li>经验研究显示，子表长度与增强信息长度的比例在5:5和4:6之间时，通常带来最佳性能。</li>
      <li>支持多种序列化格式，如HTML、XML、JSON、CSV、NL+Sep和Markdown等。</li>
    </ul>
  </li>
</ol>

<h3 id="结论">结论</h3>

<p>TAP4LLM通过表格采样、表格增强以及表格打包与序列化，解决了综合表格理解中的主要挑战，提升了LLMs在表格推理任务中的有效性。该方法不仅适用于表格建模，还能在金融、交通等领域中发挥重要作用，推动基于表格数据的研究。</p>

<h3 id="限制">限制</h3>

<p>代码生成方法已被提出用于将自然语言查询转换为可执行代码或结构化表示（Cheng et al., 2023; Gemmell and Dalton, 2023）。这一研究方向具有重要性，但由于篇幅限制，本研究未深入探讨此主题。当前的实证研究主要集中在英语场景，对多语言能力的讨论也将留待未来研究中进行。</p>

<h3 id="示例使用tap4llm进行表格数据分析">示例：使用TAP4LLM进行表格数据分析</h3>

<p>假设存在一个包含某公司过去几年季度财务报告的金融数据表格。表格列包括年份、季度、收入、支出、净利润和资产负债率。目标是通过自然语言查询“该公司在过去五年中的季度净利润趋势如何？”来生成准确的分析。</p>

<h3 id="1-表格采样">1. 表格采样</h3>

<p><strong>初始表格（T）</strong>：</p>

<table>
  <thead>
    <tr>
      <th>年份</th>
      <th>季度</th>
      <th>收入</th>
      <th>支出</th>
      <th>净利润</th>
      <th>资产负债率</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2019</td>
      <td>Q1</td>
      <td>1000</td>
      <td>800</td>
      <td>200</td>
      <td>50%</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q2</td>
      <td>1100</td>
      <td>850</td>
      <td>250</td>
      <td>48%</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q3</td>
      <td>1050</td>
      <td>820</td>
      <td>230</td>
      <td>49%</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q4</td>
      <td>1200</td>
      <td>900</td>
      <td>300</td>
      <td>47%</td>
    </tr>
    <tr>
      <td>2020</td>
      <td>Q1</td>
      <td>1300</td>
      <td>950</td>
      <td>350</td>
      <td>46%</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>Q4</td>
      <td>1600</td>
      <td>1200</td>
      <td>400</td>
      <td>45%</td>
    </tr>
  </tbody>
</table>

<p>为回答“该公司在过去五年中的季度净利润趋势如何？”这一查询，需要对表格进行采样。</p>

<p><strong>表格采样步骤</strong>：</p>

<ul>
  <li><strong>基于规则的采样</strong>：选择最近五年的数据（2019-2023）。</li>
  <li><strong>基于嵌入的采样</strong>：使用语义采样，选择与“净利润”相关的行和列。</li>
</ul>

<p><strong>采样后的子表（T’）</strong>：</p>

<table>
  <thead>
    <tr>
      <th>年份</th>
      <th>季度</th>
      <th>净利润</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2019</td>
      <td>Q1</td>
      <td>200</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q2</td>
      <td>250</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q3</td>
      <td>230</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>Q4</td>
      <td>300</td>
    </tr>
    <tr>
      <td>2020</td>
      <td>Q1</td>
      <td>350</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>Q4</td>
      <td>400</td>
    </tr>
  </tbody>
</table>

<h3 id="2-表格增强">2. 表格增强</h3>

<p><strong>元数据增强</strong>：</p>

<ul>
  <li><strong>维度/度量</strong>：添加“净利润”作为度量值。</li>
  <li><strong>统计特征</strong>：计算各季度的平均净利润、最大值和最小值。</li>
</ul>

<p><strong>检索增强</strong>：</p>

<ul>
  <li><strong>文档参考</strong>：从外部数据库检索相关的行业分析报告，解释财务术语如“净利润”的定义及其影响因素。</li>
  <li><strong>术语解释</strong>：提供“资产负债率”的解释，并说明其与净利润的关系。</li>
</ul>

<p><strong>自一致性增强</strong>：</p>

<ul>
  <li>使用自提示方法生成初步分析结果，并将这些结果重新输入模型以进一步提高准确性。</li>
</ul>

<h3 id="3-表格打包与序列化">3. 表格打包与序列化</h3>

<p><strong>序列化格式</strong>：选择JSON格式进行打包。</p>

<p><strong>打包后的序列</strong>：</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"query"</span><span class="p">:</span><span class="w"> </span><span class="s2">"该公司在过去五年中的季度净利润趋势如何？"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"table"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="nl">"年份"</span><span class="p">:</span><span class="w"> </span><span class="mi">2019</span><span class="p">,</span><span class="w"> </span><span class="nl">"季度"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q1"</span><span class="p">,</span><span class="w"> </span><span class="nl">"净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">200</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"年份"</span><span class="p">:</span><span class="w"> </span><span class="mi">2019</span><span class="p">,</span><span class="w"> </span><span class="nl">"季度"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q2"</span><span class="p">,</span><span class="w"> </span><span class="nl">"净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">250</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"年份"</span><span class="p">:</span><span class="w"> </span><span class="mi">2019</span><span class="p">,</span><span class="w"> </span><span class="nl">"季度"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q3"</span><span class="p">,</span><span class="w"> </span><span class="nl">"净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">230</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"年份"</span><span class="p">:</span><span class="w"> </span><span class="mi">2019</span><span class="p">,</span><span class="w"> </span><span class="nl">"季度"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q4"</span><span class="p">,</span><span class="w"> </span><span class="nl">"净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">300</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"年份"</span><span class="p">:</span><span class="w"> </span><span class="mi">2020</span><span class="p">,</span><span class="w"> </span><span class="nl">"季度"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q1"</span><span class="p">,</span><span class="w"> </span><span class="nl">"净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">350</span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="nl">"年份"</span><span class="p">:</span><span class="w"> </span><span class="mi">2023</span><span class="p">,</span><span class="w"> </span><span class="nl">"季度"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Q4"</span><span class="p">,</span><span class="w"> </span><span class="nl">"净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">400</span><span class="p">}</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nl">"metadata"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"度量值"</span><span class="p">:</span><span class="w"> </span><span class="s2">"净利润"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"统计特征"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"平均净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">275</span><span class="p">,</span><span class="w">
      </span><span class="nl">"最大净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">400</span><span class="p">,</span><span class="w">
      </span><span class="nl">"最小净利润"</span><span class="p">:</span><span class="w"> </span><span class="mi">200</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"augmentation"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"文档参考"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"相关行业分析报告链接"</span><span class="p">],</span><span class="w">
    </span><span class="nl">"术语解释"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"资产负债率"</span><span class="p">:</span><span class="w"> </span><span class="s2">"资产负债率是衡量公司负债与资产比率的指标，反映公司财务健康状况。"</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h3 id="4-最终分析结果">4. 最终分析结果</h3>

<p>使用TAP4LLM预处理后的数据和增强信息，生成最终的分析报告：</p>

<p><strong>生成的分析报告</strong>：</p>

<p>“根据该公司的财务数据，过去五年的季度净利润呈现逐步增长的趋势，从2019年Q1的200万增长到2023年Q4的400万。平均净利润为275万，最大值为400万，最小值为200万。此增长趋势反映了公司在财务管理和市场策略上的成功。此外，资产负债率从2019年的50%降低到2023年的45%，进一步显示了公司的财务健康状况。”</p>

<p>通过以上步骤，展示了如何利用TAP4LLM方法处理和分析表格数据，从而生成高质量的自然语言报告。</p>

<h3 id="例子使用tap4llm回答金融数据问题">例子：使用TAP4LLM回答金融数据问题</h3>

<p>假设有一个包含股票价格每日记录的金融数据表格，问题是关于特定时间段内某股票的平均价格。</p>

<h3 id="1-输入表格和问题">1. 输入表格和问题</h3>

<p><strong>表格T</strong>:</p>

<table>
  <thead>
    <tr>
      <th>日期</th>
      <th>股票代码</th>
      <th>开盘价</th>
      <th>收盘价</th>
      <th>最高价</th>
      <th>最低价</th>
      <th>成交量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2024-01-01</td>
      <td>AAPL</td>
      <td>150</td>
      <td>155</td>
      <td>157</td>
      <td>149</td>
      <td>1000000</td>
    </tr>
    <tr>
      <td>2024-01-02</td>
      <td>AAPL</td>
      <td>156</td>
      <td>158</td>
      <td>159</td>
      <td>155</td>
      <td>1200000</td>
    </tr>
    <tr>
      <td>2024-01-03</td>
      <td>AAPL</td>
      <td>157</td>
      <td>159</td>
      <td>160</td>
      <td>156</td>
      <td>1100000</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
    <tr>
      <td>2024-01-01</td>
      <td>MSFT</td>
      <td>210</td>
      <td>215</td>
      <td>217</td>
      <td>209</td>
      <td>900000</td>
    </tr>
    <tr>
      <td>2024-01-02</td>
      <td>MSFT</td>
      <td>216</td>
      <td>218</td>
      <td>219</td>
      <td>215</td>
      <td>950000</td>
    </tr>
    <tr>
      <td>2024-01-03</td>
      <td>MSFT</td>
      <td>217</td>
      <td>220</td>
      <td>221</td>
      <td>216</td>
      <td>920000</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
  </tbody>
</table>

<p><strong>问题Q</strong>:
“AAPL股票在2024年1月1日至2024年1月3日的平均收盘价是多少？”</p>

<h3 id="2-表格采样">2. 表格采样</h3>

<p>根据问题Q，只需提取AAPL股票在指定日期范围内的相关数据。可以使用基于语义的采样方法，通过计算查询Q与表格每行的相似度，选出相关行。</p>

<p><strong>采样后的子表T’</strong>:</p>

<table>
  <thead>
    <tr>
      <th>日期</th>
      <th>股票代码</th>
      <th>开盘价</th>
      <th>收盘价</th>
      <th>最高价</th>
      <th>最低价</th>
      <th>成交量</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2024-01-01</td>
      <td>AAPL</td>
      <td>150</td>
      <td>155</td>
      <td>157</td>
      <td>149</td>
      <td>1000000</td>
    </tr>
    <tr>
      <td>2024-01-02</td>
      <td>AAPL</td>
      <td>156</td>
      <td>158</td>
      <td>159</td>
      <td>155</td>
      <td>1200000</td>
    </tr>
    <tr>
      <td>2024-01-03</td>
      <td>AAPL</td>
      <td>157</td>
      <td>159</td>
      <td>160</td>
      <td>156</td>
      <td>1100000</td>
    </tr>
  </tbody>
</table>

<h3 id="3-表格增强">3. 表格增强</h3>

<p>为帮助LLM更好地理解表格数据，可以引入一些增强信息，如股票的历史数据统计信息或相关术语解释。</p>

<p><strong>增强信息</strong>:</p>

<ul>
  <li>维度/度量：股票代码为维度，收盘价为度量。</li>
  <li>术语解释：解释“收盘价”是指股票在交易日结束时的价格。</li>
  <li>历史统计信息：过去一年的平均收盘价、最高价、最低价等。</li>
</ul>

<h3 id="4-表格打包与序列化">4. 表格打包与序列化</h3>

<p>将采样后的表格和增强信息打包为一个序列，适用于LLM的输入。</p>

<p><strong>打包后的输入</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>问题: 请问AAPL股票在2024年1月1日至2024年1月3日的平均收盘价是多少？
表格:
| 日期       | 股票代码 | 收盘价 |
|------------|----------|--------|
| 2024-01-01 | AAPL     | 155    |
| 2024-01-02 | AAPL     | 158    |
| 2024-01-03 | AAPL     | 159    |

增强信息:
- 股票代码为维度，收盘价为度量。
- 收盘价是指股票在交易日结束时的价格。
- 过去

一年AAPL的平均收盘价为160，最高价为165，最低价为150。
</code></pre></div></div>

<h3 id="5-结果生成">5. 结果生成</h3>

<p>使用LLM来计算平均值并生成答案。</p>

<p><strong>答案</strong>:</p>

<p>“根据提供的数据，AAPL股票在2024年1月1日至2024年1月3日的平均收盘价为157.33。”</p>

<h2 id="colbert"><a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a></h2>

<h3 id="创新点">创新点</h3>

<ol>
  <li><strong>延迟交互框架</strong>：<a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>引入了一种延迟交互架构，通过将查询和文档的编码过程分离，并在查询和文档分别编码后再进行相似度计算。这使得可以预先计算文档的表示，减少了在线查询时的计算量。</li>
  <li><strong>最大相似度操作（MaxSim）</strong>：在评估查询和文档之间的相关性时，<a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>采用了最大相似度操作（MaxSim），即每个查询嵌入与文档嵌入之间的最大余弦相似度或L2距离，并将这些最大相似度值求和。这种机制既简单又高效。</li>
  <li><strong>BERT编码器共享</strong>：<a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>在查询和文档编码器之间共享一个BERT模型，但通过在查询和文档前分别加上特殊标记（[Q]和[D]）来区分输入。这种方法既节省了计算资源，又保持了模型的上下文理解能力。</li>
  <li><strong>查询和文档的分段和过滤</strong>：文档编码器在处理文档时，会过滤掉标点符号的嵌入，以减少计算量和存储空间。</li>
  <li><strong>基于向量相似性的检索</strong>：利用现有的向量相似性搜索库（如faiss），<a href="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075">ColBERT</a>可以通过高效的剪枝操作实现从大型文档集合中的端到端检索。</li>
</ol>

<h3 id="优点">优点</h3>

<ol>
  <li><strong>计算效率高</strong>：通过预计算文档表示和使用延迟交互机制，ColBERT在处理查询时大大减少了计算量，相对于其他基于BERT的模型，其速度提高了两个数量级。</li>
  <li><strong>高效的空间利用</strong>：通过对文档嵌入进行归一化和降维处理，ColBERT显著减少了存储空间需求，使其能够在实际应用中更加可行。</li>
  <li><strong>强大的扩展性</strong>：ColBERT的架构设计使其能够在不牺牲精度的情况下处理大规模文档集合，特别是在使用向量相似性搜索进行剪枝时，其检索效率大大提高。</li>
  <li><strong>端到端检索能力</strong>：ColBERT不仅可以用于重新排序预检索的文档集，还可以直接从大型文档集合中进行端到端检索，提高了整体检索系统的召回率和精度。</li>
</ol>

<h3 id="解决的问题">解决的问题</h3>

<ol>
  <li><strong>计算成本高</strong>：传统基于BERT的排序模型在查询-文档对的计算上非常耗时，ColBERT通过引入延迟交互和预计算机制，大大降低了在线计算成本。</li>
  <li><strong>响应时间长</strong>：高计算成本导致的长查询响应时间对用户体验有负面影响。ColBERT通过更高效的计算和检索机制，显著减少了查询延迟。</li>
  <li><strong>存储空间大</strong>：深度语言模型通常需要大量存储空间来保存文档表示，ColBERT通过归一化和降维处理减少了存储需求。</li>
  <li><strong>检索精度与效率的权衡</strong>：现有方法在提高检索效率时往往牺牲精度，ColBERT通过高效的延迟交互和向量相似性搜索，在不降低精度的情况下提高了检索效率。</li>
</ol>

<h3 id="colbert-使用流程示例详细和生动的讲解">ColBERT 使用流程示例：详细和生动的讲解</h3>

<h3 id="背景">背景</h3>

<p>假设你正在使用一个学术论文数据库，里面有数百万篇学术论文。你正在研究“机器学习的好处”，想找到最相关的论文。这就是ColBERT可以帮助你的地方。</p>

<h3 id="1-离线预处理和编码文档">1. 离线预处理和编码文档</h3>

<p>在任何查询发生之前，我们先对数据库中的每一篇论文进行预处理和编码。这是一个离线过程，类似于图书馆对所有书籍进行分类和编号。</p>

<ol>
  <li><strong>分割文档</strong>：将每篇论文分解成单词，比如“机器学习是一种数据分析的方法，可以自动构建分析模型”会被分解成“机器”、“学习”、“是”、“一种”、“数据”、“分析”……。</li>
  <li><strong>添加标记</strong>：在每篇论文的开头加上特殊标记，比如“[D]”，以表明这是一个文档。</li>
  <li><strong>BERT编码</strong>：使用BERT模型对每个单词进行编码，将它们转化为具有上下文意义的向量表示。这就像是为每个单词生成一个独特的数字签名。</li>
  <li><strong>过滤无关信息</strong>：去除标点符号等无关信息，保持重要的单词。</li>
  <li><strong>归一化和降维</strong>：对这些向量进行归一化处理，使它们的表示更加紧凑和高效，类似于将大文件压缩成小文件，便于存储和处理。</li>
  <li><strong>存储嵌入</strong>：将处理后的向量存储到数据库中，以备后续使用。</li>
</ol>

<h3 id="2-查询预处理和编码">2. 查询预处理和编码</h3>

<p>当你输入查询“机器学习的好处”时，ColBERT会立即对这个查询进行处理，这个过程是在线进行的。</p>

<ol>
  <li><strong>分割查询</strong>：将查询分解成单词，比如“机器”、“学习”、“的”、“好处”。</li>
  <li><strong>添加标记</strong>：在查询前面加上特殊标记，比如“[Q]”，以表明这是一个查询。</li>
  <li><strong>填充和BERT编码</strong>：将查询填充到固定长度并输入BERT模型，生成每个单词的上下文向量。这些向量表示了查询中每个单词的意义和它们之间的关系。</li>
  <li><strong>归一化和降维</strong>：对这些向量进行归一化处理，使它们与文档向量的格式一致。</li>
</ol>

<h3 id="3-延迟交互和相似度计算">3. 延迟交互和相似度计算</h3>

<p>接下来，ColBERT通过延迟交互和相似度计算来找到最相关的论文。</p>

<ol>
  <li><strong>加载文档嵌入</strong>：从数据库中加载所有预先计算好的文档向量表示。</li>
  <li><strong>最大相似度计算</strong>：对于查询中的每个单词向量，找到与文档中所有单词向量之间的最大相似度。这就像是找到最匹配的拼图块。</li>
  <li><strong>相似度求和</strong>：将每个查询单词与文档单词的最大相似度值相加，得到一个总的相似度分数。这个分数表示了该文档与查询的相关性。</li>
</ol>

<h3 id="4-文档排序和检索">4. 文档排序和检索</h3>

<p>最后，根据相似度分数对文档进行排序，并返回得分最高的前k个文档。</p>

<ol>
  <li><strong>文档排序</strong>：将所有候选文档按相似度得分进行排序，类似于将考试成绩从高到低排序。</li>
  <li><strong>返回结果</strong>：返回得分最高的前k个文档，这些就是与你的查询最相关的论文。</li>
</ol>

<h3 id="生动形象的示例">生动形象的示例</h3>

<p>想象你在图书馆里寻找“机器学习的好处”相关的书籍。图书馆员（ColBERT）事先已经对所有书籍进行了详细分类和标注（文档预处理和编码）。当你提出查询时，图书馆员快速浏览了每本书的内容（查询编码和相似度计算），找到了最相关的书籍并排列出来（文档排序和检索），最终将最相关的几本书递给你。这一切发生得非常迅速，因为图书馆员已经事先做好了大量准备工作。</p>

<p>通过这种方式，ColBERT既能保证高效处理大量数据，又能在查询时提供快速响应和高质量的结果。</p>

<p><a href="https://arxiv.org/abs/2004.12832">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</a></p>

<h2 id="colbert-v2">ColBERT v2</h2>

<p><a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a></p>

<h3 id="colbertv2-effective-and-efficient-retrieval-via-lightweight-late-interaction的主要改进和优化"><a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>的主要改进和优化</h3>

<ol>
  <li><strong>残差压缩机制</strong>：这是<a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>的一个主要创新点。通过将每个嵌入向量编码为其最近质心的索引和量化残差，ColBERTv2大大减少了存储需求。这个改进在不牺牲模型质量的情况下显著降低了存储成本。</li>
  <li><strong>降噪监督</strong>：<a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>引入了一种新的监督策略，包括交叉编码器蒸馏和困难负样本挖掘。这种方法选择具有挑战性的负样本，避免奖励假阳性或惩罚假阴性，从而提高了训练效果和模型质量。</li>
  <li><strong>高效索引和检索</strong>：
    <ul>
      <li><strong>质心选择</strong>：在索引阶段，<a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>通过质心选择优化了段落的表示。</li>
      <li><strong>段落编码</strong>：通过调用BERT编码器并压缩输出嵌入，将每个嵌入分配到最近的质心并计算量化残差。</li>
      <li><strong>索引倒置</strong>：为了支持快速最近邻搜索，<a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a>将对应于每个质心的嵌入ID分组并保存倒排列表，这样在检索时可以快速找到相似的标记级嵌入。</li>
    </ul>
  </li>
  <li><strong>检索过程优化</strong>：
    <ul>
      <li><strong>候选生成</strong>：对于查询中的每个向量，找到最近的质心，并使用倒排列表识别接近这些质心的段落嵌入，解压缩它们，并计算它们与查询向量的余弦相似度。</li>
      <li><strong>评分和排序</strong>：将分数按段落ID分组，并对同一段落的分数进行最大化减少，最终进行排序并返回结果。</li>
    </ul>
  </li>
</ol>

<h3 id="colbertv2-effective-and-efficient-retrieval-via-lightweight-late-interaction-的全流程示例"><a href="https://arxiv.org/pdf/2112.01488">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction</a> 的全流程示例</h3>

<h3 id="背景-1">背景</h3>

<p>假设我们有一个学术论文数据库，包含数百万篇论文。您是一名研究者，正在研究“机器学习的好处”，并希望找到最相关的论文。</p>

<h3 id="1-离线预处理和编码文档-1">1. 离线预处理和编码文档</h3>

<p><strong>步骤1：质心选择</strong></p>

<ol>
  <li><strong>采样</strong>：从数百万篇论文中随机抽取一个样本，大小为语料库规模的平方根。</li>
  <li><strong>聚类</strong>：对采样的论文片段进行k-means聚类，生成一组质心C。这些质心用于支持后续的残差编码和最近邻搜索。</li>
</ol>

<p><strong>步骤2：段落编码</strong></p>

<ol>
  <li><strong>BERT编码</strong>：对于每篇论文，用BERT模型对其每个单词进行编码，生成一个嵌入向量。例如，一篇论文的片段“Machine learning is a method of data analysis”会被编码成多个嵌入向量。</li>
  <li><strong>残差计算</strong>：对于每个嵌入向量，找到其最近的质心Ct，并计算量化残差r = v - Ct。</li>
  <li><strong>存储</strong>：将质心索引和量化残差一起存储到磁盘上。例如，段落中的一个嵌入向量可能被表示为最近质心的索引和一个小的量化残差。</li>
</ol>

<p><strong>步骤3：索引倒置</strong></p>

<ol>
  <li><strong>倒排列表</strong>：将所有质心的索引分组，并保存倒排列表，以支持快速的最近邻搜索。这类似于将书籍按分类编号存放在书架上，以便快速找到。</li>
</ol>

<h3 id="2-在线查询处理和检索">2. 在线查询处理和检索</h3>

<p><strong>步骤1：查询编码</strong></p>

<ol>
  <li><strong>BERT编码</strong>：当您输入查询“机器学习的好处”时，查询被分割成多个单词，并用BERT模型编码成嵌入向量。</li>
  <li><strong>残差计算</strong>：对于每个查询嵌入向量，找到最近的质心Ct，并计算量化残差r = v - Ct。</li>
</ol>

<p><strong>步骤2：候选生成</strong></p>

<ol>
  <li><strong>最近质心查找</strong>：对于查询中的每个向量，找到最近的质心，并使用倒排列表识别接近这些质心的段落嵌入。</li>
  <li><strong>解压缩</strong>：解压缩识别出的段落嵌入，并计算它们与查询向量的余弦相似度。</li>
  <li><strong>评分和最大化减少</strong>：将分数按段落ID分组，并对同一段落的分数进行最大化减少。这类似于在拼图中找到最匹配的片段。</li>
</ol>

<p><strong>步骤3：排序和返回结果</strong></p>

<ol>
  <li><strong>合并和排序</strong>：将所有分数相加，并按得分对段落进行排序。选择得分最高的前k个段落进行进一步评分。</li>
  <li><strong>最终评分</strong>：加载每个段落的完整嵌入集，并对每个段落进行最终评分。</li>
  <li><strong>返回结果</strong>：根据得分返回最相关的论文。这类似于从书架上找到最相关的几本书，并递给您。</li>
</ol>

<h3 id="示例">示例</h3>

<p>想象一下您在图书馆寻找“机器学习的好处”相关的书籍。图书馆员（ColBERTv2）事先已经对所有书籍进行了详细分类和标注，并将每本书的主要内容进行了数字化处理和压缩。当您提出查询时，图书馆员会迅速浏览每本书的数字化内容，找到最匹配的片段，将这些片段组合评分，最终将最相关的几本书递给您。这一切发生得非常迅速，因为图书馆员事先已经做好了大量准备工作。</p>

<h2 id="dpr">DPR</h2>

<p><a href="https://arxiv.org/pdf/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a></p>

<h3 id="示例场景">示例场景</h3>

<p>假设用户向系统提出了一个问题：“什么是光合作用？”我们将通过以下步骤使用 DPR 检索相关信息：</p>

<h3 id="步骤-1-查询编码">步骤 1: 查询编码</h3>

<ul>
  <li><strong>输入</strong>：用户的问题 “什么是光合作用？”</li>
  <li><strong>处理</strong>：首先，这个问题被送入一个预训练的 Transformer 模型（如BERT）。这个模型将文本转换成一个高维向量（通常是 768 维或更多，依赖于模型架构）。</li>
  <li><strong>输出</strong>：问题的密集向量表示。</li>
</ul>

<h3 id="步骤-2-文档库编码">步骤 2: 文档库编码</h3>

<ul>
  <li><strong>预处理</strong>：在这一步骤之前，系统已经预先将可能的回答或信息源（如维基百科条目、教科书段落等）编码成向量，并存储在向量数据库中。</li>
  <li><strong>数据库</strong>：包含了大量文档的向量表示，这些都是事先处理好的。</li>
</ul>

<h3 id="步骤-3-向量相似度计算">步骤 3: 向量相似度计算</h3>

<ul>
  <li><strong>比较</strong>：系统现在将查询向量与文档库中的每一个文档向量进行比较。比较通常采用余弦相似度</li>
  <li><strong>排名</strong>：基于相似度得分，所有文档按得分从高到低排序。</li>
</ul>

<h3 id="步骤-4-选择顶部文档">步骤 4: 选择顶部文档</h3>

<ul>
  <li><strong>选择</strong>：系统通常选择相似度得分最高的前 N 个文档（例如，前 5 或 10 个），认为这些文档与查询最相关。</li>
  <li><strong>输出</strong>：这些顶部文档的文本内容被送到生成模型，用于下一步的答案生成。</li>
</ul>

<h3 id="步骤-5-答案生成">步骤 5: 答案生成</h3>

<ul>
  <li><strong>生成模型输入</strong>：选定的文档内容作为上下文输入到一个生成模型（如 GPT）中。</li>
  <li><strong>生成答案</strong>：生成模型综合考虑这些文本信息，生成一个信息丰富且相关的答案。</li>
</ul>

<h3 id="步骤-6-输出最终答案">步骤 6: 输出最终答案</h3>

<ul>
  <li><strong>用户接收</strong>：系统输出的答案展示给用户，例如：”光合作用是植物、藻类和某些细菌使用阳光将水和二氧化碳转化为氧气和葡萄糖的过程。”</li>
</ul>

<p>这个例子展示了 DPR 在 RAG 系统中如何精确地从大量信息中检索相关内容，并辅助生成模型以提供准确且有用的回答。</p>

<p><a href="https://arxiv.org/abs/2405.01585">Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications</a></p>

<blockquote>
  <p>No code</p>
</blockquote>

<h2 id="raft">RAFT</h2>

<p><a href="https://arxiv.org/abs/2403.10131">RAFT: Adapting Language Model to Domain Specific RAG</a></p>

<p>本文提出了一种称为<strong>Retrieval Augmented Fine Tuning (RAFT)</strong> 的方法，旨在增强预训练语言模型在特定领域中的检索增强生成（RAG）能力，特别是在“开放书本”设置下。该方法通过将微调与RAG结合，以提升模型在领域特定问答任务中的表现。</p>

<p><strong>数据准备</strong>：RAFT方法为每个问题准备了一组文档，其中包含包含答案的“oracle”文档和不相关的干扰文档。对于包含正确文档的问题，生成链式思维（Chain-of-Thought, CoT）风格的答案，答案中直接引用了文档的相关片段，以减少生成过程中的幻觉问题（hallucination）。这种数据结构旨在训练模型更好地识别和利用相关信息。</p>

<p><strong>训练策略</strong>：在训练过程中，模型被微调以应对包含干扰文档的情境。模型被迫在这些干扰文档的干扰下，准确提取有用的信息并生成答案。此外，训练数据中部分问题仅包含干扰文档，以促使模型依赖已学得的领域知识进行回答。这种策略不仅强化了模型在特定领域内的知识，还提高了其在有噪声背景下作答的能力。</p>

<p><strong>微调与RAG的结合</strong>：RAFT通过微调模型，使其在开放书本的设置中更为有效地工作。在此过程中，模型学会了在处理特定领域的文档时，如何忽略无关信息并准确引用相关文档内容来生成答案。与传统的RAG方法不同，RAFT专注于特定领域的应用，进一步提高了模型的检索和生成能力。</p>

<p>实验结果表明，RAFT在多个数据集（如PubMed、HotpotQA、Gorilla API Bench）上显著优于其他基线模型，证明了其在领域特定问答任务中的强大潜力。本文为实现高效的领域特定问答任务提供了一种有效的训练策略，展示了微调与RAG相结合在提升模型性能方面的优势。</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Lu, W., Zhang, J., Zhang, J. and Chen, Y., 2024. Large language model for table processing: A survey. arXiv preprint arXiv:2402.05121.</li>
  <li>Fang, X., Xu, W., Tan, F.A., Zhang, J., Hu, Z., Qi, Y., Nickleach, S., Socolinsky, D., Sengamedu, S. and Faloutsos, C., 2024. Large Language Models on Tabular Data–A Survey. arXiv preprint arXiv:2402.17944.</li>
  <li>Zhao, Y., Long, Y., Liu, H., Nan, L., Chen, L., Kamoi, R., Liu, Y., Tang, X., Zhang, R. and Cohan, A., 2023. Docmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data. arXiv preprint arXiv:2311.09805.</li>
  <li>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023. Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. arXiv preprint arXiv:2312.09039.</li>
  <li>Dong, X., Zhang, C., Ge, Y., Mao, Y., Gao, Y., Lin, J. and Lou, D., 2023. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306.</li>
  <li>Sundar, A.S. and Heck, L., 2023. cTBLS: Augmenting large language models with conversational tables. arXiv preprint arXiv:2303.12024.</li>
  <li>Gao, D., Wang, H., Li, Y., Sun, X., Qian, Y., Ding, B. and Zhou, J., 2023. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363.</li>
  <li>Lin, W., Blloshmi, R., Byrne, B., de Gispert, A. and Iglesias, G., 2023, July. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 1557-1566).</li>
  <li>Khattab, O. and Zaharia, M., 2020, July. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval (pp. 39-48).</li>
  <li>Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C. and Zaharia, M., 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488.</li>
  <li>Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</li>
  <li>Khanna, S. and Subedi, S., 2024. Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications. arXiv preprint arXiv:2405.01585.</li>
  <li>Zhang, T., Patil, S.G., Jain, N., Shen, S., Zaharia, M., Stoica, I. and Gonzalez, J.E., 2024. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131.</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[随着大型语言模型（LLM）在自然语言处理（NLP）领域的广泛应用，它们在各种任务中展现了显著的能力，包括文本生成、问答系统和情感分析。然而，尽管LLM在处理非结构化数据方面表现优异，它们在处理结构化数据，特别是表格数据时，仍面临诸多挑战。表格数据的结构化特性和丰富的语义信息对LLM提出了更高的要求，传统的文本处理方法往往无法直接适用于此类数据。]]></summary></entry></feed>