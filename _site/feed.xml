<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-15T18:20:31+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pei Ma</title><subtitle>A blog about Large Language Model, RAG, and fine-tuning.</subtitle><entry><title type="html">TableRAG(2)</title><link href="http://localhost:4000/TableRAG-RAG-for-Table-Data-CN/" rel="alternate" type="text/html" title="TableRAG(2)" /><published>2024-08-15T14:30:00+01:00</published><updated>2024-08-15T14:30:00+01:00</updated><id>http://localhost:4000/TableRAG-RAG-for-Table-Data-CN</id><content type="html" xml:base="http://localhost:4000/TableRAG-RAG-for-Table-Data-CN/"><![CDATA[<h1 id="背景">背景</h1>
<p>表格作为一种基础且广泛应用的半结构化数据类型，广泛存在于关系数据库、电子表格应用程序和用于数据处理的编程语言中，涵盖了金融分析（Zhang et al., 2020; Li et al., 2022）、风险管理（Babaev et al., 2019）和医疗保健分析等多个领域。在这些应用中，表格问答（TableQA）是对表格数据进行推理的一个关键下游任务（Ye et al., 2023a; Cheng et al., 2023）。</p>

<p>表格问答的目标是使计算机能够理解人类针对表格内容的查询，并以自然语言作出回答。随着近年来大规模语言模型（LLMs）的快速发展，表格问答已成为一个重要的子领域，并取得了显著的进展（Ray, 2023）。目前，大多数利用LLM进行表格问答的研究都是基于单个表格的（Li et al., 2023）。这些方法通常通过将表格预处理后，将问题和表格逐个输入LLM，侧重于让LLM更好地理解表格结构。这类方法在实际应用中主要集中于金融领域，如金融表格问答、金融审计表格处理(Zhu et al., 2021)和金融数值推理等(Chen et al., 2021, Chen et al., 2020)。然而，在现实场景中，往往面临的是一组表格（a set of tables）而非单个表格，用户可能会提出涉及多个表格的任意相关问题。在这种情况下，LLM不仅需要逐个输入回答，更重要的是能够从大量表格中召回相关表格并给出答案。然而，目前在这方面的研究还相对欠缺，我们的研究旨在弥补这一差距。</p>

<p>微调大规模语言模型是解决表格问答挑战的常见方法，但这种方法需要大量的领域特定的标注数据和巨大的计算资源。此外，大多数模型在处理领域特定和复杂的表格数据时，往往过度依赖预训练知识，从而导致幻觉和错误信息（Ray, 2023; Gao et al., 2023）。</p>

<p>为了解决这些挑战，检索增强生成（RAG）方法将检索机制与生成模型相结合，引用外部知识库，以减少模型幻觉并提高领域特定问答的准确性，同时降低资源消耗（Gao et al., 2023）。然而，尽管RAG在处理非结构化文本数据方面表现出色，但在应用于半结构化表格数据时仍存在若干挑战。具体而言，我们识别了以下三个局限性：</p>

<ol>
  <li>为回答问题所需的表格可能非常庞大，包含大量与查询无关的噪声（Lu et al., 2024）。这不仅增加了不必要的计算，还会影响检索器检索时召回的准确性以及生成器响应的准确性。为了解决这个问题，我们可以采用表格采样（Sui et al., 2024）或表格过滤的方法，检索相关的行和列，从而生成最相关的子表（Jiang et al., 2023）。</li>
  <li>表格的原始内容可能包含需要进一步澄清的信息，如领域特定术语或缩写（Sui et al., 2024）。这些领域特定的细节可能导致生成器的误解或偏见。为了解决这个问题，我们可以利用外部知识库为表格提供额外的上下文信息（Bian et al., 2023），或通过LLM生成术语解释，这一过程我们称之为table clarifier。</li>
  <li>表格通常在不同列中包含多种类型的信息，而传统的检索方法如BM25（Robertson et al., 2009）或Dense Passage Retriever（DPR）（Karpukhin, et al., ）可能会忽略表格细节，影响生成结果。我们可以通过采用ColBERT模型作为检索器来解决这一问题，该模型在标记级别对文本进行编码，使得检索更加细粒度（Li et al., 2023）。</li>
</ol>

<p>通过结合这些改进，我们的研究旨在为处理多个表格的大规模表格问答任务提供一个更有效的解决方案，以应对更复杂的现实场景。</p>
<h1 id="overview">Overview</h1>
<p>在处理复杂表格问答任务时，我们设计了一个结合最新大规模语言模型（LLM）与检索增强生成（RAG）技术的系统，以应对实际应用中的多表格问题。以下是项目核心思想的图示与介绍。</p>

<h3 id="基于rag的多表格问答系统架构">基于RAG的多表格问答系统架构</h3>

<p><img src="/insert_images/The_overall_structure.png" alt="The overall structure" /></p>

<p>在这个系统架构中，我们的目标是从多个表格中检索相关信息，并生成准确的自然语言答案。流程可以分为以下几个关键步骤：</p>

<ol>
  <li><strong>表格处理与文本切分</strong>：首先，原始表格数据经过预处理和文本切分，将表格内容转换为多个文本片段。这样做的目的是使得数据更易于处理，并能够针对查询进行高效的检索。</li>
  <li><strong>向量数据库的构建</strong>：切分后的文本和表格片段经过嵌入处理并存储在向量数据库中。向量数据库通过高效的向量化检索技术，可以迅速找到与查询相关的内容片段。</li>
  <li><strong>查询与检索</strong>：当用户提出问题时，检索器会从向量数据库中查找与问题相关的表格片段。在这个过程中，我们引入了ColBERT模型来增强检索器的精度。ColBERT通过在标记级别编码文本，能够实现更细粒度的检索，从而提高检索结果的相关性。</li>
  <li><strong>生成答案</strong>：检索到的相关文本片段与用户的提问一起输入到大规模语言模型（LLM）中，由LLM生成最终的自然语言答案。</li>
</ol>

<h3 id="多表格问答的增强机制">多表格问答的增强机制</h3>

<p><img src="/insert_images/Enhancement.png" alt="Enhancement" /></p>

<p>在处理来自多张表格的数据时，我们的系统引入了多种增强机制，以提高问答任务的精确性和有效性。</p>

<ol>
  <li>
    <p><strong>基于语义的表格过滤器</strong>：当面对大量表格时，系统首先通过语义分析对表格进行过滤，选择最相关的表格。在此过程中，我们采用了以下两种不同的模型进行文本嵌入，并进行了对比：</p>

    <p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

    <ul>
      <li><strong>利用OpenAI的Embedding模型</strong>：我们使用OpenAI的Embedding模型对表格内容进行嵌入处理，然后利用FAISS向量数据库对嵌入后的数据进行存储和检索，从中返回与查询最相关的表格行和列。</li>
      <li><strong>利用ColBERT模型</strong>：我们也使用ColBERT模型对表格内容进行嵌入，并在检索过程中使用ColBERT进行更细粒度的检索。通过与OpenAI Embedding模型的结果进行对比，我们能够选择更适合特定任务的语义过滤方法。</li>
    </ul>
  </li>
  <li><strong>基于LLM的过滤器</strong>：除了语义过滤器，我们还使用大规模语言模型（LLM）对表格进行智能过滤。通过分析表格内容与查询之间的深层语义关联，LLM能够更精准地选择出最相关的表格片段，进一步提高检索的准确性。</li>
  <li>
    <p><strong>表格澄清器</strong>：在过滤后的表格基础上，我们引入了两个澄清模块：</p>

    <p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

    <ul>
      <li><strong>术语澄清</strong>：对于表格中的领域特定术语或缩写，我们调用LLM进行解释，帮助LLM更好地理解问题和表格内容。</li>
      <li><strong>基于Wiki的摘要生成</strong>：首先，我们通过表格标题、表头或上下文信息，搜索维基百科并返回相关的元数据。接着，将这些维基数据与表格的原始上下文信息打包处理，生成与需要判断的问题或澄清的陈述相关的摘要。这种方式不仅提高了信息的准确性，还为复杂表格的理解提供了更全面的背景支持。</li>
    </ul>
  </li>
</ol>

<p>上述架构与增强机制有效地应对了当前表格问答任务中存在的挑战，特别是在多表格环境下的实际应用。通过结合先进的检索技术、语义与LLM过滤，以及大规模语言模型，我们的系统能够从大量表格中迅速找到相关信息并生成精确的答案，为各类复杂数据分析任务提供了有力的支持。</p>

<h1 id="数据集的选择">数据集的选择</h1>

<h2 id="tablefact"><a href="https://tabfact.github.io/">Tablefact</a></h2>

<p>在现有的表格问答数据集中，我们已经进行了广泛的尝试和研究。关于详细的数据集整理，请参阅我的另一篇博客：<a href="https://yuhangwuai.github.io/2024/08/14/Dataset-for-Question-Answering/">Dataset for Question Answering</a>：。通过这些经验，我们在使用数据集进行表格问答的检索增强生成时，发现主要面临以下几个问题：</p>

<ol>
  <li><strong>问题简短导致召回效果不佳</strong>：
    <ul>
      <li>许多问答数据集中的问题通常非常简短，仅由几个单词组成。这种简短的提问在相似度检索或其他密集型检索过程中，往往导致相关表格的召回效果不佳。</li>
    </ul>
  </li>
  <li><strong>问题形式单一</strong>：
    <ul>
      <li>问题通常以相似的疑问词和连词开头。例如，在SQA数据集中，”What are the schools?” 和 “What are the countries?” 这个问题尽管涉及完全不同的内容，但它们的开头 “What are the” 两是相同的。如果数据集中有近500个以 “What are the” 开头的问题，这种形式上的重复会使得相关表格的准确召回变得非常困难。</li>
    </ul>
  </li>
  <li><strong>缺乏表标题</strong>：
    <ul>
      <li>大量问答数据集不包含表标题，通常一个表格仅对应一个问题，完全不涉及检索阶段。在这种情况下，每次输入时将表格和问题直接一起输入模型。然而，当缺乏表标题时，从大量表格中精准返回相关表格的难度大大增加。</li>
    </ul>
  </li>
</ol>

<p>基于这些挑战，在我们最初的实验中，TableFact数据集是我们首选的基础数据集。TableFact的数据集专注于表格事实验证这一任务，能够有效地评估模型在推理和判断方面的能力。</p>

<p>TableFact是一个大规模的数据集，包含117,854条手动标注的声明，涉及16,573个维基百科表格。这些表格和声明之间的关系被分类为“ENTAILMENT”（蕴含）和“REFUTATION”（反驳）。该数据集首次提出在结构化数据上评估语言推理能力，涉及符号推理和语义推理的混合推理技能。这种复杂性使得TableFact成为评估深度学习模型在同时处理语义和符号推理任务时的能力的理想数据集。</p>

<table>
  <thead>
    <tr>
      <th>Channel</th>
      <th>Sentence</th>
      <th>Table</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Simple (r1)</td>
      <td>50,244</td>
      <td>9,189</td>
    </tr>
    <tr>
      <td>Complex (r2)</td>
      <td>68,031</td>
      <td>7,392</td>
    </tr>
    <tr>
      <td>Total (r1 + r2)</td>
      <td>118,275</td>
      <td>16,573</td>
    </tr>
    <tr>
      <td>Split</td>
      <td>Sentence</td>
      <td>Table</td>
    </tr>
    <tr>
      <td>Train</td>
      <td>92,283</td>
      <td>13,182</td>
    </tr>
    <tr>
      <td>Val</td>
      <td>12,792</td>
      <td>1,696</td>
    </tr>
    <tr>
      <td>Test</td>
      <td>12,779</td>
      <td>1,695</td>
    </tr>
  </tbody>
</table>

<p>该数据集的示例如下：</p>

<p><img src="/insert_images/tablefact.png" alt="Tablefact sample instances（Chen et al., 2019）" /></p>

<p>Tablefact sample instances（Chen et al., 2019）</p>

<p>TableFact数据集的主要优势在于其专注于表格事实验证这一任务，能够有效地评估模型在推理和判断方面的能力。具体任务是：给定一个表格和一个声明，要求模型判断该声明是否与表格中的信息一致。模型需要对表格内容进行深入推理，并对声明标记“True”（真实）或“False”（虚假）。</p>

<p>TableFact数据集不仅包含大量复杂的表格和声明对，覆盖多种领域和主题，能够很好地模拟现实中可能遇到的多表格问答场景。这为我们提供了一个具有挑战性的测试平台，可以帮助我们更全面地评估和优化我们的多表格问答系统。使用这个数据集的另一个重要原因是，它能够更好地控制LLM的输出，使我们能够精确评估模型的表现。</p>

<p><em>我们选择使用<a href="https://tabfact.github.io/">TableFact数据集</a>的原因如下：</em></p>

<ol>
  <li><strong>纯表格数据集</strong>：TableFact的数据主要以表格形式呈现，声明内容的相似性较低，使得在检索和召回过程中难度相对较小，有助于模型准确定位相关信息。</li>
  <li><strong>明确的分类任务</strong>：TableFact的数据集任务明确，即判断声明的真假。这种任务设置使得在生成答案时更容易控制大模型的输出，从而更准确地评估模型的推理能力。</li>
</ol>

<h2 id="feverous"><a href="https://fever.ai/dataset/feverous.html">Feverous</a></h2>

<p>在使用TableFact之后，我们选择了FEVEROUS（Fact Extraction and VERification Over Unstructured and Structured information）数据集。FEVEROUS是一个专为事实验证任务设计的大规模数据集，与TableFact不同，它不仅包含结构化表格数据，还包含非结构化文本数据。这使得FEVEROUS在检索和推理过程中更加复杂和具有挑战性。</p>

<p><img src="/insert_images/feverous.png" alt="Feverous sample instances(Aly et al., 2021)" />
Feverous sample instances(Aly et al., 2021)</p>

<p><a href="https://fever.ai/dataset/feverous.html">Feverous</a>的数据集包含超过80,000个表格和文本段落对，以及与之关联的超过120,000个事实验证问题。模型在处理FEVEROUS数据集时，除了判断声明的真假之外，还需在三个选项之间做出选择：<strong>Supported</strong>（支持）、<strong>Refuted</strong>（反驳）、或<strong>Not Enough Information</strong>（信息不足）。这种三选一的任务设置进一步增加了模型的推理复杂度，与TableFact的二元分类任务相比，FEVEROUS能够更全面地评估模型的推理能力，尤其是在多源信息整合和判断中的表现。</p>

<p><em>选择<a href="https://fever.ai/dataset/feverous.html">Feverous</a>的原因</em>：</p>

<ul>
  <li>结合结构化和非结构化数据，增加了模型的推理难度。</li>
  <li>三选一任务设置，能够更好地评估模型在复杂推理任务中的表现。</li>
</ul>

<h2 id="sqa"><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></h2>

<p>在进一步扩展实验时，我们引入了<a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA（Sequential Question Answering）</a>数据集。SQA数据集的设计旨在评估模型在复杂、多步骤问答场景中的表现。这一数据集包含超过6,000个对话式问答对，每个对话涉及多个相关联的问题，这些问题通常与先前的问答上下文相关联。与TableFact和FEVEROUS不同，SQA要求模型在一个连续的问答过程中保持上下文的理解和一致性。</p>

<p>SQA中的问题不仅需要回答当前的问题，还需要基于之前的问答进行推理。更重要的是，SQA要求模型给出的答案是自由的，涵盖文本、数字等多种形式。这种开放式的问答增加了模型推理的复杂性，也考验了模型在处理自由回答时的生成能力。</p>

<p><img src="/insert_images/sqa.png" alt="SQA sample instances (Lyyer et al., 2017)" /></p>

<p>SQA sample instances (Lyyer et al., 2017)</p>

<p><em>选择<a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a>的原因</em>：</p>

<ul>
  <li>纯结构化数据，暂时不涉及两个类型数据的整合</li>
  <li>专注于多步骤问答，增加了模型在处理对话上下文和连续推理时的挑战。</li>
  <li>自由回答形式的引入，考验了模型在开放式问答任务中的表现。</li>
</ul>

<h2 id="hybridqa"><a href="https://hybridqa.github.io/">HybridQA</a></h2>

<p>最后，我们选择了HybridQA数据集，以进一步提升对模型多模态信息处理能力的评估。<a href="https://hybridqa.github.io/">HybridQA</a>是一个融合了表格和文本信息的数据集，旨在测试模型在多模态信息上的综合问答能力。该数据集包含6,241个问答对，每个问题涉及多个不同信息源的内容，包括表格和关联的非结构化文本信息。</p>

<p>HybridQA的独特之处在于，模型不仅需要从多个信息源中提取和整合相关信息，还需要在回答过程中涉及数值推理的步骤。这种多模态、多步骤的问答形式要求模型在复杂任务中表现出色，尤其是在跨模态信息整合和数值推理方面。</p>

<p><img src="/insert_images/hybridqa.png" alt="HybridQA sample instances (Chen et al., 2020)" />
HybridQA sample instances (Chen et al., 2020)</p>

<p><em>选择<a href="https://hybridqa.github.io/">HybridQA</a>的原因</em>：</p>

<ul>
  <li>涉及表格和文本的两个类型信息，进一步测试模型的跨模态整合能力。</li>
  <li>复杂的问答形式和数值推理步骤，提供了更高的挑战性，用以评估模型在处理多源信息时的综合表现。</li>
  <li>自由回答形式的引入，考验了模型在开放式问答任务中的表现。</li>
</ul>

<h1 id="实施方案">实施方案</h1>

<h2 id="第一部分表格过滤器">第一部分：表格过滤器</h2>

<p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

<ol>
  <li><strong>基于语义的过滤</strong>
    <ul>
      <li><strong>生成嵌入向量</strong>：为表格中的每一行和列生成语义嵌入向量，并为用户的查询生成相应的嵌入向量。我们采用两种方法来实现这一过程：
        <ol>
          <li><strong>向量数据库匹配</strong>：使用OpenAI或其他嵌入模型生成嵌入向量，然后通过FAISS等向量数据库计算相似度，快速返回与查询相关的行列。</li>
          <li><strong>细粒度匹配</strong>：使用ColBERT预训练模型对表格数据和查询进行嵌入和匹配，以实现更高的细粒度匹配，从而选择最相关的行列。</li>
        </ol>
      </li>
      <li><strong>选择相关行列</strong>：根据相似度得分，选取与查询最相关的前k行和前k列，构建新的子表格。</li>
    </ul>
  </li>
  <li><strong>基于大型语言模型（LLM）的过滤</strong>
    <ul>
      <li><strong>转换为字符串</strong>：将查询和表格内容转化为字符串并拼接，形成上下文。</li>
      <li><strong>调用GPT过滤</strong>：使用GPT模型过滤并提取与查询相关的行列，同时生成相应的Python代码以实现筛选。为了提高代码生成的准确性和一致性，采用了自一致性策略：
        <ol>
          <li><strong>自一致性策略</strong>：让GPT生成5次代码，选择出现频率最高的代码作为最终筛选代码。如果生成的代码版本各不相同，则选择第一次生成的结果。</li>
          <li><strong>执行和错误处理</strong>：执行最终选择的代码段，更新表格。如果代码执行过程中出现错误，则捕获错误信息并返回原始表格，以确保流程的鲁棒性。</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<h3 id="llm过滤过程中的难点及解决方案">LLM过滤过程中的难点及解决方案</h3>

<p>在表格过滤过程中，尤其是基于LLM的表格过滤器中，存在以下几个主要难点：</p>

<ol>
  <li>
    <p><strong>列名的一致性问题</strong>：GPT在生成筛选代码时，有时会误识别列名，导致生成的代码与原始表格中的列名不一致，从而引发错误。例如，把scheduled’, ‘capacity (mw)理解为scheduled capacity (mw)是一个列名，LLM将多个列名合并为一个，或者将单个列名错误分拆。</p>

    <p><strong>解决方案</strong>：为了解决这一问题，可以在Prompt中明确提供整理后的列名作为参数传递给GPT，以确保生成的代码使用的列名与原始表格完全一致。这种方式能够从根本上减少列名识别错误的发生。</p>
  </li>
  <li>
    <p><strong>信息丢失问题</strong>：在LLM过滤表格过程中，筛选后的表格可能会因为过度过滤而丢失回答问题所需的关键信息。这种情况会导致在后续生成回答时，由于缺乏必要的证据，生成的答案不准确甚至错误。</p>

    <p><strong>解决方案</strong>：为了解决这一问题，可以采用“保守筛选”策略，即让LLM仅过滤掉自己非常确定与陈述无关的内容。如果LLM在判断某些内容是否与陈述相关时存在不确定性，应倾向于保留这些内容。这种策略能够最大程度地保留潜在的关键证据，确保生成的回答能够基于完整的信息进行推理，从而提高答案的准确性和可信度。</p>
  </li>
  <li>
    <p><strong>数据类型不匹配导致的筛选问题</strong>：在处理表格数据时，尤其是在筛选数值类型的数据时，可能会因为数据类型不一致而导致筛选结果为空或不准确。</p>

    <p><strong>解决方案</strong>：即使是在处理数值数据时，也建议通过字符串匹配的方式进行筛选。这种做法可以避免由于数据类型不匹配引起的筛选错误，从而提高筛选的准确性和可靠性。</p>
  </li>
  <li>
    <p><strong>Prompt设计的有效性</strong>：为了让GPT能够准确理解任务并生成正确的筛选代码，Prompt的设计至关重要。一个不明确的Prompt可能导致GPT生成不符合预期的代码。</p>

    <p><strong>解决方案</strong>：在设计Prompt时，应确保其清晰、具体，并包含足够的上下文信息，以便GPT能够准确理解任务要求。同时，可以通过反复测试和调整Prompt，找到最适合的表达方式，提高代码生成的准确性。</p>
  </li>
  <li>
    <p><strong>代码生成的一致性问题</strong>：GPT在生成代码时可能会产生多个不同版本的代码，导致结果不一致。</p>

    <p><strong>解决方案</strong>：通过自一致性策略，生成多个版本的代码并选择出现频率最高的版本，确保结果的一致性和可靠性。如果所有生成的代码都不一致，则使用第一次生成的代码并进行错误捕获处理，以确保流程的稳定性。</p>
  </li>
</ol>

<p>最后我们使用的详细的设置如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">call_llm_code_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Synthesize code snippet from the table context."""</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: Synthesize code snippet from the table context to select the proper rows and columns for verifying a statement / answering query.
        The generated code must use the exact column names provided, including spaces, capitalization, and punctuation.
        The generated code should treat all data as strings, even if they look like numbers.
        Only filter out rows and columns that are definitely not needed to verify the statement / answering query.

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: The scheduled date for the farm with 17 turbines be 2012.
        Columns: ['wind farm', 'scheduled', 'capacity (mw)', 'turbines', 'type', 'location']
        df = pd.DataFrame(wind farm)
        User 2:
        To verify the statement 'The scheduled date for the farm with 17 turbines be 2012', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'wind farm', 'scheduled', and 'turbines' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['wind farm', 'scheduled', 'turbines']].query("turbines == '17'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: All 12 club play a total of 22 game for the wru division one east.
        Columns: ['club', 'played', 'drawn', 'lost', 'points for', 'points against', 'tries for', 'tries against', 'try bonus', 'losing bonus', 'points']
        df = pd.DataFrame(club)
        User 2:
        To verify the statement 'All 12 club play a total of 22 game for the wru division one east', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'club' and 'played' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['club', 'played']].query("played == '22'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: Touchdown Atlantic, in the category of sporting, be established in 2010.
        Columns: ['event name', 'established', 'category', 'sub category', 'main venue']
        df = pd.DataFrame(event name)
        User 2:
        To verify the statement 'Touchdown Atlantic, in the category of sporting, be established in 2010', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'event name' and 'established' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['event name', 'established']].query("`event name` == 'touchdown atlantic' and established == '2010'")

        Now, generate a code snippet from the table context to select the proper rows and columns to verify the given statement / answering query.
        Use the existing column names from the provided DataFrame.
        The column names in the generated code must match the provided column names exactly, including spaces, capitalization, and punctuation.
        Only filter out rows and columns that are definitely not needed to verify the statement.
        Only return the code. 
        </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">
        </span><span class="se">\n\n</span><span class="s">:
        """</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">USE_SELF_CONSISTENCY</span><span class="p">:</span>
            <span class="n">generated_codes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Generated codes:"</span><span class="p">,</span> <span class="n">generated_codes</span><span class="p">)</span>
            
            <span class="c1"># Find the most common code
</span>            <span class="n">code_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">generated_codes</span><span class="p">)</span>
            <span class="n">most_common_code</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">code_counter</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">most_common_code</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">generated_codes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="第二部分表格澄清器">第二部分：表格澄清器</h1>

<p>在处理复杂的表格数据时，提供澄清信息有助于增强对表格内容的理解。然而，选择合适的澄清方法至关重要。在最初的设计中，我们尝试使用Google API来检索术语解释，并通过Wikipedia的文档来增强表格内容。具体来说，在最初的设计中，我们采用了以下流程来对表格进行澄清处理。</p>

<h2 id="早期方法的全流程">早期方法的全流程</h2>

<h3 id="术语澄清"><strong>术语澄清</strong></h3>

<ul>
  <li>首先，使用大型语言模型（LLM）对表格中的内容进行分析，筛选出需要进一步解释的术语。</li>
  <li>对筛选出的术语，利用Google API进行搜索，以获取相关解释。</li>
  <li>然后，将检索到的解释附加到表格中，作为术语澄清信息。这个过程可以借助Langchain中的<code class="language-plaintext highlighter-rouge">GoogleSearchAPIWrapper()</code>来实现。</li>
</ul>

<h3 id="wiki文档澄清"><strong>Wiki文档澄清</strong></h3>

<ul>
  <li>根据表格的标题、上下文或表头信息，构建Wikipedia查询。例如，如果表格的表头为“Company Name”、“Revenue”、“Number of Employees”等，可以构建类似“company revenue employees market capitalization”的查询。</li>
  <li>使用Langchain中的<code class="language-plaintext highlighter-rouge">WikipediaRetriever.get_relevant_documents()</code>进行检索，获取相关的Wikipedia文档。</li>
  <li>从检索到的文档中提取元数据，如标题、摘要和链接，将其与表格内容结合，作为进一步的澄清数据。</li>
</ul>

<p>我们使用了下面的Prompt：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">You</span> <span class="n">are</span> <span class="n">an</span> <span class="n">expert</span> <span class="ow">in</span> <span class="n">data</span> <span class="n">analysis</span> <span class="ow">and</span> <span class="n">natural</span> <span class="n">language</span> <span class="n">processing</span><span class="p">.</span> <span class="n">Your</span> <span class="n">task</span> <span class="ow">is</span> <span class="n">to</span> <span class="n">help</span> <span class="n">identify</span> <span class="n">terms</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">table</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span> <span class="k">for</span> <span class="n">better</span> <span class="n">understanding</span><span class="p">.</span> <span class="n">The</span> <span class="n">table</span> <span class="n">contains</span> <span class="n">various</span> <span class="n">fields</span><span class="p">,</span> <span class="n">some</span> <span class="n">of</span> <span class="n">which</span> <span class="n">might</span> <span class="n">include</span> <span class="n">technical</span> <span class="n">jargon</span><span class="p">,</span> <span class="n">abbreviations</span><span class="p">,</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">commonly</span> <span class="n">understood</span> <span class="n">by</span> <span class="n">a</span> <span class="n">general</span> <span class="n">audience</span><span class="p">.</span>

<span class="n">Here</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">table</span><span class="p">:</span>
<span class="p">[</span><span class="n">Insert</span> <span class="n">table</span> <span class="n">here</span><span class="p">]</span>

<span class="n">Please</span> <span class="n">follow</span> <span class="n">these</span> <span class="n">steps</span><span class="p">:</span>
<span class="mf">1.</span> <span class="n">Analyze</span> <span class="n">the</span> <span class="n">content</span> <span class="n">of</span> <span class="n">each</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">table</span><span class="p">.</span>
<span class="mf">2.</span> <span class="n">Identify</span> <span class="nb">any</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="n">technical</span><span class="p">,</span> <span class="n">specialized</span><span class="p">,</span> <span class="ow">or</span> <span class="n">abbreviations</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span><span class="p">.</span>
<span class="mf">3.</span> <span class="n">Generate</span> <span class="n">a</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">these</span> <span class="n">terms</span> <span class="n">along</span> <span class="k">with</span> <span class="n">the</span> <span class="n">corresponding</span> <span class="n">cell</span> <span class="n">reference</span> <span class="p">(</span><span class="n">row</span> <span class="ow">and</span> <span class="n">column</span><span class="p">).</span>

<span class="n">Consider</span> <span class="n">the</span> <span class="n">following</span> <span class="n">when</span> <span class="n">identifying</span> <span class="n">terms</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Technical</span> <span class="n">terms</span> <span class="n">related</span> <span class="n">to</span> <span class="n">specific</span> <span class="n">industries</span> <span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span> <span class="n">finance</span><span class="p">,</span> <span class="n">healthcare</span><span class="p">,</span> <span class="n">technology</span><span class="p">).</span>
<span class="o">-</span> <span class="n">Abbreviations</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">universally</span> <span class="n">known</span><span class="p">.</span>
<span class="o">-</span> <span class="n">Jargon</span> <span class="n">that</span> <span class="n">may</span> <span class="n">be</span> <span class="n">specific</span> <span class="n">to</span> <span class="n">a</span> <span class="n">particular</span> <span class="n">field</span> <span class="ow">or</span> <span class="n">context</span><span class="p">.</span>

<span class="n">Output</span> <span class="n">the</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">need</span> <span class="n">explanation</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">following</span> <span class="nb">format</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="p">[</span><span class="n">Term</span><span class="p">]</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="n">Row</span><span class="p">,</span> <span class="n">Column</span><span class="p">]</span>

<span class="n">Example</span> <span class="n">output</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Revenue</span> <span class="p">(</span><span class="n">million</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Market</span> <span class="n">Cap</span> <span class="p">(</span><span class="n">billion</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="n">Be</span> <span class="n">thorough</span> <span class="ow">and</span> <span class="n">ensure</span> <span class="n">that</span> <span class="nb">all</span> <span class="n">potentially</span> <span class="n">confusing</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">are</span> <span class="n">included</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">list</span><span class="p">.</span>

</code></pre></div></div>

<p>然后我们将其传递给Lanchain，利用GoogleSearchAPIWrapper()实现检索，并将结果加入作为澄清信息。</p>

<p>对于Wikipedia的方法，我们具体实现如下：</p>

<p>例如，下列表格：</p>

<table>
  <thead>
    <tr>
      <th>Company Name</th>
      <th>Revenue (Million USD)</th>
      <th>Number of Employees</th>
      <th>Market Cap (Billion USD)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Company A</td>
      <td>1000</td>
      <td>5000</td>
      <td>50</td>
    </tr>
    <tr>
      <td>Company B</td>
      <td>2000</td>
      <td>10000</td>
      <td>100</td>
    </tr>
    <tr>
      <td>Company C</td>
      <td>1500</td>
      <td>7500</td>
      <td>75</td>
    </tr>
  </tbody>
</table>

<p>利用表头信息构建查询：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"company revenue employees market capitalization"
</code></pre></div></div>

<p>查询到的信息如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="s">"title"</span><span class="p">:</span> <span class="s">"List of largest technology companies by revenue"</span><span class="p">,</span>
    <span class="s">"summary"</span><span class="p">:</span> <span class="s">"This is a list of the largest technology companies in the world by revenue."</span><span class="p">,</span>
    <span class="s">"url"</span><span class="p">:</span> <span class="s">"&lt;https://en.wikipedia.org/wiki/List_of_largest_technology_companies_by_revenue&gt;"</span>
<span class="p">}</span>
</code></pre></div></div>

<p>将上述文档原数据内容与表格结合，作为澄清数据。</p>

<blockquote>
  <p>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023.
 Tap4llm: Table provider on sampling, augmenting, and packing 
semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</p>

</blockquote>

<hr />

<p>这个方法理论上可以帮助我们获取丰富的信息资源，但在实践中却暴露出了一些不可忽视的问题。</p>

<p>首先，<strong>Google API的结果准确性问题</strong>。尽管通过Google API检索术语解释在处理某些专有术语时可能较为有效，因为这些术语通常具有唯一的定义。但当面对缩写或多义词时，问题就变得复杂了。例如，“ABC”这一缩写可能对应多个不同的概念，如“美国广播公司”（American Broadcasting Company）或“活动为基础的成本核算”（Activity-Based Costing），甚至还有其他可能的解释。在这种情况下，从Google检索到的术语解释可能会存在不一致性，不仅无法达到预期的增强效果，反而可能导致信息混淆，使结果变得更加复杂和不可靠。</p>

<p>其次，<strong>检索内容的冗长性问题</strong>。Google检索到的查询内容和Wikipedia返回的文档可能过于冗长，包含大量与表格内容相关但与实际查询需求无关的信息。这些冗长的文档在进一步处理时，可能对数据管道（pipeline）的检索效果产生负面影响。目前的研究主要侧重于将每条查询分别传入LLM或预训练模型中进行处理，而我们当前的任务有所不同，这种方法可能会导致效果不佳。如果文档过长且包含过多无关信息，可能会降低模型的准确性和效率，从而影响最终的结果质量。</p>

<h1 id="表格澄清策略的改进与完善">表格澄清策略的改进与完善</h1>

<h2 id="术语澄清模块的精准优化">术语澄清模块的精准优化</h2>

<p>基于上述原因，在对大量文献的阅读和深思熟虑之后，我们对表格澄清信息提出了以下两个关键要求：</p>

<ol>
  <li>
    <p><strong>澄清信息必须提升对表格的理解能力</strong></p>

    <p>澄清信息的首要目标是帮助模型更好地理解表格内容。信息的添加应当是精准且有助于模型在处理表格时，能够更准确地把握其结构和含义，从而提高整体的理解水平。</p>
  </li>
  <li>
    <p><strong>澄清信息必须提高对表格的召回能力</strong></p>

    <p>其次，澄清信息应当有助于提高模型对表格相关内容的召回能力。这意味着在面对查询或分析任务时，模型能够更有效地提取和利用表格中的关键信息。</p>
  </li>
</ol>

<p>在提出这些要求的同时，我们实际上也明确了两个必须避免的情况：</p>

<ol>
  <li>
    <p><strong>澄清后的信息有误，影响了LLM对表格的理解能力</strong></p>

    <p>如果澄清信息存在错误，可能会导致模型对表格的误解，从而降低其对表格内容的正确解析。这不仅违背了澄清信息的初衷，还可能使模型的输出结果产生偏差。</p>
  </li>
  <li>
    <p><strong>澄清信息过长，过多冗余，影响模型对相关表格的召回能力</strong></p>

    <p>过长或冗余的信息可能会增加模型处理时的负担，干扰其对核心内容的关注，从而削弱模型在召回相关表格信息时的效率和准确性。</p>
  </li>
</ol>

<h2 id="table澄清器的改进">Table澄清器的改进</h2>

<p>基于前述对表格增强信息的要求和潜在问题的分析，我们提出了进一步的改进方案，以优化表格增强的方法。这些改进旨在确保增强信息既能提升模型的理解能力，又能提高相关信息的召回效率，从而避免常见的误解和冗余问题。</p>

<h3 id="术语澄清模块的改进"><strong>术语澄清模块的改进</strong></h3>

<p>针对术语澄清模块，我们决定直接利用LLM从表格中提取术语并进行解释，而不再依赖GoogleSearchAPIWrap进行外部检索。尽管这一方法无法获得网络上更为广泛的综合信息，但LLM已经能够理解大部分术语和缩写，并且能够结合具体情境提供解释。这样做不仅提高了对表格的理解能力，还有效避免了可能由于外部检索带来的误导信息和冗余信息的问题，确保增强信息的精准和简洁。</p>

<h3 id="wiki参考模块的改进"><strong>Wiki参考模块的改进</strong></h3>

<h3 id="1-表格用途的澄清"><strong>1. 表格用途的澄清</strong></h3>

<p>我们引入了一个新的澄清信息，即简要的说明表格的用途，是用来回答什么问题的。这种通过明确表格目的生成的方式，可以在使用ColBERT进行信息检索时，显著提高召回率。</p>

<p>通过这种方式，我们实现了增强信息对表格召回能力的提升，确保模型在面对特定查询时能更准确地提取相关数据。具体使用prompt和用例如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">generate_terms_explanation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">statement</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a statement, and the table's caption. Your task is to identify difficult to understand column names, terms, or abbreviations in the table and provide simple explanations for each. Only explain terms related to the statement.

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: The scheduled date for the farm with 17 turbines be 2012.
        Here is the table caption: Wind Farm Details in Ireland
        Here is the table:
        wind farm

        User 2:
        Explanations:
        "scheduled": "The planned date for the wind farm to be operational.",
        "turbines": "The number of wind turbines in the wind farm."

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: All 12 clubs play a total of 22 games for the WRU Division One East.
        Here is the table caption: WRU Division One East Standings
        Here is the table:
        club

        User 2:
        Explanations:
        "played": "The number of games played by the club.",
        "points for": "The total points scored by the club.",
        "points against": "The total points scored against the club."

        Now, explain the terms in the following table.

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Statement:
        </span><span class="si">{</span><span class="n">statement</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        explanations
        }}
        """</span>
        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>  
</code></pre></div></div>

<h3 id="2-wikipedia外部信息增强的优化"><strong>2. WikiPedia外部信息增强的优化</strong></h3>

<p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

<ol>
  <li><strong>初步检索</strong>：
    <ul>
      <li><strong>基于表格标题进行WikiPedia检索</strong>：首先使用表格标题作为关键词进行WikiPedia的检索，获取相关的增强信息。</li>
      <li><strong>备用检索</strong>：如果标题检索失败，则使用表头信息进行检索，以提供与表格内容相关的增强信息。</li>
    </ul>
  </li>
  <li><strong>信息打包：</strong>
    <ul>
      <li>将Wikipedia中的数据提取元数据，但是我们不直接将这些信息加入澄清内容中，以避免冗余。</li>
      <li>我们把Wikipedia的元数据，query、table（包括筛选后的表格或原始表格）以及caption，还有context(如果有context的话)一起打包，发送给LLM进行处理，让LLM根据多方面的信息生成一个表格摘要。</li>
    </ul>
  </li>
</ol>

<p>注意事项：</p>

<ul>
  <li><strong>避免直接揭示问题答案</strong>：在生成summary时，要注意引导类摘要的撰写，避免直接透露问题的答案或提供直接的解答。总结的目的是帮助LLM更好地理解和引导他们进行进一步探索，而不是直接提供解决方案，并且直接揭示答案的话，可能这个答案也有误导性。</li>
  <li><strong>聚焦相关内容</strong>：确保LLM生成的摘要仅包括与查询内容相关的信息，避免冗余或不必要的细节。这样可以保持摘要的简洁和聚焦。</li>
</ul>

<p>具体来说我们的详细实现如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">get_docs_references</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Starting get_docs_references method"</span><span class="p">)</span>

        <span class="n">retriever</span> <span class="o">=</span> <span class="n">WikipediaRetriever</span><span class="p">(</span><span class="n">lang</span><span class="o">=</span><span class="s">"en"</span><span class="p">,</span> <span class="n">load_max_docs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Use caption for document retrieval if available
</span>            <span class="k">if</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">):</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Using caption for document retrieval:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
                <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
            <span class="c1"># If caption is also not available, use table headers
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"No caption found, using header instead:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">'table'</span><span class="p">][</span><span class="s">'header'</span><span class="p">])</span>
                <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"header"</span><span class="p">]))</span>
            
            
            <span class="c1"># Extract relevant metadata from the retrieved documents
</span>            <span class="n">metadata_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s">'title'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                    <span class="s">'summary'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'summary'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                    <span class="s">'source'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'source'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="n">metadata_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>

            <span class="c1"># Print the metadata for debugging
</span>            <span class="k">print</span><span class="p">(</span><span class="s">"Retrieved metadata: "</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">)</span>

            <span class="c1"># Extract table, statement, and caption from parsed_example
</span>            <span class="n">table</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">"header"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="p">[]),</span>
                <span class="s">"rows"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"rows"</span><span class="p">,</span> <span class="p">[])</span>
            <span class="p">}</span>
            <span class="n">statement</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"query"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
            <span class="n">caption</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>

            <span class="c1"># Call the method to generate table summary using metadata
</span>            <span class="k">print</span><span class="p">(</span><span class="s">"Calling generate_table_summary with metadata"</span><span class="p">)</span>
            <span class="n">generated_summary</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">call_llm</span><span class="p">.</span><span class="n">generate_table_summary</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">statement</span><span class="p">,</span> <span class="n">caption</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Generated summary:"</span><span class="p">,</span> <span class="n">generated_summary</span><span class="p">)</span>
            
            <span class="c1"># Return the generated summary in a dictionary under the 'table_summary' key
</span>            <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="n">generated_summary</span><span class="p">}</span>
        <span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An error occurred while retrieving documents: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"Document retrieval failed"</span><span class="p">}</span>
        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An unexpected error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"An unexpected error occurred"</span><span class="p">}</span>
</code></pre></div></div>

<p>使用的具体prompt以及用例内容如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">generate_table_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""
        Generate a summary for a table that directly addresses a given query, using metadata and context.

        :param metadata_list: List of metadata from related Wikipedia documents.
        :param context: Additional context about the table.
        :param table: Dictionary representing the table's data.
        :param query: The query or statement to be addressed by the summary.
        :param caption: Caption of the table for context.
        :return: JSON string containing the generated summary.
        """</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a query, the table's caption, metadata from related Wikipedia documents, and the context of the table. 
        Your task is to generate a concise summary for the table that directly addresses the query, using the Wikipedia metadata and the context to enhance understanding. 
        Ensure the summary begins by rephrasing or summarizing the query in a way that naturally introduces the purpose of the table. 
        Do not directly reveal the answer, but guide the reader to make an informed decision based on the provided information.

        Now, generate a summary for the given table, addressing the query and using the Wikipedia metadata and the context provided for enhanced understanding. 
        Ensure the summary starts by rephrasing or summarizing the query to introduce the table's purpose and includes only content related to the query. 
        Please avoid directly revealing the answer.

        Query:
        </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Wikipedia metadata:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Context:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        summary

        """</span>

        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>

</code></pre></div></div>

<p>我们对表格增强方法进行了深入的思考和优化，通过上述方法，我们基本可以确保在处理复杂数据时，模型能够更加准确地理解和召回表格中的关键信息。通过改进术语澄清模块和Wiki参考模块，我们成功避免了外部信息可能带来的误导和冗余问题，提升了模型在不同场景下的整体性能。这些改进不仅为增强信息的质量提供了保障，也为模型在实际应用中的可靠性和效率奠定了坚实基础。</p>

<h1 id="第三部分检索过程增强">第三部分：检索过程增强</h1>

<p>在检索过程中，传统的方法如BM25、DPR（Dense Passage Retrieval）、或者直接利用向量数据库进行检索，通常被广泛应用。BM25通过统计关键词在文档中的出现频率进行检索，是一种经典且高效的文本检索方法。而DPR采用双塔模型，利用深度学习技术，将查询和文档嵌入到高维向量空间中，通过向量的近似相似度进行匹配。这两种方法在简单查询场景中表现较好，但在处理复杂、多样化的查询时，可能存在精度和效率的局限性。向量数据库检索则依赖于高效的向量相似性搜索库，如Faiss，来实现快速的相似度计算，适合大规模数据的检索需求。</p>

<p>然而，这些方法在面对复杂的查询或表格类数据时，检索精度都不够。因此，我们在TableRAG系统中最终选择使用ColBERT进行增强。这一选择不仅基于ColBERT独特的创新点和优点，还因为其在实际应用中展现出的高效性和准确性。目前，ColBERT的实现可以通过<a href="https://github.com/bclavie/RAGatouille">RAGatouille</a>轻松集成到RAG管道中，而Llamaindex也提供了对该仓库的集成，这使得其应用变得更加便捷。</p>

<h2 id="colbert-的创新与优点">ColBERT 的创新与优点</h2>

<h3 id="创新点"><strong>创新点</strong></h3>

<ol>
  <li><strong>延迟交互框架</strong>：ColBERT通过将查询和文档的编码过程分离，并在编码后再进行相似度计算，减少了在线查询时的计算量。这使得系统能够预先计算文档的表示，大大提高了计算效率。</li>
  <li><strong>最大相似度操作（MaxSim）</strong>：ColBERT采用最大相似度操作来评估查询和文档之间的相关性，每个查询嵌入与文档嵌入之间的最大余弦相似度或L2距离相加，简单高效。</li>
  <li><strong>BERT编码器共享</strong>：通过共享BERT编码器，并在输入前分别加上特殊标记（[Q]和[D]），ColBERT在节省计算资源的同时，保留了上下文理解能力。</li>
  <li><strong>文档的分段和过滤</strong>：过滤掉无关信息，如标点符号，减少计算和存储负担。</li>
  <li><strong>基于向量相似性的检索</strong>：利用向量相似性搜索库（如faiss），ColBERT能够高效地从大型文档集合中进行端到端检索。</li>
</ol>

<h3 id="优点"><strong>优点</strong></h3>

<ol>
  <li><strong>计算效率高</strong>：预计算文档表示和延迟交互机制使ColBERT在查询处理时的计算量大幅降低，速度提高了两个数量级。</li>
  <li><strong>空间利用率高</strong>：通过归一化和降维处理，ColBERT有效地减少了存储空间需求，提升了实际应用的可行性。</li>
  <li><strong>强大的扩展性</strong>：ColBERT的架构设计允许其处理大规模文档集合而不牺牲精度，尤其是在向量相似性搜索中的高效剪枝操作中表现突出。</li>
  <li><strong>端到端检索能力</strong>：ColBERT能够直接从大型文档集合中检索，提高了系统的召回率和精度。</li>
</ol>

<h3 id="colbertv2-的改进">ColBERTv2 的改进</h3>

<p>在ColBERTv2中，这些优势得到了进一步增强。特别是引入的<strong>残差压缩机制</strong>和<strong>降噪监督</strong>，显著降低了存储需求并提高了训练效果。此外，ColBERTv2通过优化索引和检索过程，实现了更高效的候选生成和段落排序，进一步提升了检索性能。</p>

<h3 id="检索过程中的实际应用">检索过程中的实际应用</h3>

<p>在我们的TableRAG系统中，ColBERT不仅用于重新排序预检索的文档集，还通过其端到端的检索能力直接提升了系统的召回率和精度。为进一步优化检索结果的质量，我们还引入了rerank机制，对初步检索到的文档集进行重新排序。这一机制帮助我们在获得初步结果后，进一步细化和提升结果的相关性和准确性。</p>

<p>具体来说，当我们使用ColBERT进行查询时，系统首先对表格中的所有文档进行预处理和编码，生成高效的向量表示。在查询过程中，ColBERT利用这些预先生成的文档向量，通过最大相似度操作快速找到最相关的文档。接下来，rerank机制对这些初步结果进行精细化排序，确保最终呈现给用户的文档是最符合查询意图的。</p>

<p>我们对这一组合策略进行了测试，结果显示，使用ColBERT结合rerank机制不仅大幅度提高了检索的准确性，还进一步优化了查询的响应时间。通过这种多层次的检索与排序方法，我们能够确保检索结果的高精度，同时避免了传统方法中高计算成本和长响应时间的问题。</p>

<p>最终，通过集成ColBERT和rerank机制到我们的TableRAG系统中，我们实现了检索过程中增强信息的有效利用。这一增强策略不仅提升了系统的计算效率和存储利用率，还通过其创新的检索和排序机制，在不牺牲精度的情况下，大幅度提高了检索速度和结果的相关性。这样，我们的系统在处理复杂表格查询时，能够快速且准确地返回最相关的信息，从而显著提升了用户体验和系统的整体性能。</p>

<h1 id="第四部分传入格式增强">第四部分：传入格式增强</h1>

<h2 id="传入给llm的表格格式优化">传入给LLM的表格格式优化</h2>

<p>在进行表格增强和检索的过程中，传入给大型语言模型（LLM）的表格格式对最终的处理效果有着至关重要的影响。已有研究探讨了不同的表格转换方法，并比较了它们对LLM问答系统性能的影响。这些方法包括Markdown格式、模板序列化、传统预训练语言模型（TPLM）方法以及使用大型语言模型（LLM）直接生成文本。研究表明，在不同的范式下，表格转换方法的表现各不相同。</p>

<p>在 <strong>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</strong> 一文中，作者比较了不同表格转换方法在混合数据集上的表现，特别是它们在LLM问答系统中的效果：</p>

<ul>
  <li><strong>Markdown格式</strong>：使用Markdown格式表示表格内容。</li>
  <li><strong>模板序列化</strong>：利用预定义模板将表格转换为文本。</li>
  <li><strong>传统预训练语言模型（TPLM）方法</strong>：使用像T5和BART这样的模型进行表格到文本任务的微调。</li>
  <li><strong>大型语言模型（LLM）方法</strong>：如使用ChatGPT等模型进行一次性文本生成。</li>
</ul>

<p>研究结论显示：</p>

<ul>
  <li>在数据特征学习与迁移（DSFT）范式中，使用语言模型（TPLM和LLM）进行表格到文本转换的方法表现最佳。</li>
  <li>在检索增强生成（RAG）范式中，Markdown格式展现了意想不到的效率，但LLM方法依然表现出色。</li>
</ul>

<blockquote>
  <p><a href="https://arxiv.org/abs/2402.12869">Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</a></p>

</blockquote>

<h2 id="传入格式优化">传入格式优化</h2>

<p>基于上述研究，我们在实验中选择了两种表格格式将其传入LLM，以进一步优化系统的性能：</p>

<ol>
  <li><strong>HTML格式</strong>：HTML格式提供了清晰的结构化表示，使得模型能够准确理解表格的层次和内容关系。这种格式适合在需要保留复杂表格结构时使用，特别是在多维表格或嵌套表格的场景中，HTML格式能有效传达表格的语义信息。</li>
  <li><strong>Markdown格式</strong>：Markdown格式因其简洁性和人类可读性，在各种文本表示任务中广泛使用。研究表明，在RAG范式中，Markdown格式不仅能有效表示表格内容，还能提高模型的处理效率。因此，我们在实验中采用Markdown格式来评估其在实际应用中的表现。</li>
</ol>

<p>通过采用这两种格式，我们希望能够最大限度地发挥LLM在表格处理任务中的潜力。HTML格式的结构化优势和Markdown格式的简洁高效性为我们提供了不同场景下的灵活选择，确保表格内容能够被LLM准确理解和高效处理，从而进一步提高表格问答系统的整体性能。</p>

<p>这种格式优化策略的实施，不仅基于现有研究的理论支持，还在我们的实验中得到了实际验证，为后续的系统开发提供了坚实的基础。我们将继续探索其他可能的格式，以进一步优化表格传入LLM的方式，确保系统在各种复杂场景下都能保持卓越的表现。</p>

<h1 id="评估实验">评估实验</h1>

<h2 id="1-对照实验">1. 对照实验</h2>

<p>对照实验的目的是评估在基础模型上逐步添加各个模块后的性能变化。具体设计如下：</p>

<ul>
  <li><strong>Baseline</strong>（基线模型）：不包含任何额外模块的原始模型，用作参考标准。</li>
  <li><strong>Filter</strong>（过滤器）：在基线模型上逐步添加不同的过滤模块。
    <ul>
      <li><strong>Semantics-based</strong>：这里进一步分为两个小部分：
        <ul>
          <li><strong>Colbert</strong>：加入 Colbert 语义相似度比较模块。</li>
          <li><strong>OpenAI Embedding Model</strong>：加入 OpenAI Embedding Model 进行语义相似度比较的模块。</li>
        </ul>
      </li>
      <li><strong>LLM-based</strong>：加入基于大型语言模型的过滤器。</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>（澄清器）：在基线模型上逐步添加不同的澄清策略。
    <ul>
      <li><strong>Term Exp.</strong>：加入术语扩展模块。</li>
      <li><strong>Table Summary</strong>：加入表格摘要模块。</li>
      <li><strong>Exp. &amp; Summary</strong>（术语扩展与表格摘要组合）：同时加入术语扩展与表格摘要模块。</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>（格式化器）：在基线模型上逐步添加不同的格式化方式。
    <ul>
      <li><strong>String</strong>：使用字符串格式化。</li>
      <li><strong>Markdown</strong>：使用 Markdown 格式化。</li>
      <li><strong>Html</strong>：使用 Html 格式化。</li>
    </ul>
  </li>
  <li><strong>Retriever</strong>（检索器）：在基线模型上测试不同的检索策略，特别是对于 Colbert 模型，还评估了是否使用 rerank 机制对结果进行重新排序的影响。
    <ul>
      <li><strong>BM25</strong>：使用 BM25 进行检索。</li>
      <li><strong>DPR</strong>：使用 DPR 进行检索。</li>
      <li><strong>Colbert</strong>：使用 Colbert 进行检索，同时评估是否使用 rerank 机制对检索结果进行重新排序。</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>（一致性）：在基线模型上测试加入一致性模块后的性能。</li>
</ul>

<h2 id="2-消融实验">2. 消融实验</h2>

<ul>
  <li><strong>Filter</strong>（过滤器）：探讨不同过滤器对模型性能的影响。
    <ul>
      <li><strong>Semantics-based</strong>（语义基础过滤器）：这里进一步分为两个小部分，分别移除使用 Colbert 和 OpenAI Embedding Model 进行语义相似度比较的模块。</li>
      <li><strong>LLM-based</strong>（基于大型语言模型的过滤器）：移除基于LLM的过滤模块。</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>（澄清器）：评估不同澄清策略对模型的贡献。
    <ul>
      <li><strong>Term Exp.</strong>（术语扩展）：移除术语扩展模块。</li>
      <li><strong>Table Summary</strong>（表格摘要）：移除表格摘要模块。</li>
      <li><strong>All Removed</strong>（全部移除）：移除所有澄清相关模块。</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>（格式化器）：测试不同格式化方式对模型的影响。
    <ul>
      <li><strong>Markdown</strong>：移除 Markdown 格式化。</li>
      <li><strong>Html</strong>：移除 Html 格式化。</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>（一致性）：测试模型在没有一致性模块时的性能表现。
    <ol>
      <li>检索器评估</li>
    </ol>
  </li>
</ul>

<p>为了评估不同检索器的召回率，对四个数据集进行了以下实验，并且对每个实验设置开启表格摘要和不开启表格摘要：</p>

<ul>
  <li><strong>BM25</strong>：传统的 TF-IDF 检索器。</li>
  <li><strong>ColBERT</strong>：
    <ul>
      <li>不使用 rerank：直接使用 ColBERT 生成的初始检索结果。</li>
      <li>使用 rerank：对初始检索结果进行重新排序。</li>
    </ul>
  </li>
  <li><strong>DPR</strong>：基于深度学习的稠密向量检索器。</li>
  <li><strong>FASSI 向量数据库</strong>：高效向量检索数据库。</li>
</ul>

<h1 id="acknowledgments">Acknowledgments</h1>
<p>I would like to express my sincere gratitude to the authors of the paper <a href="https://arxiv.org/abs/2312.09039">“Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning”</a> for providing valuable insights that influenced some of the ideas presented in this article. Additionally, I would like to thank PeiMa from the University of Leeds for her significant contributions to this project. Her expertise and support were instrumental in shaping the outcome of this work.</p>

<h3 id="copyright-notice">Copyright Notice</h3>
<p>© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from the author. For inquiries, please contact me at yuhang.wu-4 [at] postgrad.manchester.ac.uk.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Zhang, T., Li, Y., Jin, Y. and Li, J., 2020. Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment. <em>arXiv preprint arXiv:2002.08245</em>.</li>
  <li>Li, L., Wang, H., Zha, L., Huang, Q., Wu, S., Chen, G. and Zhao, J., 2023. Learning a data-driven policy network for pre-training automated feature engineering. In <em>The Eleventh International Conference on Learning Representations</em>.</li>
  <li>Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B. and Wang, W.Y., 2021. Finqa: A dataset of numerical reasoning over financial data. <em>arXiv preprint arXiv:2109.00122</em>.</li>
  <li>Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H. and Wang, W., 2020. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. <em>arXiv preprint arXiv:2004.07347</em>.</li>
  <li>Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F. and Chua, T.S., 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. <em>arXiv preprint arXiv:2105.07624</em>.</li>
  <li>Babaev, D., Savchenko, M., Tuzhilin, A. and Umerenkov, D., 2019, July. Et-rnn: Applying deep learning to credit loan applications. In <em>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> (pp. 2183-2190).</li>
  <li>Ye, Y., Hui, B., Yang, M., Li, B., Huang, F. and Li, Y., 2023, July. Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. In <em>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 174-184).</li>
  <li>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L. and Smith, N.A., 2022. Binding language models in symbolic languages.<em>arXiv preprint arXiv:2210.02875</em>.</li>
  <li>Robertson, S. and Zaragoza, H., 2009. The probabilistic relevance framework: BM25 and beyond. <em>Foundations and Trends® in Information Retrieval</em>, <em>3</em>(4), pp.333-389.</li>
  <li>Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. <em>arXiv preprint arXiv:2004.04906</em>.</li>
  <li>Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J. and Wang, H., 2023. 
Retrieval-augmented generation for large language models: A survey. <em>arXiv preprint arXiv:2312.10997</em>.</li>
  <li>Lu, W., Zhang, J., Zhang, J. and Chen, Y., 2024. Large language model for table processing: A survey. <em>arXiv preprint arXiv:2402.05121</em>.</li>
  <li>Jiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W.X. and Wen, J.R., 2023. Structgpt: A general framework for large language model to reason over structured data. <em>arXiv preprint arXiv:2305.09645</em>.</li>
  <li>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023. Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</li>
  <li>Bian, N., Han, X., Sun, L., Lin, H., Lu, Y., He, B., Jiang, S. and Dong, B., 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. <em>arXiv preprint arXiv:2303.16421</em>.</li>
  <li>Lin, W., Blloshmi, R., Byrne, B., de Gispert, A. and Iglesias, G., 2023, July. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em> (pp. 1557-1566).</li>
  <li>Li, X., Chan, S., Zhu, X., Pei, Y., Ma, Z., Liu, X. and Shah, S., 2023.  Are ChatGPT and GPT-4 general-purpose solvers for financial text analytics? A study on several typical tasks. <em>arXiv preprint arXiv:2305.05862</em>.</li>
  <li>Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X. and Wang, W.Y., 2019. Tabfact: A large-scale dataset for table-based fact verification. <em>arXiv preprint arXiv:1909.02164</em>.</li>
  <li>Aly, R., Guo, Z., Schlichtkrull, M., Thorne, J., Vlachos, A., Christodoulopoulos, C., Cocarascu, O. and Mittal, A., 2021. Feverous: Fact extraction and verification over unstructured and structured information. <em>arXiv preprint arXiv:2106.05707</em>.</li>
  <li>Iyyer, M., Yih, W.T. and Chang, M.W., 2017, July. Search-based neural structured learning for sequential question answering. In <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> (pp. 1821-1831).</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[This passage is about Table RAGx.]]></summary></entry><entry><title type="html">TableRAG(1)</title><link href="http://localhost:4000/TableRAG-RAG-for-Table-Data/" rel="alternate" type="text/html" title="TableRAG(1)" /><published>2024-08-15T14:30:00+01:00</published><updated>2024-08-15T14:30:00+01:00</updated><id>http://localhost:4000/TableRAG-RAG-for-Table-Data</id><content type="html" xml:base="http://localhost:4000/TableRAG-RAG-for-Table-Data/"><![CDATA[<h1 id="background">Background</h1>

<p>Tables, as a fundamental and widely used semi-structured data type, are prevalent in relational databases, spreadsheet applications, and programming languages used for data processing. They cover a range of fields such as financial analysis (Zhang et al., 2020; Li et al., 2022), risk management (Babaev et al., 2019), and healthcare analytics. In these applications, table-based question answering (TableQA) is a key downstream task for reasoning over tabular data (Ye et al., 2023a; Cheng et al., 2023).</p>

<p>The goal of TableQA is to enable computers to understand human queries about table contents and respond with natural language answers. With the rapid development of large-scale language models (LLMs) in recent years, TableQA has emerged as an important subfield and has made significant progress (Ray, 2023). Currently, most research leveraging LLMs for TableQA is based on single tables (Li et al., 2023). These approaches typically involve preprocessing the table, then inputting the question and the table into the LLM, focusing on improving the LLM’s understanding of table structure. Such methods are primarily applied in practical contexts within the financial sector, such as financial table question answering, financial audit table processing (Zhu et al., 2021), and financial numerical reasoning (Chen et al., 2021; Chen et al., 2020). However, in real-world scenarios, the challenge often involves a set of tables rather than a single table, where users may pose arbitrary questions related to multiple tables. In these cases, the LLM needs not only to input tables one by one but also to retrieve relevant tables from a large collection and provide answers. However, research in this area is still relatively lacking, and our research aims to fill this gap.</p>

<p>Fine-tuning large-scale language models is a common approach to address the challenges of TableQA, but this method requires large amounts of domain-specific labeled data and significant computational resources. Moreover, most models, when handling domain-specific and complex tabular data, often overly rely on pre-trained knowledge, leading to hallucinations and incorrect information (Ray, 2023; Gao et al., 2023).</p>

<p>To address these challenges, retrieval-augmented generation (RAG) methods combine retrieval mechanisms with generative models, referencing external knowledge bases to reduce model hallucination and improve the accuracy of domain-specific question answering while reducing resource consumption (Gao et al., 2023). However, despite the strong performance of RAG in handling unstructured text data, there are several challenges when applying it to semi-structured tabular data. Specifically, we identified the following three limitations:</p>

<ol>
  <li>The tables required to answer questions may be very large, containing a significant amount of noise unrelated to the query (Lu et al., 2024). This not only increases unnecessary computation but also affects the accuracy of retrieval and the generator’s response. To address this issue, we can employ table sampling (Sui et al., 2024) or table filtering methods to retrieve relevant rows and columns, thereby generating the most relevant sub-tables (Jiang et al., 2023).</li>
  <li>The raw content of the tables may include information that needs further clarification, such as domain-specific terms or abbreviations (Sui et al., 2024). These domain-specific details can lead to misunderstandings or biases by the generator. To solve this problem, we can use external knowledge bases to provide additional context for the tables (Bian et al., 2023), or generate term explanations through LLMs, a process we call the table clarifier.</li>
  <li>Tables often contain various types of information across different columns, and traditional retrieval methods such as BM25 (Robertson et al., 2009) or Dense Passage Retriever (DPR) (Karpukhin, et al., 2020) may overlook table details, impacting the generated results. We can address this issue by employing the ColBERT model as a retriever, which encodes text at the token level, enabling more fine-grained retrieval (Li et al., 2023).</li>
</ol>

<p>By incorporating these improvements, our research aims to provide a more effective solution for handling large-scale TableQA tasks involving multiple tables, addressing more complex real-world scenarios.</p>

<h1 id="overview">Overview</h1>

<p>In tackling complex TableQA tasks, we designed a system that combines the latest large-scale language models (LLMs) with retrieval-augmented generation (RAG) techniques to handle multi-table issues in practical applications. Below is a graphical illustration and introduction of the core ideas of the project.</p>

<h3 id="rag-based-multi-table-qa-system-architecture">RAG-Based Multi-Table QA System Architecture</h3>

<p><img src="/insert_images/The_overall_structure.png" alt="The overall structure" /></p>

<p>In this system architecture, our goal is to retrieve relevant information from multiple tables and generate accurate natural language answers. The process can be divided into the following key steps:</p>

<ol>
  <li><strong>Table Processing and Text Segmentation</strong>: First, the raw table data undergoes preprocessing and text segmentation, converting the table content into multiple text segments. The purpose of this is to make the data easier to handle and more efficiently retrieved for queries.</li>
  <li><strong>Vector Database Construction</strong>: The segmented text and table fragments are embedded and stored in a vector database. The vector database, through efficient vectorized retrieval techniques, can quickly find the content fragments related to the query.</li>
  <li><strong>Query and Retrieval</strong>: When a user poses a question, the retriever searches the vector database for table fragments related to the question. In this process, we introduce the ColBERT model to enhance the accuracy of the retriever. ColBERT encodes text at the token level, allowing for more fine-grained retrieval, thus improving the relevance of the retrieval results.</li>
  <li><strong>Answer Generation</strong>: The retrieved relevant text fragments and the user’s question are input into a large-scale language model (LLM), which generates the final natural language answer.</li>
</ol>

<h3 id="enhanced-mechanisms-for-multi-table-qa">Enhanced Mechanisms for Multi-Table QA</h3>

<p><img src="/insert_images/Enhancement.png" alt="Enhancement" /></p>

<p>When handling data from multiple tables, our system introduces various enhancement mechanisms to improve the accuracy and effectiveness of the QA task.</p>

<ol>
  <li>
    <p><strong>Semantic-Based Table Filter</strong>: When dealing with a large number of tables, the system first filters the tables based on semantic analysis to select the most relevant ones. In this process, we used two different models for text embedding and comparison:</p>

    <p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

    <ul>
      <li><strong>Using OpenAI’s Embedding Model</strong>: We used OpenAI’s embedding model to embed the table content, then stored and retrieved the embedded data using the FAISS vector database, returning the table rows and columns most relevant to the query.</li>
      <li><strong>Using the ColBERT Model</strong>: We also used the ColBERT model to embed the table content and performed more fine-grained retrieval during the search process. By comparing the results with those of the OpenAI Embedding model, we were able to select the semantic filtering method best suited to the specific task.</li>
    </ul>
  </li>
  <li><strong>LLM-Based Filter</strong>: In addition to the semantic filter, we also used a large-scale language model (LLM) for intelligent table filtering. By analyzing the deep semantic relationship between the table content and the query, the LLM can more precisely select the most relevant table fragments, further improving retrieval accuracy.</li>
  <li>
    <p><strong>Table Clarifier</strong>: Based on the filtered tables, we introduced two clarification modules:</p>

    <p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

    <ul>
      <li><strong>Term Clarification</strong>: For domain-specific terms or abbreviations in the table, we called the LLM for explanation, helping the LLM better understand the question and table content.</li>
      <li><strong>Wiki-Based Summary Generation</strong>: First, we search Wikipedia for metadata related to the table title, header, or context. Then, we package this Wikipedia data with the original table context information to generate a summary related to the query or clarification statement. This approach not only improves the accuracy of information but also provides more comprehensive background support for understanding complex tables.</li>
    </ul>
  </li>
</ol>

<p>The above architecture and enhancement mechanisms effectively address the challenges present in current TableQA tasks, especially in the real-world application of multi-table environments. By combining advanced retrieval technology, semantic and LLM filtering, and large-scale language models, our system can quickly find relevant information from a large number of tables and generate accurate answers, providing strong support for various complex data analysis tasks.</p>

<h1 id="dataset-selection">Dataset Selection</h1>

<h2 id="tablefact"><a href="https://tabfact.github.io/">Tablefact</a></h2>

<p>In the existing TableQA datasets, we have conducted extensive attempts and research. For detailed dataset organization, please refer to my other blog: <a href="https://yuhangwuai.github.io/2024/08/14/Dataset-for-Question-Answering/">Dataset for Question Answering</a>. Through these experiences, we found that when using datasets for retrieval-augmented generation in TableQA, we mainly face the following issues:</p>

<ol>
  <li><strong>Short Questions Lead to Poor Recall</strong>:
    <ul>
      <li>Questions in many QA datasets are typically very short, consisting of only a few words. Such short queries often lead to poor recall of relevant tables in similarity-based retrieval or other dense retrieval processes.</li>
    </ul>
  </li>
  <li><strong>Uniform Question Format</strong>:
    <ul>
      <li>Questions often begin with similar interrogative words and conjunctions. For example, in the SQA dataset, questions like “What are the schools?” and “What are the countries?” involve completely different content, but their opening “What are the” is the same. If the dataset contains nearly 500 questions beginning with “What are the,” this format repetition makes it very difficult to accurately recall relevant tables.</li>
    </ul>
  </li>
  <li><strong>Lack of Table Titles</strong>:
    <ul>
      <li>A large number of QA datasets lack table titles, and typically, one table corresponds to one question, with no retrieval phase involved. In such cases, tables and questions are directly input together into the model. However, in the absence of table titles, accurately retrieving relevant tables from a large number of tables becomes much more difficult.</li>
    </ul>
  </li>
</ol>

<p>Based on these challenges, in our initial experiments, the TableFact dataset was our primary foundational dataset. The TableFact dataset focuses on the task of table fact verification, effectively evaluating a model’s reasoning and judgment capabilities.</p>

<p>TableFact is a large-scale dataset containing 117,854 manually annotated statements related to 16,573 Wikipedia tables. The relationships between these tables and statements are classified as “ENTAILMENT” and “REFUTATION.” This dataset first proposed evaluating language reasoning ability on structured data, involving a mix of symbolic and semantic reasoning skills. This complexity makes TableFact an ideal dataset for evaluating deep learning models’ ability to handle tasks that involve both semantic and symbolic reasoning.</p>

<table>
  <thead>
    <tr>
      <th>Channel</th>
      <th>Sentence</th>
      <th>Table</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Simple (r1)</td>
      <td>50,244</td>
      <td>9,189</td>
    </tr>
    <tr>
      <td>Complex (r2)</td>
      <td>68,031</td>
      <td>7,392</td>
    </tr>
    <tr>
      <td>Total (r1 + r2)</td>
      <td>118,275</td>
      <td>16,573</td>
    </tr>
    <tr>
      <td>Split</td>
      <td>Sentence</td>
      <td>Table</td>
    </tr>
    <tr>
      <td>Train</td>
      <td>92,283</td>
      <td>13,182</td>
    </tr>
    <tr>
      <td>Val</td>
      <td>12,792</td>
      <td>1,696</td>
    </tr>
    <tr>
      <td>Test</td>
      <td>12,779</td>
      <td>1,695</td>
    </tr>
  </tbody>
</table>

<p>An example from this dataset is as follows:</p>

<p><img src="/insert_images/tablefact.png" alt="Tablefact sample instances（Chen et al., 2019）" /></p>

<p>TableFact sample instances（Chen et al., 2019）</p>

<p>The primary advantage of the TableFact dataset lies in its focus on the task of table fact verification, effectively evaluating the model’s reasoning and judgment capabilities. The specific task is: given a table and a statement, the model is required to judge whether the statement is consistent with the information in the table. The model needs to deeply reason over the table content and mark the statement as “True” or “False.”</p>

<p>The TableFact dataset not only includes a large number of complex table and statement pairs, covering a wide range of domains and topics but also effectively simulates the multi-table QA scenarios that may be encountered in real-world situations. This provides us with a challenging test platform that helps us comprehensively evaluate and optimize our multi-table QA system. Another important reason for using this dataset is that it allows us to better control the output of the LLM, enabling us to precisely evaluate the model’s performance.</p>

<p><em>The reasons we chose to use the <a href="https://tabfact.github.io/">TableFact dataset</a> are as follows:</em></p>

<ol>
  <li><strong>Pure Table Dataset</strong>: TableFact data is mainly presented in tabular form, with relatively low similarity between statements, making it relatively easier to accurately locate relevant information during retrieval.</li>
  <li><strong>Clear Classification Task</strong>: The TableFact dataset’s task is clear: judging the truthfulness of statements. This task setting makes it easier to control the output of large models during answer generation, allowing us to more accurately evaluate the model’s reasoning ability.</li>
</ol>

<h2 id="feverous"><a href="https://fever.ai/dataset/feverous.html">Feverous</a></h2>

<p>After using TableFact, we chose the FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) dataset. FEVEROUS is a large-scale dataset specifically designed for fact verification tasks, which, unlike TableFact, contains not only structured tabular data but also unstructured text data. This makes FEVEROUS more complex and challenging in terms of retrieval and reasoning.</p>

<p><img src="/insert_images/feverous.png" alt="Feverous sample instances(Aly et al., 2021)" />
Feverous sample instances(Aly et al., 2021)</p>

<p>The <a href="https://fever.ai/dataset/feverous.html">FEVEROUS</a> dataset contains over 80,000 table and text paragraph pairs, as well as more than 120,000 associated fact verification questions. When dealing with the FEVEROUS dataset, the model, in addition to judging the truthfulness of the statement, must also choose between three options: <strong>Supported</strong>, <strong>Refuted</strong>, or <strong>Not Enough Information</strong>. This three-choice task setup further increases the complexity of the model’s reasoning, making FEVEROUS a more comprehensive dataset for evaluating the model’s reasoning ability, especially in the integration and judgment of multi-source information.</p>

<p><em>Reasons for choosing <a href="https://fever.ai/dataset/feverous.html">FEVEROUS</a>:</em></p>

<ul>
  <li>Combining structured and unstructured data increases the difficulty of the model’s reasoning.</li>
  <li>The three-choice task setup allows for a better evaluation of the model’s performance in complex reasoning tasks.</li>
</ul>

<h2 id="sqa"><a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a></h2>

<p>In further expanding our experiments, we introduced the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA (Sequential Question Answering)</a> dataset. The SQA dataset is designed to evaluate the model’s performance in complex, multi-step QA scenarios. This dataset contains over 6,000 conversational QA pairs, with each conversation involving multiple related questions that are usually contextually linked to the previous Q&amp;A. Unlike TableFact and FEVEROUS, SQA requires the model to maintain contextual understanding and consistency throughout a continuous Q&amp;A process.</p>

<p>Questions in SQA not only require answering the current question but also require reasoning based on previous Q&amp;A. Moreover, SQA requires the model’s answers to be freeform, covering various formats such as text, numbers, and more. This open-ended QA increases the model’s reasoning complexity and tests the model’s generative capabilities in handling freeform answers.</p>

<p><img src="/insert_images/sqa.png" alt="SQA sample instances (Lyyer et al., 2017)" /></p>

<p>SQA sample instances (Lyyer et al., 2017)</p>

<p><em>Reasons for choosing <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA</a>:</em></p>

<ul>
  <li>Purely structured data, temporarily not involving the integration of two types of data.</li>
  <li>Focuses on multi-step QA, increasing the challenges for the model in handling conversational context and continuous reasoning.</li>
  <li>The introduction of freeform answers tests the model’s performance in open-ended QA tasks.</li>
</ul>

<h2 id="hybridqa"><a href="https://hybridqa.github.io/">HybridQA</a></h2>

<p>Finally, we chose the HybridQA dataset to further enhance the evaluation of the model’s multi-modal information processing capabilities. <a href="https://hybridqa.github.io/">HybridQA</a> is a dataset that integrates tabular and textual information, designed to test the model’s comprehensive QA capabilities over multi-modal information. The dataset contains 6,241 QA pairs, with each question involving content from multiple different information sources, including tables and associated unstructured text information.</p>

<p>The unique aspect of HybridQA is that the model not only needs to extract and integrate relevant information from multiple sources but also needs to involve numerical reasoning steps during the answering process. This multi-modal, multi-step QA format requires the model to excel in complex tasks, especially in cross-modal information integration and numerical reasoning.</p>

<p><img src="/insert_images/hybridqa.png" alt="HybridQA sample instances (Chen et al., 2020)" />
HybridQA sample instances (Chen et al., 2020)</p>

<p><em>Reasons for choosing <a href="https://hybridqa.github.io/">HybridQA</a>:</em></p>

<ul>
  <li>Involves both tabular and textual types of information, further testing the model’s cross-modal integration capabilities.</li>
  <li>Complex QA formats and numerical reasoning steps provide higher challenges to evaluate the model’s comprehensive performance in handling multi-source information.</li>
  <li>The introduction of freeform answers tests the model’s performance in open-ended QA tasks.</li>
</ul>

<h1 id="implementation-plan">Implementation Plan</h1>

<h2 id="part-1-table-filter">Part 1: Table Filter</h2>

<p><img src="/insert_images/filter_overview.png" alt="Overview of table filter" /></p>

<ol>
  <li><strong>Semantic-Based Filtering</strong>
    <ul>
      <li><strong>Generating Embedding Vectors</strong>: Generate semantic embedding vectors for each row and column in the table, as well as for the user’s query. We implemented this process in two ways:
        <ol>
          <li><strong>Vector Database Matching</strong>: Use OpenAI or other embedding models to generate embedding vectors, then calculate similarities using a vector database like FAISS, quickly returning the rows and columns related to the query.</li>
          <li><strong>Fine-Grained Matching</strong>: Use the ColBERT pre-trained model to embed and match table data and queries for more fine-grained matching, selecting the most relevant rows and columns.</li>
        </ol>
      </li>
      <li><strong>Selecting Relevant Rows and Columns</strong>: Based on similarity scores, select the top k rows and columns most relevant to the query to construct new sub-tables.</li>
    </ul>
  </li>
  <li><strong>Large Language Model (LLM) Based Filtering</strong>
    <ul>
      <li><strong>Convert to String</strong>: Convert the query and table content into strings and concatenate them to form a context.</li>
      <li><strong>Call GPT for Filtering</strong>: Use the GPT model to filter and extract rows and columns related to the query, generating the corresponding Python code for filtering. To improve the accuracy and consistency of code generation, we adopted a self-consistency strategy:
        <ol>
          <li><strong>Self-Consistency Strategy</strong>: Have GPT generate the code five times, selecting the most frequently generated code as the final filtering code. If the generated code versions are different, select the result from the first generation.</li>
          <li><strong>Execution and Error Handling</strong>: Execute the final selected code segment to update the table. If an error occurs during code execution, capture the error message and return the original table to ensure the robustness of the process.</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<h3 id="challenges-and-solutions-in-llm-based-filtering">Challenges and Solutions in LLM-Based Filtering</h3>

<p>In the process of table filtering, especially in the LLM-based table filter, the following major challenges exist:</p>

<ol>
  <li>
    <p><strong>Column Name Consistency Issue</strong>: GPT may misinterpret column names when generating filtering code, leading to inconsistencies between the generated code and the original table column names, causing errors. For example, ‘scheduled’, and ‘capacity (mw)’ may be misunderstood as ‘scheduled capacity (mw)’ as a single column name, or the LLM may merge multiple column names into one or incorrectly split a single column name.</p>

    <p><strong>Solution</strong>: To address this issue, the Prompt can explicitly provide the cleaned-up column names as parameters to be passed to GPT, ensuring that the generated code uses column names that are completely consistent with the original table. This approach can fundamentally reduce the occurrence of column name recognition errors.</p>
  </li>
  <li>
    <p><strong>Information Loss Issue</strong>: During LLM filtering, the filtered table may lose critical information needed to answer the question due to over-filtering. This can lead to inaccurate or incorrect answers being generated in subsequent answer generation due to a lack of necessary evidence.</p>

    <p><strong>Solution</strong>: To address this issue, a “conservative filtering” strategy can be adopted, where the LLM only filters out content that it is very certain is unrelated to the statement. If the LLM is uncertain whether some content is related to the statement, it should lean towards retaining this content. This strategy can maximize the retention of potential key evidence, ensuring that the generated answers can be based on complete information, thus improving the accuracy and credibility of the answers.</p>
  </li>
  <li>
    <p><strong>Data Type Mismatch Filtering Issue</strong>: When processing table data, especially when filtering numerical data, mismatches in data types may result in empty or inaccurate filtering results.</p>

    <p><strong>Solution</strong>: Even when processing numerical data, it is recommended to perform filtering using string matching. This approach can avoid filtering errors caused by data type mismatches, thereby improving filtering accuracy and reliability.</p>
  </li>
  <li>
    <p><strong>Effectiveness of Prompt Design</strong>: The design of the Prompt is crucial for ensuring that GPT accurately understands the task and generates correct filtering code. An unclear Prompt may lead to GPT generating code that does not meet expectations.</p>

    <p><strong>Solution</strong>: In designing the Prompt, it should be ensured that it is clear, specific, and contains sufficient context information so that GPT can accurately understand the task requirements. At the same time, the Prompt can be repeatedly tested and adjusted to find the most suitable expression, improving the accuracy of code generation.</p>
  </li>
  <li>
    <p><strong>Code Generation Consistency Issue</strong>: GPT may generate multiple different versions of the code during code generation, leading to inconsistent results.</p>

    <p><strong>Solution</strong>: By using the self-consistency strategy, generating multiple versions of the code and selecting the most frequently occurring version, consistency and reliability of the results can be ensured. If all generated codes are inconsistent, the first generated code can be used with error capture handling to ensure the stability of the process.</p>
  </li>
</ol>

<p>Finally, the detailed settings we used are as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">call_llm_code_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""Synthesize code snippet from the table context."""</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: Synthesize code snippet from the table context to select the proper rows and columns for verifying a statement / answering query.
        The generated code must use the exact column names provided, including spaces, capitalization, and punctuation.
        The generated code should treat all data as strings, even if they look like numbers.
        Only filter out rows and columns that are definitely not needed to verify the statement / answering query.

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: The scheduled date for the farm with 17 turbines be 2012.
        Columns: ['wind farm', 'scheduled', 'capacity (mw)', 'turbines', 'type', 'location']
        df = pd.DataFrame(wind farm)
        User 2:
        To verify the statement 'The scheduled date for the farm with 17 turbines be 2012', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'wind farm', 'scheduled', and 'turbines' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['wind farm', 'scheduled', 'turbines']].query("turbines == '17'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: All 12 club play a total of 22 game for the wru division one east.
        Columns: ['club', 'played', 'drawn', 'lost', 'points for', 'points against', 'tries for', 'tries against', 'try bonus', 'losing bonus', 'points']
        df = pd.DataFrame(club)
        User 2:
        To verify the statement 'All 12 club play a total of 22 game for the wru division one east', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'club' and 'played' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['club', 'played']].query("played == '22'")

        User 1:
        I need an expert to help me verify the statement by filtering the table to make it smaller. Statement: Touchdown Atlantic, in the category of sporting, be established in 2010.
        Columns: ['event name', 'established', 'category', 'sub category', 'main venue']
        df = pd.DataFrame(event name)
        User 2:
        To verify the statement 'Touchdown Atlantic, in the category of sporting, be established in 2010', we need to filter the rows and columns to focus on relevant information. 
        Since we are interested in the 'event name' and 'established' columns, the most impactful change will be to filter the rows and columns as follows:
        filtered_table = df[['event name', 'established']].query("`event name` == 'touchdown atlantic' and established == '2010'")

        Now, generate a code snippet from the table context to select the proper rows and columns to verify the given statement / answering query.
        Use the existing column names from the provided DataFrame.
        The column names in the generated code must match the provided column names exactly, including spaces, capitalization, and punctuation.
        Only filter out rows and columns that are definitely not needed to verify the statement.
        Only return the code. 
        </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">
        </span><span class="se">\n\n</span><span class="s">:
        """</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">USE_SELF_CONSISTENCY</span><span class="p">:</span>
            <span class="n">generated_codes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Generated codes:"</span><span class="p">,</span> <span class="n">generated_codes</span><span class="p">)</span>
            
            <span class="c1"># Find the most common code
</span>            <span class="n">code_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">generated_codes</span><span class="p">)</span>
            <span class="n">most_common_code</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">code_counter</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">most_common_code</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">generated_codes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="part-2-table-clarifier">Part 2: Table Clarifier</h1>

<p>When dealing with complex tabular data, providing clarifications can significantly enhance the understanding of the table’s content. However, selecting the appropriate clarification method is crucial. In the initial design, we attempted to use the Google API to retrieve term explanations and enhance the table content with Wikipedia documents. Specifically, in the initial design, we followed the process outlined below for clarifying the table.</p>

<h2 id="early-method-workflow">Early Method Workflow</h2>

<h3 id="term-clarification"><strong>Term Clarification</strong></h3>

<ul>
  <li>First, a large language model (LLM) analyzes the table content to identify terms that require further explanation.</li>
  <li>For the identified terms, the Google API is used to search for relevant explanations.</li>
  <li>The retrieved explanations are then appended to the table as term clarification information. This process can be implemented using the <code class="language-plaintext highlighter-rouge">GoogleSearchAPIWrapper()</code> in Langchain.</li>
</ul>

<h3 id="wiki-document-clarification"><strong>Wiki Document Clarification</strong></h3>

<ul>
  <li>Based on the table’s title, context, or header information, a Wikipedia query is constructed. For example, if the table header includes “Company Name,” “Revenue,” and “Number of Employees,” a query like “company revenue employees market capitalization” can be constructed.</li>
  <li>The <code class="language-plaintext highlighter-rouge">WikipediaRetriever.get_relevant_documents()</code> in Langchain is then used to retrieve relevant Wikipedia documents.</li>
  <li>Metadata, such as titles, summaries, and links, is extracted from the retrieved documents and combined with the table content as additional clarification data.</li>
</ul>

<p>We used the following Prompt:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">You</span> <span class="n">are</span> <span class="n">an</span> <span class="n">expert</span> <span class="ow">in</span> <span class="n">data</span> <span class="n">analysis</span> <span class="ow">and</span> <span class="n">natural</span> <span class="n">language</span> <span class="n">processing</span><span class="p">.</span> <span class="n">Your</span> <span class="n">task</span> <span class="ow">is</span> <span class="n">to</span> <span class="n">help</span> <span class="n">identify</span> <span class="n">terms</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">table</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span> <span class="k">for</span> <span class="n">better</span> <span class="n">understanding</span><span class="p">.</span> <span class="n">The</span> <span class="n">table</span> <span class="n">contains</span> <span class="n">various</span> <span class="n">fields</span><span class="p">,</span> <span class="n">some</span> <span class="n">of</span> <span class="n">which</span> <span class="n">might</span> <span class="n">include</span> <span class="n">technical</span> <span class="n">jargon</span><span class="p">,</span> <span class="n">abbreviations</span><span class="p">,</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">commonly</span> <span class="n">understood</span> <span class="n">by</span> <span class="n">a</span> <span class="n">general</span> <span class="n">audience</span><span class="p">.</span>

<span class="n">Here</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">table</span><span class="p">:</span>
<span class="p">[</span><span class="n">Insert</span> <span class="n">table</span> <span class="n">here</span><span class="p">]</span>

<span class="n">Please</span> <span class="n">follow</span> <span class="n">these</span> <span class="n">steps</span><span class="p">:</span>
<span class="mf">1.</span> <span class="n">Analyze</span> <span class="n">the</span> <span class="n">content</span> <span class="n">of</span> <span class="n">each</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">table</span><span class="p">.</span>
<span class="mf">2.</span> <span class="n">Identify</span> <span class="nb">any</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">are</span> <span class="n">technical</span><span class="p">,</span> <span class="n">specialized</span><span class="p">,</span> <span class="ow">or</span> <span class="n">abbreviations</span> <span class="n">that</span> <span class="n">may</span> <span class="n">need</span> <span class="n">further</span> <span class="n">explanation</span><span class="p">.</span>
<span class="mf">3.</span> <span class="n">Generate</span> <span class="n">a</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">these</span> <span class="n">terms</span> <span class="n">along</span> <span class="k">with</span> <span class="n">the</span> <span class="n">corresponding</span> <span class="n">cell</span> <span class="n">reference</span> <span class="p">(</span><span class="n">row</span> <span class="ow">and</span> <span class="n">column</span><span class="p">).</span>

<span class="n">Consider</span> <span class="n">the</span> <span class="n">following</span> <span class="n">when</span> <span class="n">identifying</span> <span class="n">terms</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Technical</span> <span class="n">terms</span> <span class="n">related</span> <span class="n">to</span> <span class="n">specific</span> <span class="n">industries</span> <span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span> <span class="n">finance</span><span class="p">,</span> <span class="n">healthcare</span><span class="p">,</span> <span class="n">technology</span><span class="p">).</span>
<span class="o">-</span> <span class="n">Abbreviations</span> <span class="n">that</span> <span class="n">are</span> <span class="ow">not</span> <span class="n">universally</span> <span class="n">known</span><span class="p">.</span>
<span class="o">-</span> <span class="n">Jargon</span> <span class="n">that</span> <span class="n">may</span> <span class="n">be</span> <span class="n">specific</span> <span class="n">to</span> <span class="n">a</span> <span class="n">particular</span> <span class="n">field</span> <span class="ow">or</span> <span class="n">context</span><span class="p">.</span>

<span class="n">Output</span> <span class="n">the</span> <span class="n">terms</span> <span class="n">that</span> <span class="n">need</span> <span class="n">explanation</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">following</span> <span class="nb">format</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="p">[</span><span class="n">Term</span><span class="p">]</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="n">Row</span><span class="p">,</span> <span class="n">Column</span><span class="p">]</span>

<span class="n">Example</span> <span class="n">output</span><span class="p">:</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Revenue</span> <span class="p">(</span><span class="n">million</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="o">-</span> <span class="n">Term</span><span class="p">:</span> <span class="n">Market</span> <span class="n">Cap</span> <span class="p">(</span><span class="n">billion</span> <span class="n">dollars</span><span class="p">)</span>
  <span class="n">Cell</span> <span class="n">Reference</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>

<span class="n">Be</span> <span class="n">thorough</span> <span class="ow">and</span> <span class="n">ensure</span> <span class="n">that</span> <span class="nb">all</span> <span class="n">potentially</span> <span class="n">confusing</span> <span class="ow">or</span> <span class="n">specialized</span> <span class="n">terms</span> <span class="n">are</span> <span class="n">included</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">list</span><span class="p">.</span>

</code></pre></div></div>
<p>Certainly! Here’s the translated content in academic English in Markdown format:</p>

<hr />

<p>Afterward, we pass it to Lanchain, utilizing the <code class="language-plaintext highlighter-rouge">GoogleSearchAPIWrapper()</code> to perform retrieval, and integrate the results as clarification information.</p>

<p>For the Wikipedia method, we implement it as follows:</p>

<p>For instance, consider the following table:</p>

<table>
  <thead>
    <tr>
      <th>Company Name</th>
      <th>Revenue (Million USD)</th>
      <th>Number of Employees</th>
      <th>Market Cap (Billion USD)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Company A</td>
      <td>1000</td>
      <td>5000</td>
      <td>50</td>
    </tr>
    <tr>
      <td>Company B</td>
      <td>2000</td>
      <td>10000</td>
      <td>100</td>
    </tr>
    <tr>
      <td>Company C</td>
      <td>1500</td>
      <td>7500</td>
      <td>75</td>
    </tr>
  </tbody>
</table>

<p>We construct the query based on the table headers:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"company revenue employees market capitalization"
</code></pre></div></div>

<p>The retrieved information is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
    <span class="s">"title"</span><span class="p">:</span> <span class="s">"List of largest technology companies by revenue"</span><span class="p">,</span>
    <span class="s">"summary"</span><span class="p">:</span> <span class="s">"This is a list of the largest technology companies in the world by revenue."</span><span class="p">,</span>
    <span class="s">"url"</span><span class="p">:</span> <span class="s">"&lt;https://en.wikipedia.org/wiki/List_of_largest_technology_companies_by_revenue&gt;"</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The above document metadata, combined with the table, serves as the clarification data.</p>

<blockquote>
  <p>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S., &amp; Zhang, D. (2023). Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</p>
</blockquote>

<hr />

<p>This method theoretically helps us acquire rich information resources; however, in practice, it reveals some significant issues.</p>

<p>First, the <strong>accuracy issue of the Google API results</strong>. While retrieving term explanations through the Google API might be effective for handling certain specialized terms that usually have a unique definition, the situation becomes complicated when dealing with acronyms or polysemous words. For example, the acronym “ABC” might correspond to multiple different concepts, such as “American Broadcasting Company” or “Activity-Based Costing,” among others. In such cases, the term explanations retrieved from Google may exhibit inconsistencies, not only failing to achieve the intended enhancement but potentially causing confusion and making the results more complex and unreliable.</p>

<p>Second, the <strong>verbosity issue of the retrieved content</strong>. The content retrieved from Google and the documents returned by Wikipedia may be excessively verbose, containing large amounts of information related to the table content but irrelevant to the actual query needs. These verbose documents, when further processed, might negatively impact the retrieval effectiveness of the data pipeline. Most research currently focuses on feeding each query individually into an LLM or pre-trained model for processing. However, our current task differs, and this approach might lead to suboptimal results. If the documents are too long and contain excessive irrelevant information, it could reduce the accuracy and efficiency of the model, thereby affecting the overall quality of the results.</p>

<h1 id="improving-and-refining-the-table-clarification-strategy">Improving and Refining the Table Clarification Strategy</h1>

<h2 id="precise-optimization-of-the-term-clarification-module">Precise Optimization of the Term Clarification Module</h2>

<p>Based on the above reasons, after extensive literature review and careful consideration, we propose the following two key requirements for table clarification information:</p>

<ol>
  <li>
    <p><strong>Clarification information must enhance the understanding of the table.</strong></p>

    <p>The primary goal of clarification information is to assist the model in better understanding the table content. The added information should be precise and helpful in enabling the model to more accurately grasp the structure and meaning of the table during processing, thereby improving overall comprehension.</p>
  </li>
  <li>
    <p><strong>Clarification information must improve the recall capability related to the table.</strong></p>

    <p>Secondly, clarification information should contribute to enhancing the model’s ability to recall content related to the table. This means that when faced with a query or analysis task, the model should be able to more effectively extract and utilize key information from the table.</p>
  </li>
</ol>

<p>In proposing these requirements, we also identified two situations that must be avoided:</p>

<ol>
  <li>
    <p><strong>Incorrect clarification information that impairs the LLM’s understanding of the table.</strong></p>

    <p>If the clarification information contains errors, it may lead to misinterpretation of the table by the model, thereby reducing its ability to correctly parse the table content. This not only defeats the purpose of providing clarification information but may also cause biases in the model’s output.</p>
  </li>
  <li>
    <p><strong>Excessively lengthy and redundant clarification information that hinders the model’s ability to recall relevant tables.</strong></p>

    <p>Lengthy or redundant information may increase the processing burden on the model, distracting it from the core content, and thus weakening the model’s efficiency and accuracy in recalling relevant table information.</p>
  </li>
</ol>

<h2 id="improvements-to-the-table-clarifier">Improvements to the Table Clarifier</h2>

<p>Based on the analysis of the requirements for table augmentation information and the potential issues, we propose further improvements to optimize the method of augmenting tables. These improvements aim to ensure that the augmentation information enhances both the model’s understanding and the retrieval efficiency of relevant information, thereby avoiding common pitfalls such as misunderstandings and redundancy.</p>

<h3 id="improvements-to-the-term-clarification-module"><strong>Improvements to the Term Clarification Module</strong></h3>

<p>For the term clarification module, we decided to directly utilize the LLM to extract and explain terms from the table, rather than relying on external retrieval through the GoogleSearchAPIWrapper. While this method may not obtain the broader comprehensive information available on the internet, the LLM is already capable of understanding most terms and abbreviations and can provide explanations in context. This approach not only improves the understanding of the table but also effectively avoids potential misleading information and redundancy issues arising from external retrieval, ensuring the precision and conciseness of the augmentation information.</p>

<h3 id="improvements-to-the-wiki-reference-module"><strong>Improvements to the Wiki Reference Module</strong></h3>

<h3 id="1-clarification-of-table-purpose"><strong>1. Clarification of Table Purpose</strong></h3>

<p>We introduced a new piece of clarification information, a brief explanation of the table’s purpose, i.e., what question the table is intended to answer. By generating information based on a clear statement of the table’s purpose, we can significantly improve recall rates when using ColBERT for information retrieval.</p>

<p>Through this method, we achieve an enhancement in the table’s recall ability, ensuring that the model can more accurately extract relevant data when faced with specific queries. The specific prompt and example usage are as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">generate_terms_explanation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">statement</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a statement, and the table's caption. Your task is to identify difficult to understand column names, terms, or abbreviations in the table and provide simple explanations for each. Only explain terms related to the statement.

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: The scheduled date for the farm with 17 turbines be 2012.
        Here is the table caption: Wind Farm Details in Ireland
        Here is the table:
        wind farm

        User 2:
        Explanations:
        "scheduled": "The planned date for the wind farm to be operational.",
        "turbines": "The number of wind turbines in the wind farm."

        User 1:
        I need an expert to help me explain the terms in this table. Here is the statement: All 12 clubs play a total of 22 games for the WRU Division One East.
        Here is the table caption: WRU Division One East Standings
        Here is the table:
        club

        User 2:
        Explanations:
        "played": "The number of games played by the club.",
        "points for": "The total points scored by the club.",
        "points against": "The total points scored against the club."

        Now, explain the terms in the following table.

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Statement:
        </span><span class="si">{</span><span class="n">statement</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        explanations
        }}
        """</span>
        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>  
</code></pre></div></div>
<h3 id="2-optimization-of-wikipedia-external-information-augmentation"><strong>2. Optimization of Wikipedia External Information Augmentation</strong></h3>

<p><img src="/insert_images/clarifier_overview.png" alt="image.png" /></p>

<ol>
  <li><strong>Initial Retrieval</strong>:
    <ul>
      <li><strong>Wikipedia Retrieval Based on Table Title</strong>: Initially, we use the table title as a keyword to retrieve related augmentation information from Wikipedia.</li>
      <li><strong>Alternative Retrieval</strong>: If the title-based retrieval fails, we use the table header information to conduct the search, providing augmentation information relevant to the table content.</li>
    </ul>
  </li>
  <li><strong>Information Packaging</strong>:
    <ul>
      <li>We extract metadata from the Wikipedia data, but we do not directly incorporate this information into the clarification content to avoid redundancy.</li>
      <li>Instead, we package the Wikipedia metadata, query, table (including the filtered or original table), caption, and context (if available) together, and send it to the LLM for processing. The LLM will then generate a table summary based on this multifaceted information.</li>
    </ul>
  </li>
</ol>

<h3 id="key-considerations">Key Considerations:</h3>

<ul>
  <li><strong>Avoid Directly Revealing the Answer</strong>: When generating the summary, care should be taken to craft a guiding summary that avoids directly disclosing the answer to the question or providing an outright solution. The purpose of the summary is to help the LLM better understand and guide further exploration, rather than offering a direct solution. Additionally, directly revealing the answer may result in misleading information.</li>
  <li><strong>Focus on Relevant Content</strong>: Ensure that the summary generated by the LLM includes only information relevant to the query, avoiding redundancy or unnecessary details. This helps maintain the summary’s brevity and focus.</li>
</ul>

<p>Our detailed implementation is as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_docs_references</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Starting get_docs_references method"</span><span class="p">)</span>

    <span class="n">retriever</span> <span class="o">=</span> <span class="n">WikipediaRetriever</span><span class="p">(</span><span class="n">lang</span><span class="o">=</span><span class="s">"en"</span><span class="p">,</span> <span class="n">load_max_docs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Use caption for document retrieval if available
</span>        <span class="k">if</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Using caption for document retrieval:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
            <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"caption"</span><span class="p">])</span>
        <span class="c1"># If caption is also not available, use table headers
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"No caption found, using header instead:"</span><span class="p">,</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">'table'</span><span class="p">][</span><span class="s">'header'</span><span class="p">])</span>
            <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">"table"</span><span class="p">][</span><span class="s">"header"</span><span class="p">]))</span>
        
        
        <span class="c1"># Extract relevant metadata from the retrieved documents
</span>        <span class="n">metadata_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">'title'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                <span class="s">'summary'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'summary'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">),</span>
                <span class="s">'source'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'source'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span>
            <span class="p">}</span>
            <span class="n">metadata_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>

        <span class="c1"># Print the metadata for debugging
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"Retrieved metadata: "</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">)</span>

        <span class="c1"># Extract table, statement, and caption from parsed_example
</span>        <span class="n">table</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"header"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="p">[]),</span>
            <span class="s">"rows"</span><span class="p">:</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"rows"</span><span class="p">,</span> <span class="p">[])</span>
        <span class="p">}</span>
        <span class="n">statement</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"query"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
        <span class="n">caption</span> <span class="o">=</span> <span class="n">parsed_example</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"table"</span><span class="p">,</span> <span class="p">{}).</span><span class="n">get</span><span class="p">(</span><span class="s">"caption"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>

        <span class="c1"># Call the method to generate table summary using metadata
</span>        <span class="k">print</span><span class="p">(</span><span class="s">"Calling generate_table_summary with metadata"</span><span class="p">)</span>
        <span class="n">generated_summary</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">call_llm</span><span class="p">.</span><span class="n">generate_table_summary</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">table</span><span class="p">,</span> <span class="n">statement</span><span class="p">,</span> <span class="n">caption</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Generated summary:"</span><span class="p">,</span> <span class="n">generated_summary</span><span class="p">)</span>
        
        <span class="c1"># Return the generated summary in a dictionary under the 'table_summary' key
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="n">generated_summary</span><span class="p">}</span>
    <span class="k">except</span> <span class="n">requests</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An error occurred while retrieving documents: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"Document retrieval failed"</span><span class="p">}</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"An unexpected error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s">"table_summary"</span><span class="p">:</span> <span class="s">"An unexpected error occurred"</span><span class="p">}</span>
</code></pre></div></div>

<p>The specific prompt and example content used are as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="o">@</span><span class="n">retry</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="n">wait_random_exponential</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">generate_table_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metadata_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">table</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">caption</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="s">"""
        Generate a summary for a table that directly addresses a given query, using metadata and context.

        :param metadata_list: List of metadata from related Wikipedia documents.
        :param context: Additional context about the table.
        :param table: Dictionary representing the table's data.
        :param query: The query or statement to be addressed by the summary.
        :param caption: Caption of the table for context.
        :return: JSON string containing the generated summary.
        """</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
        Example: You will be given a table, a query, the table's caption, metadata from related Wikipedia documents, and the context of the table. 
        Your task is to generate a concise summary for the table that directly addresses the query, using the Wikipedia metadata and the context to enhance understanding. 
        Ensure the summary begins by rephrasing or summarizing the query in a way that naturally introduces the purpose of the table. 
        Do not directly reveal the answer, but guide the reader to make an informed decision based on the provided information.

        Now, generate a summary for the given table, addressing the query and using the Wikipedia metadata and the context provided for enhanced understanding. 
        Ensure the summary starts by rephrasing or summarizing the query to introduce the table's purpose and includes only content related to the query. 
        Please avoid directly revealing the answer.

        Query:
        </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">

        Table caption:
        </span><span class="si">{</span><span class="n">caption</span><span class="si">}</span><span class="s">

        Table:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Wikipedia metadata:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">metadata_list</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Context:
        </span><span class="si">{</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">

        Please return the result in the following format:
        summary

        """</span>

        <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">generated_text</span>

</code></pre></div></div>

<p>We have conducted in-depth consideration and optimization of the table augmentation methods. Through the aforementioned methods, we can largely ensure that when dealing with complex data, the model can more accurately understand and recall key information from the tables. By improving the term clarification module and the Wikipedia reference module, we have successfully avoided the potential pitfalls of external information, such as misleading data and redundancy, thereby enhancing the overall performance of the model in various scenarios. These improvements not only guarantee the quality of augmented information but also lay a solid foundation for the reliability and efficiency of the model in practical applications.</p>

<h1 id="part-three-enhancing-the-retrieval-process">Part Three: Enhancing the Retrieval Process</h1>

<p>In the retrieval process, traditional methods such as BM25, DPR (Dense Passage Retrieval), or direct vector database searches are commonly used. BM25, a classical and efficient text retrieval method, ranks documents based on the frequency of keyword occurrences. On the other hand, DPR employs a dual-tower model that uses deep learning techniques to embed queries and documents into high-dimensional vector spaces, matching them based on approximate similarity. Both methods perform well in simple query scenarios, but they may have limitations in precision and efficiency when handling complex and diverse queries. Vector database retrieval, leveraging efficient vector similarity search libraries like Faiss, meets the demands of large-scale data retrieval.</p>

<p>However, these methods may lack sufficient retrieval accuracy when faced with complex queries or table-like data. Therefore, we ultimately chose to enhance the TableRAG system using ColBERT. This decision was based not only on ColBERT’s unique innovations and advantages but also on its demonstrated efficiency and accuracy in practical applications. Currently, ColBERT can be easily integrated into the RAG pipeline via <a href="https://github.com/bclavie/RAGatouille">RAGatouille</a>, and Llamaindex provides integration with this repository, making its application even more convenient.</p>

<h2 id="innovations-and-advantages-of-colbert">Innovations and Advantages of ColBERT</h2>

<h3 id="innovations"><strong>Innovations</strong></h3>

<ol>
  <li><strong>Late Interaction Framework</strong>: ColBERT reduces online query computation by decoupling the encoding of queries and documents, with similarity calculation performed after encoding. This allows for precomputing document representations, significantly improving computational efficiency.</li>
  <li><strong>MaxSim Operation</strong>: ColBERT uses the MaxSim operation to evaluate the relevance between queries and documents. It sums the maximum cosine similarity or L2 distance between each query embedding and the document embeddings, a simple yet effective approach.</li>
  <li><strong>Shared BERT Encoder</strong>: By sharing a BERT encoder and adding special tokens ([Q] and [D]) before input, ColBERT saves computational resources while retaining contextual understanding.</li>
  <li><strong>Document Segmentation and Filtering</strong>: Unrelated information, such as punctuation, is filtered out to reduce computational and storage burdens.</li>
  <li><strong>Vector Similarity-Based Retrieval</strong>: Leveraging vector similarity search libraries like Faiss, ColBERT efficiently retrieves documents from large collections end-to-end.</li>
</ol>

<h3 id="advantages"><strong>Advantages</strong></h3>

<ol>
  <li><strong>High Computational Efficiency</strong>: Precomputing document representations and the late interaction mechanism drastically reduce the computational load during query processing, improving speed by two orders of magnitude.</li>
  <li><strong>High Space Utilization</strong>: Through normalization and dimensionality reduction, ColBERT effectively reduces storage space requirements, enhancing feasibility in practical applications.</li>
  <li><strong>Strong Scalability</strong>: ColBERT’s architecture is designed to handle large document collections without sacrificing accuracy, particularly excelling in efficient pruning operations during vector similarity searches.</li>
  <li><strong>End-to-End Retrieval Capability</strong>: ColBERT can directly retrieve from large document collections, improving system recall rates and accuracy.</li>
</ol>

<h3 id="improvements-in-colbertv2">Improvements in ColBERTv2</h3>

<p>In ColBERTv2, these advantages are further enhanced. Specifically, the introduction of <strong>residual compression mechanisms</strong> and <strong>denoising supervision</strong> significantly reduces storage needs while improving training effectiveness. Additionally, ColBERTv2 optimizes the indexing and retrieval process, achieving more efficient candidate generation and passage ranking, further enhancing retrieval performance.</p>

<h3 id="practical-applications-in-the-retrieval-process">Practical Applications in the Retrieval Process</h3>

<p>In our TableRAG system, ColBERT is used not only to rerank the pre-retrieved document set but also to directly improve system recall and accuracy through its end-to-end retrieval capabilities. To further optimize the quality of retrieval results, we have introduced a rerank mechanism that reorders the initially retrieved document set. This mechanism helps refine and enhance the relevance and accuracy of the results after the initial retrieval.</p>

<p>Specifically, when using ColBERT for queries, the system first preprocesses and encodes all documents in the table, generating efficient vector representations. During the query process, ColBERT uses these pre-generated document vectors to quickly identify the most relevant documents through the MaxSim operation. Subsequently, the rerank mechanism further refines the ordering of these initial results, ensuring that the final documents presented to the user most closely align with the query intent.</p>

<p>Our tests show that using ColBERT combined with the rerank mechanism not only significantly improves retrieval accuracy but also further optimizes query response times. Through this multi-layered retrieval and ranking approach, we can ensure high-precision retrieval results while avoiding the high computational costs and long response times associated with traditional methods.</p>

<p>In conclusion, by integrating ColBERT and the rerank mechanism into our TableRAG system, we effectively utilize augmented information during the retrieval process. This enhancement strategy not only boosts the system’s computational efficiency and storage utilization but also, through its innovative retrieval and ranking mechanisms, significantly increases retrieval speed and result relevance without sacrificing accuracy. As a result, our system can quickly and accurately return the most relevant information when handling complex table queries, thereby significantly enhancing user experience and overall system performance.</p>

<h1 id="part-four-enhancing-input-formats">Part Four: Enhancing Input Formats</h1>

<h2 id="optimization-of-table-formats-passed-to-llms">Optimization of Table Formats Passed to LLMs</h2>

<p>In the process of table augmentation and retrieval, the format in which tables are passed to large language models (LLMs) is crucial to the final processing effectiveness. Existing research has explored different table conversion methods and compared their impact on the performance of LLM-based question-answering systems. These methods include Markdown format, template serialization, traditional pre-trained language model (TPLM) methods, and direct text generation using large language models (LLMs). Studies have shown that the performance of table conversion methods varies across different paradigms.</p>

<p>In the paper <strong>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</strong>, the authors compared the performance of different table conversion methods on hybrid datasets, particularly their effects on LLM-based question-answering systems:</p>

<ul>
  <li><strong>Markdown Format</strong>: Representing table content using Markdown format.</li>
  <li><strong>Template Serialization</strong>: Using predefined templates to convert tables into text.</li>
  <li><strong>Traditional Pre-trained Language Model (TPLM) Methods</strong>: Fine-tuning models such as T5 and BART for table-to-text tasks.</li>
  <li><strong>Large Language Model (LLM) Methods</strong>: Generating text in one-shot using models like ChatGPT.</li>
</ul>

<p>The study concludes that:</p>

<ul>
  <li>In the Data-Specific Feature Transfer (DSFT) paradigm, table-to-text conversion methods using language models (TPLM and LLM) performed best.</li>
  <li>In the Retrieval-Augmented Generation (RAG) paradigm, the Markdown format exhibited unexpected efficiency, though LLM methods still performed well.</li>
</ul>

<blockquote>
  <p><a href="https://arxiv.org/abs/2402.12869">Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</a></p>
</blockquote>

<h2 id="optimization-of-input-formats">Optimization of Input Formats</h2>

<p>Based on the above research, we selected two table formats to be passed to LLMs in our experiments to further optimize system performance:</p>

<ol>
  <li><strong>HTML Format</strong>: HTML format provides a clear, structured representation that allows the model to accurately understand the hierarchy and relational content of the table. This format is suitable for scenarios requiring the preservation of complex table structures, especially in multi-dimensional or nested table contexts where HTML format can effectively convey the semantic information of the table.</li>
  <li><strong>Markdown Format</strong>: Markdown format, known for its simplicity and human readability, is widely used in various text representation tasks. Research indicates that in the RAG paradigm, Markdown format not only effectively represents table content but also enhances model processing efficiency. Therefore, we adopted the Markdown format in our experiments to evaluate its performance in practical applications.</li>
</ol>

<p>By adopting these two formats, we aim to maximize the potential of LLMs in table processing tasks. The structural advantage of the HTML format and the concise efficiency of the Markdown format offer flexible choices for different scenarios, ensuring that table content can be accurately understood and efficiently processed by LLMs, thereby further improving the overall performance of table-based question-answering systems.</p>

<p>The implementation of this format optimization strategy is not only theoretically supported by existing research but has also been practically validated in our experiments, providing a solid foundation for subsequent system development. We will continue to explore other possible formats to further optimize the way tables are passed to LLMs, ensuring that the system maintains excellent performance in various complex scenarios.</p>

<h1 id="evaluation-experiments">Evaluation Experiments</h1>

<h2 id="1-control-experiment">1. Control Experiment</h2>

<p>The purpose of the control experiment is to evaluate the performance changes when gradually adding various modules to the baseline model. The specific design is as follows:</p>

<ul>
  <li><strong>Baseline</strong>: The original model without any additional modules, serving as a reference standard.</li>
  <li><strong>Filter</strong>: Gradually adding different filtering modules to the baseline model.
    <ul>
      <li><strong>Semantics-based</strong>: This is further divided into two sub-parts:
        <ul>
          <li><strong>ColBERT</strong>: Adding the ColBERT semantic similarity comparison module.</li>
          <li><strong>OpenAI Embedding Model</strong>: Adding the OpenAI Embedding Model for semantic similarity comparison.</li>
        </ul>
      </li>
      <li><strong>LLM-based</strong>: Adding an LLM-based filter.</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>: Gradually adding different clarification strategies to the baseline model.
    <ul>
      <li><strong>Term Exp.</strong>: Adding the term expansion module.</li>
      <li><strong>Table Summary</strong>: Adding the table summary module.</li>
      <li><strong>Exp. &amp; Summary</strong>: Adding both the term expansion and table summary modules.</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>: Gradually adding different formatting</li>
</ul>

<p>methods to the baseline model.
    - <strong>String</strong>: Using string formatting.
    - <strong>Markdown</strong>: Using Markdown formatting.
    - <strong>HTML</strong>: Using HTML formatting.</p>
<ul>
  <li><strong>Retriever</strong>: Testing different retrieval strategies on the baseline model, particularly evaluating the impact of using the rerank mechanism with the ColBERT model to reorder results.
    <ul>
      <li><strong>BM25</strong>: Using BM25 for retrieval.</li>
      <li><strong>DPR</strong>: Using DPR for retrieval.</li>
      <li><strong>ColBERT</strong>: Using ColBERT for retrieval, also evaluating whether reranking the retrieval results impacts the outcome.</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>: Testing the performance of the model after adding a consistency module.</li>
</ul>

<h2 id="2-ablation-experiment">2. Ablation Experiment</h2>

<ul>
  <li><strong>Filter</strong>: Exploring the impact of different filters on model performance.
    <ul>
      <li><strong>Semantics-based</strong>: Further divided into two sub-parts, where the modules using ColBERT and the OpenAI Embedding Model for semantic similarity comparison are removed.</li>
      <li><strong>LLM-based</strong>: Removing the LLM-based filter module.</li>
    </ul>
  </li>
  <li><strong>Clarifier</strong>: Evaluating the contribution of different clarification strategies to the model.
    <ul>
      <li><strong>Term Exp.</strong>: Removing the term expansion module.</li>
      <li><strong>Table Summary</strong>: Removing the table summary module.</li>
      <li><strong>All Removed</strong>: Removing all clarification-related modules.</li>
    </ul>
  </li>
  <li><strong>Formatter</strong>: Testing the impact of different formatting methods on the model.
    <ul>
      <li><strong>Markdown</strong>: Removing Markdown formatting.</li>
      <li><strong>HTML</strong>: Removing HTML formatting.</li>
    </ul>
  </li>
  <li><strong>Consist.</strong>: Testing the model’s performance without the consistency module.</li>
</ul>

<h3 id="retriever-evaluation">Retriever Evaluation</h3>

<p>To evaluate the recall rates of different retrievers, the following experiments were conducted on four datasets, with and without table summaries:</p>

<ul>
  <li><strong>BM25</strong>: Traditional TF-IDF retriever.</li>
  <li><strong>ColBERT</strong>:
    <ul>
      <li>No rerank: Using the initial retrieval results generated by ColBERT.</li>
      <li>With rerank: Reordering the initial retrieval results.</li>
    </ul>
  </li>
  <li><strong>DPR</strong>: A dense vector retriever based on deep learning.</li>
  <li><strong>Faiss Vector Database</strong>: An efficient vector retrieval database.</li>
</ul>

<h1 id="acknowledgments">Acknowledgments</h1>
<p>I would like to express my sincere gratitude to the authors of the paper <a href="https://arxiv.org/abs/2312.09039">“Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning”</a> for providing valuable insights that influenced some of the ideas presented in this article. 
Additionally, I would like to thank PeiMa from the University of Leeds for her significant contributions to this project. Her expertise and support were instrumental in shaping the outcome of this work.</p>

<h3 id="copyright-notice">Copyright Notice</h3>
<p>© Wuyuhang, 2024. All rights reserved. This article is entirely the work of Wuyuhang from the University of Manchester. It may not be reproduced, distributed, or used without explicit permission from the author. For inquiries, please contact me at yuhang.wu-4 [at] postgrad.manchester.ac.uk.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Zhang, T., Li, Y., Jin, Y. and Li, J., 2020. Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment. <em>arXiv preprint arXiv:2002.08245</em>.</li>
  <li>Li, L., Wang, H., Zha, L., Huang, Q., Wu, S., Chen, G. and Zhao, J., 2023. Learning a data-driven policy network for pre-training automated feature engineering. In <em>The Eleventh International Conference on Learning Representations</em>.</li>
  <li>Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.H., Routledge, B. and Wang, W.Y., 2021. Finqa: A dataset of numerical reasoning over financial data. <em>arXiv preprint arXiv:2109.00122</em>.</li>
  <li>Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H. and Wang, W., 2020. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. <em>arXiv preprint arXiv:2004.07347</em>.</li>
  <li>Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F. and Chua, T.S., 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. <em>arXiv preprint arXiv:2105.07624</em>.</li>
  <li>Babaev, D., Savchenko, M., Tuzhilin, A. and Umerenkov, D., 2019, July. Et-rnn: Applying deep learning to credit loan applications. In <em>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em> (pp. 2183-2190).</li>
  <li>Ye, Y., Hui, B., Yang, M., Li, B., Huang, F. and Li, Y., 2023, July. Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. In <em>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 174-184).</li>
  <li>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L. and Smith, N.A., 2022. Binding language models in symbolic languages.<em>arXiv preprint arXiv:2210.02875</em>.</li>
  <li>Robertson, S. and Zaragoza, H., 2009. The probabilistic relevance framework: BM25 and beyond. <em>Foundations and Trends® in Information Retrieval</em>, <em>3</em>(4), pp.333-389.</li>
  <li>Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. and Yih, W.T., 2020. Dense passage retrieval for open-domain question answering. <em>arXiv preprint arXiv:2004.04906</em>.</li>
  <li>Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J. and Wang, H., 2023. 
Retrieval-augmented generation for large language models: A survey. <em>arXiv preprint arXiv:2312.10997</em>.</li>
  <li>Lu, W., Zhang, J., Zhang, J. and Chen, Y., 2024. Large language model for table processing: A survey. <em>arXiv preprint arXiv:2402.05121</em>.</li>
  <li>Jiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W.X. and Wen, J.R., 2023. Structgpt: A general framework for large language model to reason over structured data. <em>arXiv preprint arXiv:2305.09645</em>.</li>
  <li>Sui, Y., Zou, J., Zhou, M., He, X., Du, L., Han, S. and Zhang, D., 2023. Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning. <em>arXiv preprint arXiv:2312.09039</em>.</li>
  <li>Bian, N., Han, X., Sun, L., Lin, H., Lu, Y., He, B., Jiang, S. and Dong, B., 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. <em>arXiv preprint arXiv:2303.16421</em>.</li>
  <li>Lin, W., Blloshmi, R., Byrne, B., de Gispert, A. and Iglesias, G., 2023, July. LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering. In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em> (pp. 1557-1566).</li>
  <li>Li, X., Chan, S., Zhu, X., Pei, Y., Ma, Z., Liu, X. and Shah, S., 2023.  Are ChatGPT and GPT-4 general-purpose solvers for financial text analytics? A study on several typical tasks. <em>arXiv preprint arXiv:2305.05862</em>.</li>
  <li>Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X. and Wang, W.Y., 2019. Tabfact: A large-scale dataset for table-based fact verification. <em>arXiv preprint arXiv:1909.02164</em>.</li>
  <li>Aly, R., Guo, Z., Schlichtkrull, M., Thorne, J., Vlachos, A., Christodoulopoulos, C., Cocarascu, O. and Mittal, A., 2021. Feverous: Fact extraction and verification over unstructured and structured information. <em>arXiv preprint arXiv:2106.05707</em>.</li>
  <li>Iyyer, M., Yih, W.T. and Chang, M.W., 2017, July. Search-based neural structured learning for sequential question answering. In <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em> (pp. 1821-1831).</li>
</ol>]]></content><author><name>Pei Ma</name></author><category term="blog" /><summary type="html"><![CDATA[This passage is about Table RAGx.]]></summary></entry><entry><title type="html">:ramen: Indigo, minimalist jekyll theme</title><link href="http://localhost:4000/indigo-jekyll-theme/" rel="alternate" type="text/html" title=":ramen: Indigo, minimalist jekyll theme" /><published>2016-01-23T22:10:00+00:00</published><updated>2016-01-23T22:10:00+00:00</updated><id>http://localhost:4000/indigo-jekyll-theme</id><content type="html" xml:base="http://localhost:4000/indigo-jekyll-theme/"><![CDATA[<p><img src="https://raw.githubusercontent.com/sergiokopplin/indigo/gh-pages/assets/screen-shot.png" alt="Screenshot" /></p>

<p>Example of project - Indigo Minimalist Jekyll Template - <a href="https://sergiokopplin.github.io/indigo/">Demo</a>. This is a simple and minimalist template for Jekyll for those who likes to eat noodles.</p>

<hr />

<p>What has inside?</p>

<ul>
  <li>Gulp</li>
  <li>BrowserSync</li>
  <li>Stylus</li>
  <li>SVG</li>
  <li>No JS</li>
  <li><a href="https://developers.google.com/speed/pagespeed/insights/?url=http%3A%2F%2Fsergiokopplin.github.io%2Findigo%2F">98/100</a></li>
</ul>

<hr />

<p><a href="https://sergiokopplin.github.io/indigo/">Check it out</a> here.
If you need some help, just <a href="https://github.com/sergiokopplin/indigo/issues">tell me</a>.</p>]]></content><author><name>johndoe</name></author><category term="project" /><category term="jekyll" /><summary type="html"><![CDATA[This is a simple and minimalist template for Jekyll for those who likes to eat noodles.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" /><media:content medium="image" url="https://sergiokopplin.github.io/indigo/assets/images/jekyll-logo-light-solid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>